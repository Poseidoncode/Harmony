[
  {
    "role": "user",
    "parts": [
      {
        "text": "This is the Gemini CLI. We are setting up the context for our chat.\nToday's date is Thursday, February 12, 2026 (formatted according to the user's locale).\nMy operating system is: darwin\nThe project's temporary directory is: /Users/poseidomhung/.gemini/tmp/08526d3f4291f9d55d22dac70d4aac16a9b53eb9b4ef382e5fbb1d9ee234fd74\nI'm currently working in the following directories:\n  - /Users/poseidomhung/Documents/github/Infinity/Harmony\n  - /Users/poseidomhung/.gemini/extensions/ralph\n  - /Users/poseidomhung/.gemini/tmp/08526d3f4291f9d55d22dac70d4aac16a9b53eb9b4ef382e5fbb1d9ee234fd74/plans\nHere is the folder structure of the current working directories:\n\nShowing up to 200 items (files + folders). Folders or files indicated with ... contain more items not shown, were ignored, or the display limit (200 items) was reached.\n\n/Users/poseidomhung/Documents/github/Infinity/Harmony/\n├───.eslintrc.json\n├───.gitignore\n├───harmony-prompt-assistant-0.0.1.vsix\n├───LICENSE\n├───Makefile\n├───package-lock.json\n├───package.json\n├───PUBLISHING_GUIDE.md\n├───README.md\n├───tsconfig.json\n├───vite.config.ts\n├───walkthrough.md\n├───.gemini/\n│   └───task.md\n├───.git/...\n├───.vscode/\n├───.vscode-test/\n│   ├───extensions/...\n│   ├───user-data/...\n│   └───vscode-darwin-arm64-1.108.2/...\n├───example-template/\n│   └───usual.json\n├───media/\n│   └───icon.svg\n├───node_modules/...\n├───out/\n│   ├───test/...\n│   └───webview-ui/...\n├───resources/\n│   └───templates.json\n├───src/\n│   ├───extension.ts\n│   ├───SidebarProvider.ts\n│   └───test/\n│       ├───runTest.ts\n│       └───suite/\n│           ├───extension.test.ts\n│           └───index.ts\n└───webview-ui/\n    ├───index.html\n    └───src/\n        ├───App.vue\n        ├───main.ts\n        ├───assets/\n        └───components/\nShowing up to 200 items (files + folders).\n\n/Users/poseidomhung/.gemini/extensions/ralph/\n├───.gemini-extension-install.json\n├───CONTRIBUTING.md\n├───gemini-extension.json\n├───LICENSE\n├───README.md\n├───SECURITY.md\n├───.gemini/\n│   └───ralph/\n│       └───state.json\n├───commands/\n│   └───ralph/\n│       ├───cancel.toml\n│       ├───explore.md\n│       ├───explore.toml\n│       ├───help.toml\n│       ├───loop.toml\n│       ├───ultrawork.md.bak\n│       └───ultrawork.toml\n├───hooks/\n│   ├───hooks.json\n│   ├───stop-hook.sh\n│   └───todo-enforcer.sh\n├───release/\n│   └───ralph.tar.gz\n├───scripts/\n│   ├───cancel.sh\n│   └───setup.sh\n└───tests/\n    ├───hook_test.sh\n    └───setup_test.sh\nShowing up to 200 items (files + folders).\n\n/Users/poseidomhung/.gemini/tmp/08526d3f4291f9d55d22dac70d4aac16a9b53eb9b4ef382e5fbb1d9ee234fd74/plans/\n└───webview-persistence.md\n\nReminder: Do not return an empty response when a tool call is required.\n\nMy setup is complete. I will provide my first command in the next turn."
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "text": "我想問一下兩個GEMINI CLI之間可以共享記憶體嗎？"
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "text": "在當前的設計中，不同的 **Gemini CLI 實例（Session）** 之間預設是**不共享執行中的短期記憶（Context）**的，但可以透過以下幾種方式實現「共享」或「持久化」記憶：\n\n### 1. `save_memory` 工具（長期記憶）\n這是我最主要的記憶工具。當你要求我「記住某件事」時，我會將其儲存到你的使用者設定檔中。\n- **作用範圍**：跨 Session 共享。\n- **使用場景**：偏好的程式碼風格、常用的專案路徑、特定的 API 金鑰位置等。\n- **限制**：適合儲存短小、關鍵的事實，而非大量的專案細節。\n\n### 2. `.gemini` 目錄（專案級記憶）\n在你的專案根目錄中，通常會有一個 `.gemini/` 資料夾：\n- **`task.md`**：我會在這裡追蹤任務進度。如果你在另一個 CLI 視窗開啟同一個專案，我可以透過讀取這個檔案來得知目前的開發狀態。\n- **專案規範**：你可以將專案特有的規則寫在 `.gemini/GEMINI.md` 中，這樣任何在此專案啟動的 Gemini CLI 都會讀取到這些規範。\n\n### 3. Git 狀態與程式碼庫\n既然我們都在同一個專案目錄下工作，**程式碼本身就是最強大的共享記憶**。\n- 我可以透過 `git log` 查看之前的修改原因。\n- 透過 `grep_search` 查找之前實作過的邏輯。\n\n### 4. 共享 Temporary Directory\n你提供的環境變數中包含了一個專案暫存路徑：\n`/Users/poseidomhung/.gemini/tmp/08526d3f4291f9d55d22dac70d4aac16a9b53eb9b4ef382e5fbb1d9ee234fd74/`\n在這個目錄下的檔案（如 `plans/`）可以被不同的 CLI 實例存取，作為中轉站。\n\n---\n\n**總結：**\n如果你希望我「記住」某個全域偏好，請直接說：**「請記住我偏好使用 TypeScript 撰寫所有後端邏輯」**。\n如果你希望另一個 CLI 實例接手目前的開發任務，我建議先執行 **`git status`** 並確保 **`task.md`** 已更新。\n\n你有特定的資訊想要跨實例共享嗎？我可以幫你決定最適合的儲存方式。"
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "text": "因為AI的一次對話中，超過一定長度後，他表現不是就開始變差了，大家都怎麼解決的"
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "text": "針對 AI 在長對話中表現下滑（Context Window 飽和或遺忘）的問題，在 **Gemini CLI** 的工作流程中，我們主要透過以下幾種 **「外部腦」策略** 來解決：\n\n### 1. 任務碎片化與 `task.md` (State Management)\n這是最核心的解決方案。我們不會把所有的細節都塞在對話裡，而是將進度寫入專案中的 `.gemini/task.md`。\n*   **做法**：當對話變得太長時，我會將當前的「階段性狀態」、「剩餘待辦事項」和「關鍵架構決定」總結到檔案中。\n*   **好處**：當對話太累贅時，你可以直接重啟一個新 Session (輸入 `exit` 再重新進入)，新 Session 會自動讀取 `task.md`，瞬間「接上」進度，且 Context 是乾淨的。\n\n### 2. 減少對話負擔，增加「現場搜索」\n比起讓 AI「記住」程式碼，更好的做法是讓我「需要用時再去查」。\n*   **做法**：我會頻繁使用 `grep_search` 或 `codebase_investigator` 來動態獲取資訊。\n*   **好處**：Context Window 裡只會存放「目前正在修改的片段」，而不是整個專案的記憶，大幅降低 AI 的負擔。\n\n### 3. 使用 `ultrawork` (強化推理模式)\n如果你開啟了 `ultrawork` 模式，我會啟動 **Sequential Thinking (連續推理)**。\n*   **做法**：這會強迫我將思考過程拆解成多個步驟，並在每個步驟自我驗證。\n*   **好處**：即便在長對話中，這種結構化的思考方式能有效抑制 AI 產生幻覺（Hallucination）。\n\n### 4. 專案級設定 (`GEMINI.md`)\n將「不變的規則」從對話中抽離，寫入 `.gemini/GEMINI.md` 或 `.geminiignore`。\n*   **好處**：這些規則每次啟動都會自動加載，不需要你在對話中反覆提醒，節省對話空間。\n\n### 5. 主動清理 Context (或重啟)\n當你發現我開始出現「胡言亂語」或「無視指令」時，資深用戶通常會：\n1.  **要求我總結目前的任務**：`請更新當前的 task.md，確保包含所有目前的進度。`\n2.  **重啟 Session**：`exit` 掉舊的，開一個新的 CLI。\n\n**簡單來說：把記憶「檔案化」，對話只處理「邏輯」。**\n\n目前你在 `/Users/poseidomhung/Documents/github/Infinity/Harmony` 這個專案中，如果你覺得目前的對話開始變得沉重，我可以幫你把目前的狀況整理進 `task.md`，然後你隨時可以開啟新對話。\n\n需要我為你目前的狀態做個總結嗎？"
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "text": "那RLM https://github.com/alexzhang13/rlm 有什麼用.. 作的事情不是跟你差不多"
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "text": "我會先去研究一下這個 RLM (Reinforcement Learning Memory) 專案的具體功能，以便為你分析它與 Gemini CLI 記憶機制的差異。"
      },
      {
        "functionCall": {
          "name": "web_fetch",
          "args": {
            "prompt": "Summarize the purpose and key features of the RLM project at https://github.com/alexzhang13/rlm, especially focusing on how it handles memory and context for LLMs."
          },
          "id": "web_fetch-1770835657039-c670927e3353e8"
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "web_fetch-1770835657039-c670927e3353e8",
          "name": "web_fetch",
          "response": {
            "output": "RLM 是一個通用的即插即用推斷函式庫，專為遞迴語言模型（RLMs）設計，支援多種沙盒環境。它透過讓語言模型以程式設計方式檢查、分解並遞迴呼叫自身來處理近乎無限長的上下文，從而取代傳統的 `llm.completion` 呼叫。RLM 將[1]上下文作為 REPL 環境中的變數，讓語言模型能夠與之互動並啟動子語言模型呼叫。它支援隔離和非隔離的 REPL 環境，確保在執行語言模型生成的程式碼時的安全性。[1][1][1]\n\nSources:\n[1] alexzhang13/rlm: General plug-and-play inference library ... - GitHub (https://github.com/alexzhang13/rlm)"
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "text": "這個 **RLM (Recursive Language Model)** 與 **Gemini CLI** 在解決「長上下文」問題上的思路非常不同，可以說是一個是「**算法層面**」的解決方案，一個是「**工程工作流**」的解決方案。\n\n以下是它們的對比分析：\n\n### 1. RLM 的核心邏輯：遞迴 (Recursion)\nRLM 是一個推論庫（Inference Library），它的核心想法是：**「與其一次讀完所有內容，不如讓 AI 把自己當成一個遞迴函數。」**\n*   **做法**：它將上下文變成 REPL（互動式程式環境）中的變數。當任務太長時，AI 會啟動一個「子 AI（Sub-call）」去處理其中一部分，處理完後回傳結果給父 AI。\n*   **解決問題**：它處理的是**理論上無限長**的上下文，透過不斷的遞迴縮減，讓每一層 AI 只處理它能負荷的 Context。\n\n### 2. Gemini CLI 的核心邏輯：外部記憶與工具調用\nGemini CLI（也就是現在的我）更像是一個**「配備了完整工具箱的資深工程師」**。\n*   **做法**：我並不試圖「遞迴地」讀取所有檔案，而是透過 **`grep_search`**、**`codebase_investigator`** 等工具，精準地從你的硬碟中「檢索」出現在需要的片段。\n*   **狀態管理**：我使用 **`.gemini/task.md`** 作為我的「外部記事本」，這不是 AI 的原生記憶，而是物理檔案，這確保了即便我重啟，我的進度也不會消失。\n\n### 3. 主要差異對照\n\n| 維度 | RLM (alexzhang13/rlm) | Gemini CLI |\n| :--- | :--- | :--- |\n| **角色定位** | 一套底層的**開發框架/庫** | 一個可以直接使用的**智能代理 (Agent)** |\n| **處理長文本** | 透過遞迴呼叫 (Recursive calls) | 透過工具檢索 (Tool-based Search) |\n| **記憶方式** | 將 Context 變為程式變數 | 寫入檔案 (`task.md`) 與使用者事實 (`save_memory`) |\n| **目標場景** | 開發需要處理極長文本的新應用 | 實際的軟體開發、除錯、重構任務 |\n| **安全性** | 著重於沙盒 REPL 環境的執行安全性 | 著重於對開發者系統操作的透明度與安全 |\n\n### 總結來說：\n*   如果你是一個**開發者**，想要寫一個程式來自動分析幾萬行的 Log 或整本小說，**RLM** 提供了一種優雅的遞迴處理架構。\n*   如果你是在**寫專案**，需要一個 AI 幫你寫 Feature、修 Bug，並且希望它能「記得」你昨天做了什麼，**Gemini CLI** 的工作流（透過檔案持久化記憶）會更符合實際生產需求。\n\n**這兩者其實可以結合：** 未來我的底層邏輯（如何思考長難題）可以引入 RLM 這種遞迴思考模型，但在使用者介面上，我依然會是用 CLI 的形式來為你服務。\n\n目前的 Gemini CLI 對於一般專案開發來說，靠著「精準檢索」而非「全量讀取」，已經能避開 90% 的 Context 飽和問題。如果你有特別龐大的專案（例如 Linux Kernel 等級），那我們可能真的會需要用到 RLM 這種遞迴分治的思想了。"
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "text": "可是RLM要怎麼用，每次問都要CALL？"
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "text": "我會先深入研究 RLM 的 README 和範例程式碼，確認它的具體使用方式，以及它是如何處理對話與呼叫的。"
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "functionCall": {
          "name": "read_wiki_contents",
          "args": {
            "repoName": "alexzhang13/rlm"
          },
          "id": "read_wiki_contents-1770835965133-bfe3ebe93f1618"
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "read_wiki_contents-1770835965133-bfe3ebe93f1618",
          "name": "read_wiki_contents",
          "response": {
            "output": "# Page: Overview\n\n# Overview\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [Makefile](Makefile)\n- [README.md](README.md)\n\n</details>\n\n\n\nThis document introduces the Recursive Language Models (RLM) system, explaining its core purpose, architecture, and key components. The RLM system is an inference paradigm that enables language models to process extensive contexts by programmatically examining, decomposing, and recursively invoking itself over input data.\n\nFor installation instructions and initial setup, see [Getting Started](#2). For detailed architecture documentation, see [Core Architecture](#3). For information on specific execution environments, see [Execution Environments](#5).\n\n## What are Recursive Language Models?\n\nRecursive Language Models (RLMs) are a task-agnostic inference paradigm designed to handle near-infinite length contexts by enabling language models to **programmatically** interact with their input. Instead of the canonical API call pattern:\n\n```python\nresponse = llm.completion(prompt, model)\n```\n\nRLMs introduce a new pattern:\n\n```python\nresponse = rlm.completion(prompt, model)\n```\n\nThe key difference is that RLMs offload the context as a variable in a REPL (Read-Eval-Print Loop) environment that the language model can examine, decompose, and recursively call itself over.\n\n**Sources:** [README.md:30-36]()\n\n## Core Concepts\n\n### The Problem: Context Length Limitations\n\nStandard language model APIs require all context to fit within a fixed token limit (e.g., 128K tokens for GPT-4). For tasks involving large codebases, extensive documents, or complex reasoning chains, this limitation becomes a fundamental bottleneck.\n\n### The RLM Solution: Programmatic Context Handling\n\nRather than forcing all context through the LM's attention mechanism, RLMs:\n\n1. **Store context in a REPL environment** where it can be accessed as a variable\n2. **Allow the LM to generate code** that examines and processes this context\n3. **Execute that code** in a sandboxed environment\n4. **Enable recursive sub-calls** where generated code can invoke the LM again on decomposed sub-problems\n\nThis transforms context processing from a passive attention operation into an active, programmatic computation that the LM controls through code generation.\n\n### Key Innovation: Recursive Self-Invocation\n\nThe system supports **recursive depth** through the `llm_query()` function available in REPL environments. When the LM generates code that calls `llm_query()`, the system:\n\n1. Routes the request through the `LMHandler` TCP server\n2. Tracks recursion depth to prevent infinite loops\n3. Can use different LM backends at different depths (e.g., GPT-4 for top-level reasoning, GPT-3.5 for sub-tasks)\n4. Aggregates usage and cost metrics across all recursive calls\n\nFor details on recursion mechanics, see [Recursion and Sub-Calls](#3.4).\n\n**Sources:** [README.md:30-36]()\n\n## System Architecture\n\nThe RLM system consists of four primary layers that work together to orchestrate recursive language model execution:\n\n```mermaid\ngraph TB\n    subgraph UserLayer[\"User Interface Layer\"]\n        UserCode[\"User Code<br/>rlm.completion()\"]\n        ExampleScripts[\"examples/quickstart.py<br/>examples/docker_repl_example.py\"]\n    end\n    \n    subgraph OrchestrationLayer[\"Core Orchestration Layer\"]\n        RLMClass[\"RLM<br/>rlm/core/rlm.py<br/>RLM class\"]\n        LMHandlerClass[\"LMHandler<br/>rlm/core/lm_handler.py<br/>LMHandler class\"]\n        LoggerClass[\"RLMLogger<br/>rlm/logger/rlm_logger.py\"]\n    end\n    \n    subgraph ClientLayer[\"LM Backend Layer\"]\n        GetClientFactory[\"get_client()<br/>rlm/core/client_factory.py\"]\n        BaseLMClass[\"BaseLM<br/>rlm/clients/base_lm.py\"]\n        OpenAIClient[\"OpenAIClient<br/>rlm/clients/openai.py\"]\n        AnthropicClient[\"AnthropicClient<br/>rlm/clients/anthropic.py\"]\n        OtherClients[\"PortkeyClient, LiteLLMClient,<br/>AzureOpenAIClient, GeminiClient\"]\n    end\n    \n    subgraph EnvironmentLayer[\"Execution Environment Layer\"]\n        GetEnvFactory[\"get_environment()<br/>rlm/core/environment_factory.py\"]\n        BaseEnvClass[\"BaseEnv<br/>rlm/environments/base_env.py\"]\n        LocalREPL[\"LocalREPL<br/>rlm/environments/local_repl.py\"]\n        DockerREPL[\"DockerREPL<br/>rlm/environments/docker_repl.py\"]\n        CloudREPLs[\"ModalREPL, PrimeREPL,<br/>DaytonaREPL\"]\n    end\n    \n    UserCode --> RLMClass\n    ExampleScripts --> RLMClass\n    \n    RLMClass --> LMHandlerClass\n    RLMClass --> LoggerClass\n    RLMClass --> GetClientFactory\n    RLMClass --> GetEnvFactory\n    \n    GetClientFactory --> BaseLMClass\n    BaseLMClass --> OpenAIClient\n    BaseLMClass --> AnthropicClient\n    BaseLMClass --> OtherClients\n    \n    GetEnvFactory --> BaseEnvClass\n    BaseEnvClass --> LocalREPL\n    BaseEnvClass --> DockerREPL\n    BaseEnvClass --> CloudREPLs\n    \n    LMHandlerClass --> BaseLMClass\n    BaseEnvClass --> LMHandlerClass\n```\n\n**Figure 1: System Architecture - Core Components and Code Locations**\n\n### Layer Descriptions\n\n| Layer | Purpose | Key Files |\n|-------|---------|-----------|\n| **User Interface** | Entry point for RLM usage | [examples/quickstart.py](), user code |\n| **Core Orchestration** | Coordinates execution flow, manages iteration loop, routes requests | [rlm/core/rlm.py](), [rlm/core/lm_handler.py]() |\n| **LM Backend** | Abstracts LLM provider APIs (OpenAI, Anthropic, etc.) | [rlm/clients/base_lm.py](), [rlm/clients/openai.py]() |\n| **Execution Environment** | Sandboxed code execution contexts (local, Docker, cloud) | [rlm/environments/base_env.py](), [rlm/environments/local_repl.py]() |\n\n**Sources:** [README.md:30-36](), [README.md:66-77](), [README.md:79-96](), Diagram 1 from high-level architecture\n\n## Execution Workflow\n\nThe following diagram illustrates the complete execution flow of an RLM completion request, showing the actual method calls and data types used:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant RLM[\"RLM<br/>rlm.completion()\"]\n    participant LMHandler[\"LMHandler<br/>handle_request()\"]\n    participant BaseLM[\"BaseLM<br/>completion()\"]\n    participant BaseEnv[\"BaseEnv<br/>execute_code()\"]\n    \n    User->>RLM: rlm.completion(prompt, model)\n    activate RLM\n    \n    RLM->>RLM: _setup_handler_and_environment()\n    RLM->>RLM: _build_prompt()\n    \n    loop \"Iteration Loop (max_iterations)\"\n        RLM->>LMHandler: handle_request(LMRequest)\n        activate LMHandler\n        LMHandler->>BaseLM: completion(messages)\n        activate BaseLM\n        BaseLM-->>LMHandler: LMResponse\n        deactivate BaseLM\n        LMHandler-->>RLM: RLMChatCompletion\n        deactivate LMHandler\n        \n        RLM->>RLM: _extract_code_blocks()\n        \n        opt \"If code blocks found\"\n            RLM->>BaseEnv: execute_code(code)\n            activate BaseEnv\n            \n            opt \"Sub-LM call via llm_query()\"\n                BaseEnv->>LMHandler: Socket/HTTP request\n                activate LMHandler\n                LMHandler->>BaseLM: completion(depth+1)\n                BaseLM-->>LMHandler: response\n                LMHandler-->>BaseEnv: LMResponse\n                deactivate LMHandler\n            end\n            \n            BaseEnv-->>RLM: REPLResult\n            deactivate BaseEnv\n        end\n        \n        RLM->>RLM: _check_for_final_answer()\n        \n        alt \"Final answer found\"\n            RLM-->>User: RLMChatCompletion\n        end\n    end\n    \n    RLM-->>User: RLMChatCompletion\n    deactivate RLM\n```\n\n**Figure 2: Execution Flow - Method Calls and Data Types**\n\n### Workflow Steps\n\n1. **Initialization** ([rlm/core/rlm.py:__init__]()): User creates `RLM` instance with backend and environment configuration\n2. **Completion Request** ([rlm/core/rlm.py:completion]()): User calls `rlm.completion(prompt)` \n3. **Setup Phase**: System initializes `LMHandler` TCP server and `BaseEnv` execution environment\n4. **Iteration Loop**: System enters a loop with maximum iteration limit:\n   - Sends prompt to LM through handler\n   - Extracts code blocks from response\n   - Executes code in environment\n   - Handles recursive `llm_query()` calls through socket/HTTP protocol\n   - Checks for final answer marker\n5. **Result Aggregation**: Returns `RLMChatCompletion` with full execution trace\n\n**Sources:** [README.md:66-77](), Diagram 2 from high-level architecture\n\n## Core Components\n\n### RLM Orchestrator\n\nThe `RLM` class ([rlm/core/rlm.py]()) serves as the main entry point and orchestrates all system components. It:\n\n- Manages the iteration loop with configurable `max_iterations`\n- Coordinates between `LMHandler` and `BaseEnv` instances  \n- Extracts code blocks from LM responses using regex patterns\n- Aggregates `REPLResult` objects into `RLMIteration` objects\n- Tracks execution metadata and usage statistics\n\nFor detailed documentation, see [RLM Class](#3.1).\n\n### LM Handler\n\nThe `LMHandler` class ([rlm/core/lm_handler.py]()) implements a multi-threaded TCP server that:\n\n- Manages communication between execution environments and LM clients\n- Routes requests to appropriate clients based on recursion depth\n- Supports depth-based backend switching (e.g., different models at different recursion levels)\n- Handles both single and batched LM requests\n- Implements socket protocol for non-isolated environments\n- Implements HTTP broker pattern for isolated environments\n\nFor detailed documentation, see [LM Handler](#3.2).\n\n### REPL Environments\n\nThe environment layer provides sandboxed execution contexts through the `BaseEnv` interface ([rlm/environments/base_env.py]()). Environments are categorized into:\n\n| Environment Type | Class | Isolation Level | Communication |\n|------------------|-------|----------------|---------------|\n| **Local** | `LocalREPL` | Same process | Direct socket |\n| **Docker** | `DockerREPL` | Container | Socket via host.docker.internal |\n| **Modal** | `ModalREPL` | Cloud sandbox | HTTP broker + encrypted tunnels |\n| **Prime** | `PrimeREPL` | Cloud sandbox | HTTP broker + port exposure |\n| **Daytona** | `DaytonaREPL` | Cloud sandbox | HTTP broker + preview URLs |\n\nAll environments expose `llm_query()` functions that allow generated code to make recursive LM calls. For detailed documentation, see [REPL Environments](#3.3) and [Execution Environments](#5).\n\n### LM Clients\n\nThe client layer abstracts LLM provider APIs through the `BaseLM` interface ([rlm/clients/base_lm.py]()). The system supports:\n\n- **OpenAI-compatible APIs**: OpenAI, OpenRouter, Vercel AI Gateway, vLLM, Prime Intellect\n- **Native clients**: Anthropic, Azure OpenAI, Google Gemini\n- **Gateway platforms**: Portkey, LiteLLM\n\nThe `get_client()` factory function ([rlm/core/client_factory.py]()) selects the appropriate client implementation at runtime based on the `ClientBackend` literal type.\n\nFor detailed documentation, see [LLM Client Backends](#4).\n\n**Sources:** [rlm/core/rlm.py](), [rlm/core/lm_handler.py](), [rlm/environments/base_env.py](), [rlm/clients/base_lm.py](), [README.md:79-117]()\n\n## Data Types and Communication\n\nThe system uses strongly-typed data structures defined in [rlm/core/types.py]() to ensure type safety across component boundaries:\n\n```mermaid\ngraph LR\n    subgraph RequestResponse[\"Request/Response Flow\"]\n        LMRequest[\"LMRequest<br/>prompt, model, depth\"]\n        LMResponse[\"LMResponse<br/>chat_completion(s), error\"]\n        RLMChatCompletion[\"RLMChatCompletion<br/>prompt, response, usage\"]\n    end\n    \n    subgraph ExecutionResults[\"Execution Results\"]\n        CodeBlock[\"CodeBlock<br/>code, result\"]\n        REPLResult[\"REPLResult<br/>stdout, stderr, locals\"]\n        RLMIteration[\"RLMIteration<br/>response, code_blocks\"]\n    end\n    \n    subgraph UsageTracking[\"Usage Tracking\"]\n        ModelUsage[\"ModelUsageSummary<br/>calls, input_tokens, output_tokens\"]\n        UsageSummary[\"UsageSummary<br/>per-model aggregation\"]\n    end\n    \n    LMRequest --> LMResponse\n    LMResponse --> RLMChatCompletion\n    RLMChatCompletion --> UsageSummary\n    \n    CodeBlock --> REPLResult\n    REPLResult --> RLMChatCompletion\n    RLMIteration --> CodeBlock\n    \n    UsageSummary --> ModelUsage\n```\n\n**Figure 3: Data Type Relationships**\n\n### Communication Patterns\n\nThe system implements two communication protocols depending on environment isolation:\n\n1. **Socket Protocol** (Non-Isolated): Direct TCP connection using 4-byte length-prefix + JSON serialization\n2. **HTTP Broker Pattern** (Isolated): Flask server in sandbox, poller thread on host, platform-specific tunnels\n\nFor detailed documentation on communication protocols and data types, see [Communication Protocols and Data Types](#3.5).\n\n**Sources:** [rlm/core/types.py](), Diagram 5 from high-level architecture, Diagram 2 and 3 from high-level architecture\n\n## Getting Started\n\nTo begin using the RLM system:\n\n1. **Installation**: See [Installation and Setup](#2.1) for dependency installation with `uv`\n2. **Quick Start**: See [Quick Start Example](#2.2) for a basic usage example\n3. **Environment Selection**: Choose an execution environment based on isolation requirements (see [Execution Environments](#5))\n4. **Backend Configuration**: Select an LLM provider backend (see [LLM Client Backends](#4))\n\nA minimal example:\n\n```python\nfrom rlm import RLM\n\nrlm = RLM(\n    backend=\"openai\",\n    backend_kwargs={\"model_name\": \"gpt-4\"},\n    environment=\"local\",  # or \"docker\", \"modal\", \"prime\"\n    verbose=True\n)\n\nresult = rlm.completion(\"Analyze the first 100 Fibonacci numbers\")\nprint(result.response)\n```\n\n**Sources:** [README.md:48-77](), [README.md:82-87]()\n\n## Advanced Features\n\nThe RLM system supports several advanced features for production use:\n\n- **Multi-Backend Configuration**: Route requests to different LLM providers based on recursion depth ([Multi-Backend Configuration](#8.1))\n- **Persistent Sessions**: Maintain state across multiple completion requests ([Persistent Environments and Multi-Turn Sessions](#8.2))\n- **Usage Tracking**: Comprehensive token and cost tracking across all recursive calls ([Usage Tracking and Cost Monitoring](#8.4))\n- **Trajectory Visualization**: Debug and analyze execution traces with the web-based visualizer ([Logging and Visualization](#6))\n\n## References\n\n- Full Paper: [arXiv:2512.24601](https://arxiv.org/abs/2512.24601)\n- Blogpost: [Recursive Language Models](https://alexzhang13.github.io/blog/2025/rlm/)\n- Minimal Implementation: [rlm-minimal](https://github.com/alexzhang13/rlm-minimal)\n\n**Sources:** [README.md:120-136]()\n\n---\n\n# Page: Getting Started\n\n# Getting Started\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [Makefile](Makefile)\n- [README.md](README.md)\n- [pyproject.toml](pyproject.toml)\n\n</details>\n\n\n\nThis section guides you through the initial setup and basic usage of the RLM system. It covers the minimum requirements to install dependencies, configure API credentials, and execute your first RLM completion request. The material focuses on the standard workflow for new users working with the default `LocalREPL` environment and common LLM provider backends.\n\nFor detailed installation steps and environment configuration, see [Installation and Setup](#2.1). For a complete walkthrough of a basic example with code explanation, see [Quick Start Example](#2.2). For information about the core architectural components and how they interact, see [Core Architecture](#3).\n\n---\n\n## Overview of the RLM Workflow\n\nThe RLM system replaces the standard `llm.completion(prompt, model)` pattern with `rlm.completion(prompt, model)`, where the language model can programmatically examine input context, generate and execute Python code, and recursively invoke itself. The basic user workflow involves three main steps:\n\n```mermaid\ngraph LR\n    A[\"Install<br/>Dependencies\"] --> B[\"Configure<br/>Backend & Environment\"]\n    B --> C[\"Execute<br/>rlm.completion()\"]\n    C --> D[\"Receive<br/>RLMChatCompletion\"]\n```\n\n**Sources:** [README.md:30-77]()\n\n---\n\n## Core Components from a User Perspective\n\nWhen working with RLM, you interact with three primary configuration points:\n\n```mermaid\ngraph TB\n    subgraph \"User Code\"\n        UserScript[\"Python Script<br/>examples/quickstart.py\"]\n    end\n    \n    subgraph \"RLM Configuration\"\n        RLMClass[\"RLM Class<br/>rlm.core.rlm.RLM\"]\n        \n        BackendParam[\"backend parameter<br/>ClientBackend literal\"]\n        BackendKwargs[\"backend_kwargs dict<br/>model_name, api_key, etc\"]\n        \n        EnvParam[\"environment parameter<br/>EnvironmentType literal\"]\n        EnvKwargs[\"environment_kwargs dict<br/>image, timeout, etc\"]\n        \n        VerboseParam[\"verbose parameter<br/>bool for console output\"]\n        LoggerParam[\"logger parameter<br/>RLMLogger instance\"]\n    end\n    \n    subgraph \"Backend Selection\"\n        GetClient[\"get_client factory<br/>rlm.clients.__init__.py\"]\n        \n        OpenAIClient[\"OpenAIClient\"]\n        AnthropicClient[\"AnthropicClient\"]\n        PortkeyClient[\"PortkeyClient\"]\n        GeminiClient[\"GeminiClient\"]\n        \n        GetClient --> OpenAIClient\n        GetClient --> AnthropicClient\n        GetClient --> PortkeyClient\n        GetClient --> GeminiClient\n    end\n    \n    subgraph \"Environment Selection\"\n        GetEnvironment[\"get_environment factory<br/>rlm.environments.__init__.py\"]\n        \n        LocalREPL[\"LocalREPL<br/>default, in-process\"]\n        DockerREPL[\"DockerREPL<br/>containerized\"]\n        ModalREPL[\"ModalREPL<br/>Modal Sandboxes\"]\n        PrimeREPL[\"PrimeREPL<br/>Prime Intellect\"]\n        \n        GetEnvironment --> LocalREPL\n        GetEnvironment --> DockerREPL\n        GetEnvironment --> ModalREPL\n        GetEnvironment --> PrimeREPL\n    end\n    \n    UserScript --> RLMClass\n    \n    RLMClass --> BackendParam\n    RLMClass --> BackendKwargs\n    RLMClass --> EnvParam\n    RLMClass --> EnvKwargs\n    RLMClass --> VerboseParam\n    RLMClass --> LoggerParam\n    \n    BackendParam --> GetClient\n    EnvParam --> GetEnvironment\n```\n\n**Diagram: RLM Initialization Components**\n\nThe `RLM` class constructor accepts parameters that determine which LLM provider backend to use (via `backend` and `backend_kwargs`) and which execution environment to use (via `environment` and `environment_kwargs`). Factory functions `get_client` and `get_environment` resolve these string literals to concrete implementations at runtime.\n\n**Sources:** [README.md:66-87](), [rlm/core/rlm.py]()\n\n---\n\n## Minimal Example Structure\n\nThe simplest RLM program follows this pattern:\n\n| Step | Code Element | Purpose |\n|------|--------------|---------|\n| 1 | `from rlm import RLM` | Import the main orchestrator class |\n| 2 | `RLM(backend=\"...\", backend_kwargs={...})` | Initialize with LLM provider configuration |\n| 3 | `rlm.completion(prompt)` | Execute completion request with prompt |\n| 4 | `.response` | Extract text response from `RLMChatCompletion` object |\n\n```mermaid\nsequenceDiagram\n    participant User as \"User Code\"\n    participant RLM as \"RLM Instance<br/>rlm.core.rlm.RLM\"\n    participant Handler as \"LMHandler<br/>TCP Server\"\n    participant Client as \"LM Client<br/>BaseLM\"\n    participant Env as \"Environment<br/>BaseEnv\"\n    \n    User->>RLM: \"RLM(backend, environment)\"\n    RLM->>Handler: \"start TCP server\"\n    RLM->>Client: \"get_client(backend)\"\n    RLM->>Env: \"get_environment(environment)\"\n    \n    User->>RLM: \"completion(prompt)\"\n    \n    loop \"max_iterations\"\n        RLM->>Client: \"request completion\"\n        Client-->>RLM: \"LLM response\"\n        RLM->>Env: \"execute code blocks\"\n        Env->>Handler: \"llm_query() sub-calls\"\n        Handler->>Client: \"handle sub-requests\"\n        Client-->>Handler: \"sub-responses\"\n        Handler-->>Env: \"return results\"\n        Env-->>RLM: \"REPLResult\"\n        \n        alt \"final_answer found\"\n            RLM-->>User: \"RLMChatCompletion\"\n        end\n    end\n    \n    RLM-->>User: \"RLMChatCompletion\"\n```\n\n**Diagram: Basic RLM Execution Sequence**\n\nThis sequence shows the lifecycle of a single `rlm.completion()` call. The `RLM` instance initializes an `LMHandler` (TCP server) and creates client and environment instances. During iteration, the LLM generates code, the environment executes it, and sub-LM calls route through the handler back to the client. The loop continues until either a final answer is found or `max_iterations` is reached.\n\n**Sources:** [README.md:66-77](), [rlm/core/rlm.py](), Diagram 2 from high-level architecture\n\n---\n\n## Installation Prerequisites\n\nThe RLM system requires the following minimum environment:\n\n| Requirement | Specification | Configuration |\n|-------------|---------------|---------------|\n| Python Version | >= 3.11 | Specified in `pyproject.toml` |\n| Package Manager | `uv` (recommended) | Installation via curl script |\n| LLM Provider API Key | Environment variable | `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`, etc. |\n| Optional Dependencies | `modal`, `prime`, `daytona` | For isolated cloud environments |\n\nThe base installation installs core LLM client libraries (OpenAI, Anthropic, Google GenAI, Portkey) and utilities (requests, rich, python-dotenv). Optional dependencies are required only when using specific isolated environments.\n\n**Sources:** [pyproject.toml:9-24](), [README.md:48-64]()\n\n---\n\n## Configuration Options Overview\n\nThe `RLM` class accepts the following key initialization parameters:\n\n```mermaid\ngraph TB\n    subgraph \"Backend Configuration\"\n        Backend[\"backend: ClientBackend\"]\n        BackendKwargs[\"backend_kwargs: dict\"]\n        \n        ModelName[\"model_name: str\"]\n        APIKey[\"api_key: str\"]\n        BaseURL[\"base_url: str\"]\n        OtherParams[\"other params...\"]\n        \n        Backend --> BackendKwargs\n        BackendKwargs --> ModelName\n        BackendKwargs --> APIKey\n        BackendKwargs --> BaseURL\n        BackendKwargs --> OtherParams\n    end\n    \n    subgraph \"Environment Configuration\"\n        Environment[\"environment: EnvironmentType\"]\n        EnvironmentKwargs[\"environment_kwargs: dict\"]\n        \n        Image[\"image: str\"]\n        Timeout[\"timeout: int\"]\n        WorkDir[\"workdir: str\"]\n        EnvOtherParams[\"other params...\"]\n        \n        Environment --> EnvironmentKwargs\n        EnvironmentKwargs --> Image\n        EnvironmentKwargs --> Timeout\n        EnvironmentKwargs --> WorkDir\n        EnvironmentKwargs --> EnvOtherParams\n    end\n    \n    subgraph \"Behavior Configuration\"\n        MaxIterations[\"max_iterations: int<br/>default=10\"]\n        MaxDepth[\"max_depth: int<br/>default=3\"]\n        Verbose[\"verbose: bool<br/>default=False\"]\n        Logger[\"logger: RLMLogger<br/>optional\"]\n        OtherBackends[\"other_backends: list<br/>depth-based routing\"]\n    end\n    \n    subgraph \"Default Values\"\n        DefaultBackend[\"default: 'openai'\"]\n        DefaultEnv[\"default: 'local'\"]\n        \n        Backend -.->|\"if not specified\"| DefaultBackend\n        Environment -.->|\"if not specified\"| DefaultEnv\n    end\n```\n\n**Diagram: RLM Initialization Parameters**\n\nThe `backend` parameter accepts string literals like `\"openai\"`, `\"anthropic\"`, `\"portkey\"`, `\"litellm\"`, `\"gemini\"`, or `\"azure_openai\"`. The `environment` parameter accepts `\"local\"`, `\"docker\"`, `\"modal\"`, `\"prime\"`, or `\"daytona\"`. Both have corresponding `_kwargs` dictionaries that pass configuration to the underlying client or environment implementation.\n\n**Sources:** [rlm/core/rlm.py](), [rlm/core/types.py](), [README.md:66-87]()\n\n---\n\n## Quick Start Workflow\n\nThe quickest path to a working RLM setup follows these steps:\n\n```mermaid\ngraph TB\n    Start[\"Start\"] --> InstallUV[\"Install uv package manager<br/>curl -LsSf https://astral.sh/uv/install.sh | sh\"]\n    \n    InstallUV --> InitEnv[\"Initialize environment<br/>uv init && uv venv --python 3.12\"]\n    \n    InitEnv --> InstallDeps[\"Install dependencies<br/>uv pip install -e .\"]\n    \n    InstallDeps --> SetAPIKey[\"Set API key environment variable<br/>export OPENAI_API_KEY=...\"]\n    \n    SetAPIKey --> RunQuickstart[\"Run quickstart example<br/>make quickstart\"]\n    \n    RunQuickstart --> CheckOutput[\"Check console output<br/>and log files\"]\n    \n    CheckOutput --> End[\"RLM is working\"]\n    \n    InstallDeps -.->|\"optional\"| InstallModal[\"Install Modal support<br/>uv pip install -e .[modal]<br/>modal setup\"]\n    \n    InstallDeps -.->|\"optional\"| InstallPrime[\"Install Prime support<br/>uv pip install -e .[prime]<br/>export PRIME_API_KEY=...\"]\n```\n\n**Diagram: Quick Start Installation Flow**\n\nThe default configuration uses the `LocalREPL` environment (no additional setup required) and the `OpenAIClient` backend (requires `OPENAI_API_KEY` environment variable). The `make quickstart` command executes [examples/quickstart.py]() which demonstrates a basic completion request.\n\n**Sources:** [README.md:48-64](), [Makefile:37-38]()\n\n---\n\n## Available Make Commands\n\nThe project includes a `Makefile` that provides convenient commands for common tasks:\n\n| Command | Purpose | Requirements |\n|---------|---------|--------------|\n| `make install` | Install base dependencies with uv sync | None |\n| `make install-dev` | Install development dependencies (ruff, pytest, pre-commit) | None |\n| `make install-modal` | Install Modal sandbox support | Modal account |\n| `make quickstart` | Run basic OpenAI example | `OPENAI_API_KEY` |\n| `make docker-repl` | Run Docker environment example | Docker installed |\n| `make modal-repl` | Run Modal sandbox example | Modal setup complete |\n| `make check` | Run linter, formatter, and tests | Dev dependencies |\n\nThe `make run-all` command executes all examples sequentially, but requires all dependencies and API keys to be configured.\n\n**Sources:** [Makefile:1-58]()\n\n---\n\n## Next Steps\n\nAfter completing the installation and running a basic example, you can:\n\n- Explore different LLM backends by changing the `backend` parameter (see [OpenAI and Compatible Clients](#4.2), [Portkey Client](#4.3), [Other Backends](#4.4))\n- Try containerized execution with `DockerREPL` (see [DockerREPL](#5.3))\n- Experiment with cloud-based isolation using Modal or Prime sandboxes (see [Cloud Sandbox Environments](#5.4))\n- Enable logging to track execution trajectories with `RLMLogger` (see [RLM Logger and Verbose Output](#6.1))\n- Configure multi-turn conversations with persistent environments (see [Persistent Environments and Multi-Turn Sessions](#8.2))\n\nFor detailed installation instructions including troubleshooting and advanced configuration, proceed to [Installation and Setup](#2.1). For a complete walkthrough of the quickstart example with line-by-line explanation, see [Quick Start Example](#2.2).\n\n**Sources:** [README.md:79-161]()\n\n---\n\n# Page: Installation and Setup\n\n# Installation and Setup\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [Makefile](Makefile)\n- [README.md](README.md)\n- [pyproject.toml](pyproject.toml)\n- [uv.lock](uv.lock)\n\n</details>\n\n\n\nThis page provides detailed instructions for installing the RLM system and configuring the necessary dependencies and API keys. It covers the base installation using `uv`, optional dependencies for specific execution environments, and API key setup for various LLM providers.\n\nFor information about running your first RLM example after installation, see [Quick Start Example](#2.2). For details about specific execution environments (Modal, Prime, Daytona), see [Cloud Sandbox Environments](#5.4).\n\n---\n\n## Prerequisites\n\nThe RLM system requires Python 3.11 or higher and uses `uv` (not `pip`) as its package manager. `uv` provides faster, more reliable dependency resolution with deterministic installs.\n\n### Python Version\n\nRLM requires **Python 3.11 or later**, as specified in [pyproject.toml:9](). The codebase uses features like pattern matching and improved type hints that are not available in earlier Python versions.\n\n### Installing uv\n\n`uv` is the required package manager for this project. Install it using the official installation script:\n\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\nAfter installation, initialize a Python virtual environment:\n\n```bash\nuv init && uv venv --python 3.12  # or specify your Python version\n```\n\n**Sources:** [README.md:49-53](), [pyproject.toml:9]()\n\n---\n\n## Installation Methods\n\n### Base Installation\n\nThe base installation includes core dependencies for all LLM client backends and the default `LocalREPL` execution environment.\n\n```bash\nuv sync\n```\n\nAlternatively, using the Makefile:\n\n```bash\nmake install\n```\n\nThis installs the following core dependencies as defined in [pyproject.toml:10-19]():\n\n| Dependency | Version | Purpose |\n|------------|---------|---------|\n| `anthropic` | ≥0.75.0 | Anthropic API client |\n| `google-genai` | ≥1.56.0 | Google Gemini API client |\n| `openai` | ≥2.14.0 | OpenAI API client (also supports OpenRouter, vLLM) |\n| `portkey-ai` | ≥2.1.0 | Portkey router client |\n| `pytest` | ≥9.0.2 | Test framework |\n| `python-dotenv` | ≥1.2.1 | Environment variable management |\n| `requests` | ≥2.32.5 | HTTP library |\n| `rich` | ≥13.0.0 | Console output formatting |\n\n**Sources:** [pyproject.toml:10-19](), [Makefile:26-27](), [README.md:48-54]()\n\n### Development Installation\n\nFor contributors or developers who need linting, formatting, and testing tools:\n\n```bash\nuv sync --group dev --group test\n```\n\nOr via Makefile:\n\n```bash\nmake install-dev\n```\n\nThis adds development dependencies defined in [pyproject.toml:33-43]():\n\n| Dependency Group | Tools | Purpose |\n|-----------------|-------|---------|\n| `dev` | `pre-commit`, `ruff`, `ty` | Linting, formatting, type checking |\n| `test` | `pytest`, `pytest-asyncio`, `pytest-cov` | Testing framework and async support |\n\n**Sources:** [pyproject.toml:33-43](), [Makefile:29-30]()\n\n### Optional Environment Dependencies\n\nRLM supports multiple execution environments beyond the default `LocalREPL`. Each requires additional dependencies.\n\n#### Modal Sandboxes\n\nTo use [Modal Sandboxes](https://modal.com/docs/guide/sandboxes) for isolated execution:\n\n```bash\nuv add modal\nmodal setup  # authenticate your Modal account\n```\n\nOr install with optional dependencies:\n\n```bash\nuv pip install -e \".[modal]\"\n```\n\nThis installs [pyproject.toml:22]():\n- `modal` ≥0.73.0\n- `dill` ≥0.3.7 (for state serialization)\n\n**Sources:** [README.md:98-103](), [pyproject.toml:22](), [Makefile:32-33]()\n\n#### Prime Intellect Sandboxes\n\nTo use [Prime Sandboxes](https://docs.primeintellect.ai/sandboxes/sdk):\n\n```bash\nuv pip install -e \".[prime]\"\nexport PRIME_API_KEY=your_api_key_here\n```\n\nThis installs [pyproject.toml:24]():\n- `prime-sandboxes` ≥0.2.0\n- `dill` ≥0.3.7\n\nNote: Prime Sandboxes are currently in beta and may experience slower runtimes.\n\n**Sources:** [README.md:105-114](), [pyproject.toml:24]()\n\n#### Daytona Workspaces\n\nTo use Daytona workspaces:\n\n```bash\nuv pip install -e \".[daytona]\"\n```\n\nThis installs [pyproject.toml:23]():\n- `daytona` ≥0.128.1\n- `dill` ≥0.3.7\n\n**Sources:** [pyproject.toml:23]()\n\n---\n\n## Installation Architecture\n\nThe following diagram shows the dependency structure and installation flow:\n\n```mermaid\ngraph TB\n    subgraph \"Installation Commands\"\n        UVSync[\"uv sync<br/>make install\"]\n        UVSyncDev[\"uv sync --group dev --group test<br/>make install-dev\"]\n        UVModal[\"uv pip install -e '.[modal]'<br/>make install-modal\"]\n        UVPrime[\"uv pip install -e '.[prime]'\"]\n        UVDaytona[\"uv pip install -e '.[daytona]'\"]\n    end\n    \n    subgraph \"Core Dependencies [pyproject.toml:10-19]\"\n        Anthropic[\"anthropic>=0.75.0\"]\n        OpenAI[\"openai>=2.14.0\"]\n        GoogleGenAI[\"google-genai>=1.56.0\"]\n        Portkey[\"portkey-ai>=2.1.0\"]\n        Rich[\"rich>=13.0.0\"]\n        Pytest[\"pytest>=9.0.2\"]\n        Dotenv[\"python-dotenv>=1.2.1\"]\n        Requests[\"requests>=2.32.5\"]\n    end\n    \n    subgraph \"Dev Dependencies [pyproject.toml:34-43]\"\n        PreCommit[\"pre-commit>=4.5.1\"]\n        Ruff[\"ruff>=0.14.10\"]\n        PytestAsync[\"pytest-asyncio>=0.24.0\"]\n        PytestCov[\"pytest-cov>=6.0.0\"]\n    end\n    \n    subgraph \"Optional: Modal [pyproject.toml:22]\"\n        Modal[\"modal>=0.73.0\"]\n        DillModal[\"dill>=0.3.7\"]\n    end\n    \n    subgraph \"Optional: Prime [pyproject.toml:24]\"\n        Prime[\"prime-sandboxes>=0.2.0\"]\n        DillPrime[\"dill>=0.3.7\"]\n    end\n    \n    subgraph \"Optional: Daytona [pyproject.toml:23]\"\n        Daytona[\"daytona>=0.128.1\"]\n        DillDaytona[\"dill>=0.3.7\"]\n    end\n    \n    subgraph \"Lock File\"\n        UVLock[\"uv.lock<br/>SHA256 hashes<br/>Exact versions\"]\n    end\n    \n    UVSync --> Anthropic\n    UVSync --> OpenAI\n    UVSync --> GoogleGenAI\n    UVSync --> Portkey\n    UVSync --> Rich\n    UVSync --> Pytest\n    UVSync --> Dotenv\n    UVSync --> Requests\n    \n    UVSyncDev --> PreCommit\n    UVSyncDev --> Ruff\n    UVSyncDev --> PytestAsync\n    UVSyncDev --> PytestCov\n    \n    UVModal --> Modal\n    UVModal --> DillModal\n    \n    UVPrime --> Prime\n    UVPrime --> DillPrime\n    \n    UVDaytona --> Daytona\n    UVDaytona --> DillDaytona\n    \n    UVSync -.-> UVLock\n    UVSyncDev -.-> UVLock\n    UVModal -.-> UVLock\n    UVPrime -.-> UVLock\n    UVDaytona -.-> UVLock\n```\n\n**Dependency Resolution Flow**\n\nThis diagram shows how installation commands map to dependency groups and how `uv.lock` ensures reproducible installs.\n\n**Sources:** [pyproject.toml:1-44](), [uv.lock:1-3](), [Makefile:26-33]()\n\n---\n\n## API Key Configuration\n\nRLM supports multiple LLM providers through the `backend` parameter. Each provider requires its own API key to be set as an environment variable.\n\n### Supported Backends and API Keys\n\nThe following table lists all supported backends and their required environment variables:\n\n| Backend | Environment Variable | Client Class | Notes |\n|---------|---------------------|--------------|-------|\n| `openai` | `OPENAI_API_KEY` | `OpenAIClient` | Default backend |\n| `anthropic` | `ANTHROPIC_API_KEY` | `AnthropicClient` | Claude models |\n| `portkey` | `PORTKEY_API_KEY` | `PortkeyClient` | LLM router/gateway |\n| `litellm` | Provider-specific | `LiteLLMClient` | Unified interface |\n| `azure_openai` | `AZURE_OPENAI_API_KEY` | `AzureOpenAIClient` | Azure-hosted OpenAI |\n| `gemini` | `GEMINI_API_KEY` | `GeminiClient` | Google Gemini models |\n\n**OpenAI-Compatible Services:** The `OpenAIClient` at [rlm/clients/openai.py]() also supports OpenRouter, Vercel AI Gateway, vLLM, and Prime Intellect by configuring the `base_url` parameter.\n\n**Sources:** [pyproject.toml:10-14](), [README.md:117-118]()\n\n### Setting API Keys\n\nAPI keys can be set in three ways:\n\n#### 1. Environment Variables (Recommended)\n\n```bash\nexport OPENAI_API_KEY=\"sk-...\"\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"\nexport PORTKEY_API_KEY=\"...\"\n```\n\n#### 2. `.env` File\n\nCreate a `.env` file in your project root:\n\n```\nOPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\nPORTKEY_API_KEY=...\nPRIME_API_KEY=...\n```\n\nThe `python-dotenv` package (installed by default) automatically loads these variables.\n\n#### 3. Runtime Configuration\n\nPass API keys directly when initializing the RLM client:\n\n```python\nfrom rlm import RLM\n\nrlm = RLM(\n    backend=\"openai\",\n    backend_kwargs={\n        \"api_key\": \"sk-...\",\n        \"model_name\": \"gpt-4\"\n    }\n)\n```\n\n**Sources:** [pyproject.toml:16](), [README.md:61-64]()\n\n### API Key Configuration Flow\n\n```mermaid\ngraph LR\n    subgraph \"Configuration Sources\"\n        EnvVars[\"Environment Variables<br/>export OPENAI_API_KEY=...\"]\n        DotEnv[\".env File<br/>OPENAI_API_KEY=...\"]\n        RuntimeKwargs[\"backend_kwargs<br/>{'api_key': '...'}\"]\n    end\n    \n    subgraph \"Loading Mechanism\"\n        PythonDotenv[\"python-dotenv<br/>[pyproject.toml:16]\"]\n        OSEnviron[\"os.environ\"]\n    end\n    \n    subgraph \"RLM Initialization\"\n        RLMClass[\"RLM.__init__()<br/>[rlm/core/rlm.py]\"]\n        GetClient[\"get_client()<br/>[rlm/clients/__init__.py]\"]\n    end\n    \n    subgraph \"Client Classes\"\n        OpenAIClient[\"OpenAIClient<br/>[rlm/clients/openai.py]\"]\n        AnthropicClient[\"AnthropicClient<br/>[rlm/clients/anthropic.py]\"]\n        PortkeyClient[\"PortkeyClient<br/>[rlm/clients/portkey.py]\"]\n        OtherClients[\"GeminiClient, LiteLLMClient, etc.\"]\n    end\n    \n    EnvVars --> OSEnviron\n    DotEnv --> PythonDotenv\n    PythonDotenv --> OSEnviron\n    RuntimeKwargs --> RLMClass\n    \n    OSEnviron --> RLMClass\n    RLMClass --> GetClient\n    \n    GetClient --> OpenAIClient\n    GetClient --> AnthropicClient\n    GetClient --> PortkeyClient\n    GetClient --> OtherClients\n```\n\n**API Key Loading Priority**\n\nThis diagram shows how API keys flow from configuration sources through the loading mechanism to the appropriate client implementation.\n\n**Sources:** [pyproject.toml:16](), [rlm/clients/__init__.py](), [rlm/clients/openai.py](), [rlm/clients/anthropic.py]()\n\n---\n\n## Makefile Convenience Commands\n\nThe [Makefile:1-59]() provides shortcuts for common installation and development tasks:\n\n### Installation Commands\n\n| Command | Equivalent `uv` Command | Purpose |\n|---------|------------------------|---------|\n| `make install` | `uv sync` | Install base dependencies |\n| `make install-dev` | `uv sync --group dev --group test` | Install dev dependencies |\n| `make install-modal` | `uv pip install -e \".[modal]\"` | Install Modal dependencies |\n\n### Development Commands\n\n| Command | Purpose |\n|---------|---------|\n| `make lint` | Run `ruff check .` |\n| `make format` | Run `ruff format .` |\n| `make test` | Run `pytest` |\n| `make check` | Run lint + format + test |\n\n### Example Execution Commands\n\n| Command | Runs | Requirements |\n|---------|------|-------------|\n| `make quickstart` | `examples/quickstart.py` | `OPENAI_API_KEY` |\n| `make docker-repl` | `examples/docker_repl_example.py` | Docker installed |\n| `make modal-repl` | `examples/modal_repl_example.py` | Modal setup |\n| `make run-all` | All examples | All dependencies + API keys |\n\n**Sources:** [Makefile:1-59]()\n\n---\n\n## Verification\n\nAfter installation, verify your setup by checking installed packages and running a basic test.\n\n### Check Installed Packages\n\n```bash\nuv pip list | grep -E \"(rlm|anthropic|openai|portkey)\"\n```\n\nExpected output should show:\n- `rlm` (version 0.1.0)\n- Core dependencies with versions matching [pyproject.toml:10-19]()\n\n### Run Import Test\n\n```bash\nuv run python -c \"from rlm import RLM; print('✓ RLM imported successfully')\"\n```\n\n### Run Quickstart Example\n\nWith `OPENAI_API_KEY` configured:\n\n```bash\nmake quickstart\n```\n\nThis runs [examples/quickstart.py]() which performs a basic RLM completion using the OpenAI backend and `LocalREPL` environment. Successful execution indicates that:\n- Dependencies are correctly installed\n- API key is properly configured\n- The default `LocalREPL` environment is functional\n\n**Sources:** [README.md:61-64](), [Makefile:37-38](), [examples/quickstart.py]()\n\n---\n\n## Dependency Lock File\n\nThe [uv.lock:1-3]() file ensures reproducible installations by pinning exact versions and SHA256 hashes of all dependencies. This file is automatically managed by `uv` and should be committed to version control.\n\n### Lock File Structure\n\n```mermaid\ngraph TB\n    subgraph \"uv.lock Format\"\n        Version[\"version = 1<br/>revision = 3\"]\n        PythonVersion[\"requires-python = '>=3.11'\"]\n        Packages[\"[[package]]<br/>name, version, source<br/>dependencies<br/>sdist, wheels\"]\n    end\n    \n    subgraph \"Package Entries\"\n        Anthropic[\"anthropic-0.75.0<br/>SHA256: ea831...]\n        OpenAI[\"openai-2.14.0<br/>SHA256: 7a832...]\n        Modal[\"modal-0.73.0<br/>SHA256: b4ec5...\"]\n        Dill[\"dill-0.3.7<br/>SHA256: 22a07...\"]\n    end\n    \n    subgraph \"Dependency Resolution\"\n        DirectDeps[\"Direct dependencies<br/>from pyproject.toml\"]\n        TransitiveDeps[\"Transitive dependencies<br/>auto-resolved\"]\n        PlatformWheels[\"Platform-specific wheels<br/>cp311-macosx, manylinux, win32\"]\n    end\n    \n    Version --> Packages\n    PythonVersion --> Packages\n    \n    Packages --> Anthropic\n    Packages --> OpenAI\n    Packages --> Modal\n    Packages --> Dill\n    \n    DirectDeps --> Packages\n    TransitiveDeps --> Packages\n    Packages --> PlatformWheels\n```\n\n**Lock File Structure**\n\nThe lock file contains metadata, package entries with SHA256 hashes, and platform-specific wheels for reproducible cross-platform installs.\n\n**Sources:** [uv.lock:1-3]()\n\n---\n\n## Summary\n\nTo get started with RLM:\n\n1. **Install `uv`**: `curl -LsSf https://astral.sh/uv/install.sh | sh`\n2. **Install base dependencies**: `uv sync` or `make install`\n3. **Set API keys**: `export OPENAI_API_KEY=...`\n4. **Verify installation**: `make quickstart`\n\nFor development work, use `make install-dev`. For specific execution environments (Modal, Prime, Daytona), install their optional dependencies using `uv pip install -e \".[modal]\"` or equivalent commands.\n\nThe next step is to run your first RLM completion. See [Quick Start Example](#2.2) for a walkthrough.\n\n**Sources:** [README.md:48-64](), [pyproject.toml:1-44](), [Makefile:1-59]()\n\n---\n\n# Page: Quick Start Example\n\n# Quick Start Example\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [Makefile](Makefile)\n- [README.md](README.md)\n- [examples/daytona_repl_example.py](examples/daytona_repl_example.py)\n- [examples/quickstart.py](examples/quickstart.py)\n- [rlm/environments/daytona_repl.py](rlm/environments/daytona_repl.py)\n\n</details>\n\n\n\nThis page walks through a minimal working example of creating an `RLM` instance and making your first completion request. The example demonstrates basic initialization, setting up logging, and executing a simple task that requires code generation. For detailed installation instructions, see [Installation and Setup](#2.1). For comprehensive documentation of the `RLM` class and its parameters, see [RLM Class](#3.1).\n\n## Minimal Example\n\nThe simplest RLM usage requires only a backend specification and a prompt. Here is the minimal example from the README:\n\n[README.md:66-77]()\n\nThis example initializes an `RLM` instance with:\n- `backend=\"openai\"`: Uses the OpenAI client (requires `OPENAI_API_KEY` environment variable)\n- `backend_kwargs`: Specifies the model name (here, `\"gpt-5-nano\"`)\n- `verbose=True`: Enables rich console output with the Tokyo Night theme\n\nThe `rlm.completion()` call processes the prompt through an iteration loop where the LM generates code, the code executes in a `LocalREPL` environment, and the process repeats until a final answer is produced.\n\n**Sources:** [README.md:66-77]()\n\n## Complete Working Example\n\nThe repository includes a complete example in `examples/quickstart.py` that demonstrates best practices:\n\n[examples/quickstart.py:1-28]()\n\nThis example adds:\n- Environment variable loading via `dotenv`\n- `RLMLogger` initialization for trajectory capture\n- Explicit API key specification\n- Docker-based execution environment\n- Maximum recursion depth configuration\n\n**Sources:** [examples/quickstart.py:1-28]()\n\n## Quick Start Execution Flow\n\nThe following diagram shows the execution path for the quick start example, mapping from user code to internal components:\n\n```mermaid\ngraph TB\n    User[\"User Code<br/>examples/quickstart.py\"]\n    \n    subgraph \"RLM Initialization\"\n        RLMClass[\"RLM class<br/>rlm.core.rlm.RLM\"]\n        ClientFactory[\"get_client()<br/>rlm.clients.__init__\"]\n        EnvFactory[\"get_environment()<br/>rlm.environments.__init__\"]\n        LMHandler[\"LMHandler<br/>rlm.core.lm_handler.LMHandler\"]\n        Logger[\"RLMLogger<br/>rlm.logger.rlm_logger.RLMLogger\"]\n    end\n    \n    subgraph \"Backend Instantiation\"\n        OpenAIClient[\"OpenAIClient<br/>rlm.clients.openai.OpenAIClient\"]\n    end\n    \n    subgraph \"Environment Instantiation\"\n        DockerREPL[\"DockerREPL<br/>rlm.environments.docker_repl.DockerREPL\"]\n        Container[\"Docker Container<br/>python:3.11-slim\"]\n    end\n    \n    subgraph \"Completion Request\"\n        CompletionMethod[\"rlm.completion()<br/>rlm.core.rlm.RLM.completion\"]\n        IterationLoop[\"Iteration Loop<br/>max_iterations=10\"]\n        CodeExtraction[\"Code Extraction<br/>extract_code_blocks()\"]\n        CodeExecution[\"Code Execution<br/>docker_repl.execute_code()\"]\n    end\n    \n    subgraph \"Result\"\n        RLMChatCompletion[\"RLMChatCompletion<br/>rlm.core.types.RLMChatCompletion\"]\n        LogFile[\"Log File<br/>./logs/*.jsonl\"]\n        ConsoleOutput[\"Console Output<br/>VerbosePrinter\"]\n    end\n    \n    User -->|\"1. Initialize\"| RLMClass\n    RLMClass -->|\"2. Create backend\"| ClientFactory\n    ClientFactory -->|\"3. Instantiate\"| OpenAIClient\n    RLMClass -->|\"4. Create environment\"| EnvFactory\n    EnvFactory -->|\"5. Instantiate\"| DockerREPL\n    DockerREPL -->|\"6. Launch\"| Container\n    RLMClass -->|\"7. Start handler\"| LMHandler\n    RLMClass -->|\"8. Set logger\"| Logger\n    \n    User -->|\"9. Call\"| CompletionMethod\n    CompletionMethod -->|\"10. Enter\"| IterationLoop\n    IterationLoop -->|\"11. Request LM\"| LMHandler\n    LMHandler -->|\"12. Forward\"| OpenAIClient\n    OpenAIClient -->|\"13. Response\"| IterationLoop\n    IterationLoop -->|\"14. Extract\"| CodeExtraction\n    CodeExtraction -->|\"15. Execute\"| CodeExecution\n    CodeExecution -->|\"16. Results\"| IterationLoop\n    IterationLoop -->|\"17. Check FINAL_VAR()\"| IterationLoop\n    IterationLoop -->|\"18. Return\"| RLMChatCompletion\n    \n    RLMChatCompletion -->|\"19. Log\"| LogFile\n    RLMChatCompletion -->|\"20. Print\"| ConsoleOutput\n    RLMChatCompletion -->|\"21. Return\"| User\n```\n\n**Sources:** [examples/quickstart.py:1-28](), [rlm/core/rlm.py:1-300](), [rlm/clients/__init__.py:1-50](), [rlm/environments/__init__.py:1-50]()\n\n## Component Breakdown\n\n### RLM Class Initialization\n\nThe `RLM` constructor accepts several key parameters:\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `backend` | `str` | `\"openai\"` | LLM client backend identifier (see [LLM Client Backends](#4)) |\n| `backend_kwargs` | `dict` | `{}` | Configuration passed to the client (e.g., `model_name`, `api_key`) |\n| `environment` | `str` | `\"local\"` | Execution environment type (see [Execution Environments](#5)) |\n| `environment_kwargs` | `dict` | `{}` | Configuration passed to the environment |\n| `max_depth` | `int` | `1` | Maximum recursion depth for sub-LM calls |\n| `max_iterations` | `int` | `10` | Maximum reasoning iterations before stopping |\n| `logger` | `RLMLogger` | `None` | Optional logger for trajectory capture |\n| `verbose` | `bool` | `False` | Enable rich console output |\n\nThe constructor internally:\n1. Calls `get_client(backend, **backend_kwargs)` to instantiate the LM client [rlm/clients/__init__.py:15-45]()\n2. Calls `get_environment(environment, **environment_kwargs)` to create the execution environment [rlm/environments/__init__.py:15-60]()\n3. Initializes an `LMHandler` TCP server for routing LM requests [rlm/core/lm_handler.py:30-100]()\n4. Sets up the `VerbosePrinter` if `verbose=True` [rlm/logger/verbose_printer.py:1-200]()\n\n**Sources:** [rlm/core/rlm.py:40-150](), [rlm/clients/__init__.py:15-45](), [rlm/environments/__init__.py:15-60]()\n\n### Backend Configuration\n\nThe `backend_kwargs` parameter is a dictionary passed directly to the client constructor. For the OpenAI client:\n\n- `model_name` (str): Model identifier (e.g., `\"gpt-4\"`, `\"gpt-3.5-turbo\"`)\n- `api_key` (str): API key (defaults to `OPENAI_API_KEY` environment variable)\n- `base_url` (str): Custom API endpoint for OpenAI-compatible APIs\n- `temperature` (float): Sampling temperature\n- `max_tokens` (int): Maximum tokens in response\n\n[rlm/clients/openai.py:20-80]()\n\n**Sources:** [rlm/clients/openai.py:20-80](), [examples/quickstart.py:14-17]()\n\n### Environment Configuration\n\nThe quick start example uses `environment=\"docker\"`, which creates a `DockerREPL` instance. The `environment_kwargs` can specify:\n\n- `image` (str): Docker image name (default: `\"python:3.11-slim\"`)\n- `volumes` (dict): Volume mounts for file access\n- `network` (str): Docker network mode\n- `persistent` (bool): Whether to maintain state across calls\n\n[rlm/environments/docker_repl.py:30-120]()\n\nFor local testing without Docker, omit the `environment` parameter to use the default `LocalREPL`:\n\n```python\nrlm = RLM(backend=\"openai\", backend_kwargs={\"model_name\": \"gpt-4\"})\n```\n\n**Sources:** [rlm/environments/docker_repl.py:30-120](), [examples/quickstart.py:18-19]()\n\n### Logger Setup\n\nThe `RLMLogger` creates `.jsonl` log files that capture the complete execution trace:\n\n[examples/quickstart.py:10]()\n\nEach completion generates a timestamped log file in `./logs/` containing:\n- `RLMMetadata`: Root model, max depth, max iterations, backend configuration\n- `RLMIteration[]`: Array of iterations, each with prompts, responses, code blocks, and execution results\n- `RLMChatCompletion`: Nested sub-LM calls with full recursion tree\n\nThese log files can be visualized using the trajectory visualizer (see [Trajectory Visualizer](#6.3)).\n\n**Sources:** [examples/quickstart.py:10](), [rlm/logger/rlm_logger.py:1-150]()\n\n## Running the Example\n\nThe repository includes a Makefile target for running the quick start example:\n\n```bash\nmake quickstart\n```\n\nThis target executes:\n[Makefile:37-38]()\n\nAlternatively, run directly with `uv`:\n\n```bash\nuv run python -m examples.quickstart\n```\n\n**Requirements:**\n- `OPENAI_API_KEY` environment variable set (or API key specified in code)\n- Docker installed and running (if using `environment=\"docker\"`)\n- Base dependencies installed via `uv sync` (see [Installation and Setup](#2.1))\n\n**Sources:** [Makefile:37-38](), [examples/quickstart.py:1-28]()\n\n## Understanding the Output\n\n### Console Output (verbose=True)\n\nWith `verbose=True`, the system prints rich-formatted output to the console:\n\n1. **Iteration Header**: Shows current iteration number and total\n2. **LM Request**: Displays the prompt sent to the language model\n3. **LM Response**: Shows the raw response from the model\n4. **Code Blocks**: Extracted Python code blocks with syntax highlighting\n5. **Execution Results**: stdout/stderr from code execution\n6. **Sub-LM Calls**: Nested calls with indentation showing recursion depth\n7. **Final Answer**: The extracted final answer (if present)\n\nThe output uses the Tokyo Night color theme for readability. This is implemented in [rlm/logger/verbose_printer.py:1-300]().\n\n**Sources:** [rlm/logger/verbose_printer.py:1-300](), [README.md:73]()\n\n### Return Value Structure\n\nThe `rlm.completion()` method returns an `RLMChatCompletion` object:\n\n```python\nresult = rlm.completion(prompt)\nprint(result.response)          # Final answer string\nprint(result.prompt)            # Original prompt\nprint(result.usage_summary)     # Token usage across all calls\nprint(result.execution_time)    # Total time in seconds\n```\n\nThe `RLMChatCompletion` type is defined in [rlm/core/types.py:1-100]() and contains:\n- `root_model` (str): The primary model used\n- `prompt` (str): The original prompt\n- `response` (str): The final answer extracted from execution\n- `usage_summary` (UsageSummary): Aggregated token counts and costs\n- `execution_time` (float): Total execution time\n- `final_answer` (str | None): Raw final answer if found\n\n**Sources:** [rlm/core/types.py:50-120](), [examples/quickstart.py:25-27]()\n\n## Code-to-Execution Mapping\n\nThe following diagram shows how the quick start code maps to actual execution steps:\n\n```mermaid\nsequenceDiagram\n    participant User as \"User Code\"\n    participant RLM as \"RLM Instance\"\n    participant Handler as \"LMHandler Server\"\n    participant Client as \"OpenAIClient\"\n    participant Env as \"DockerREPL\"\n    participant Docker as \"Docker Container\"\n    \n    User->>RLM: \"RLM(__init__)\"\n    RLM->>Client: \"get_client('openai')\"\n    RLM->>Env: \"get_environment('docker')\"\n    Env->>Docker: \"docker run python:3.11-slim\"\n    RLM->>Handler: \"LMHandler(client)\"\n    Handler-->>RLM: \"address: (host, port)\"\n    RLM->>Env: \"update_handler_address()\"\n    \n    User->>RLM: \"completion(prompt)\"\n    \n    loop \"Iteration 0 to max_iterations\"\n        RLM->>Handler: \"LMRequest(prompt, depth=0)\"\n        Handler->>Client: \"completion(prompt)\"\n        Client->>Client: \"OpenAI API call\"\n        Client-->>Handler: \"response text\"\n        Handler-->>RLM: \"LMResponse\"\n        \n        RLM->>RLM: \"extract_code_blocks()\"\n        \n        loop \"For each code block\"\n            RLM->>Env: \"execute_code(code)\"\n            Env->>Docker: \"exec: python script\"\n            \n            alt \"Code contains llm_query()\"\n                Docker->>Handler: \"Socket: LMRequest(depth=1)\"\n                Handler->>Client: \"completion(sub-prompt)\"\n                Client-->>Handler: \"sub-response\"\n                Handler-->>Docker: \"Socket: LMResponse\"\n            end\n            \n            Docker-->>Env: \"REPLResult(stdout, stderr, locals)\"\n            Env-->>RLM: \"REPLResult + rlm_calls[]\"\n        end\n        \n        alt \"FINAL_VAR found in stdout\"\n            RLM-->>User: \"RLMChatCompletion\"\n        end\n    end\n    \n    RLM-->>User: \"RLMChatCompletion\"\n```\n\n**Key execution steps:**\n\n1. **Initialization** (lines 12-23 of quickstart.py): Creates all components and starts the LM handler server\n2. **Completion request** (line 25): Enters the iteration loop in [rlm/core/rlm.py:200-350]()\n3. **LM invocation**: Handler routes requests to the appropriate client based on depth\n4. **Code extraction**: Uses regex to find code blocks in LM response [rlm/core/rlm.py:150-180]()\n5. **Execution**: Environment runs code with access to `llm_query()` functions\n6. **Sub-calls**: Nested LM calls increment depth and route to appropriate clients\n7. **Final answer**: Loop terminates when `FINAL_VAR()` is found or max iterations reached\n\n**Sources:** [examples/quickstart.py:12-25](), [rlm/core/rlm.py:200-350](), [rlm/environments/docker_repl.py:200-400]()\n\n## Next Steps\n\nAfter running the quick start example:\n\n1. **Explore environments**: Try different execution environments (see [Execution Environments](#5))\n   - `environment=\"local\"` for faster local execution\n   - `environment=\"modal\"` for cloud-based isolation\n   - `environment=\"prime\"` for Prime Intellect sandboxes\n\n2. **Configure backends**: Experiment with different LLM providers (see [LLM Client Backends](#4))\n   - Anthropic: `backend=\"anthropic\"`\n   - Portkey: `backend=\"portkey\"`\n   - LiteLLM: `backend=\"litellm\"`\n\n3. **Enable multi-depth**: Configure hierarchical model selection (see [Multi-Backend Configuration](#8.1))\n   ```python\n   rlm = RLM(\n       backend=\"openai\",\n       backend_kwargs={\"model_name\": \"gpt-4\"},\n       other_backends=[(\"openai\", {\"model_name\": \"gpt-3.5-turbo\"})],\n       max_depth=2\n   )\n   ```\n\n4. **Visualize trajectories**: Use the trajectory visualizer to inspect execution traces (see [Trajectory Visualizer](#6.3))\n   ```bash\n   cd visualizer/\n   npm run dev\n   ```\n\n5. **Understand core architecture**: Read about the RLM orchestrator, LM handler, and recursion model (see [Core Architecture](#3))\n\n**Sources:** [README.md:79-116](), [examples/]()\n\n---\n\n# Page: Core Architecture\n\n# Core Architecture\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [Makefile](Makefile)\n- [README.md](README.md)\n- [rlm/clients/__init__.py](rlm/clients/__init__.py)\n- [rlm/clients/azure_openai.py](rlm/clients/azure_openai.py)\n- [rlm/core/rlm.py](rlm/core/rlm.py)\n- [rlm/core/types.py](rlm/core/types.py)\n\n</details>\n\n\n\n## Overview\n\nThe RLM system implements a task-agnostic inference paradigm that enables language models to handle arbitrarily long contexts by offloading context to a REPL environment where the LM can programmatically examine, decompose, and recursively invoke itself. The architecture consists of four primary layers:\n\n1. **Core Orchestration** - The `RLM` class [rlm/core/rlm.py:32-400]() orchestrates iterative LLM-code-LLM execution loops\n2. **LLM Communication** - The `LMHandler` class manages multi-threaded TCP communication with LLM backends and routes depth-based requests\n3. **Execution Environments** - Pluggable REPL environments (`LocalREPL`, `DockerREPL`, `ModalREPL`, `PrimeREPL`, `DaytonaREPL`) execute code with varying isolation levels\n4. **Data Layer** - Core types [rlm/core/types.py]() and communication utilities structure data flow and usage tracking\n\nThis page provides an architectural overview of how these components integrate. For detailed documentation of individual components:\n- RLM class implementation: see page **3.1**\n- LM Handler communication layer: see page **3.2**\n- REPL environment abstractions: see page **3.3**\n- Recursive call mechanisms: see page **3.4**\n- Communication protocols and data types: see page **3.5**\n\n**Sources:** [rlm/core/rlm.py:32-400](), [rlm/core/types.py](), README.md\n\n## System Architecture Diagram\n\nThe following diagram shows the complete RLM system architecture, mapping high-level concepts to concrete code entities:\n\n```mermaid\ngraph TB\n    subgraph UserAPI[\"User API Layer\"]\n        User[\"User Code\"]\n        CompletionAPI[\"rlm.completion(prompt)<br/>[rlm/core/rlm.py:192-295]\"]\n    end\n    \n    subgraph CoreOrch[\"Core Orchestration - rlm/core/rlm.py\"]\n        RLMClass[\"RLM class<br/>__init__: lines 40-124<br/>completion: lines 192-295\"]\n        SpawnContext[\"_spawn_completion_context()<br/>lines 125-179\"]\n        CompletionTurn[\"_completion_turn()<br/>lines 297-323\"]\n        SetupPrompt[\"_setup_prompt()<br/>lines 180-191\"]\n    end\n    \n    subgraph LLMComm[\"LLM Communication Layer\"]\n        LMHandler[\"LMHandler class<br/>[rlm/core/lm_handler.py]<br/>TCP Server on host:port\"]\n        ClientRouter[\"get_client() factory<br/>[rlm/clients/__init__.py:11-63]\"]\n        BaseLM[\"BaseLM interface<br/>[rlm/clients/base_lm.py]\"]\n    end\n    \n    subgraph ExecLayer[\"Execution Environment Layer\"]\n        EnvRouter[\"get_environment() factory<br/>[rlm/environments/__init__.py]\"]\n        BaseEnv[\"BaseEnv interface<br/>[rlm/environments/base_env.py]\"]\n        NonIsolated[\"NonIsolatedEnv<br/>Socket Protocol\"]\n        Isolated[\"IsolatedEnv<br/>HTTP Broker Pattern\"]\n    end\n    \n    subgraph DataLayer[\"Data Layer - rlm/core/types.py\"]\n        Types[\"RLMChatCompletion<br/>REPLResult<br/>RLMIteration<br/>CodeBlock\"]\n        Usage[\"UsageSummary<br/>ModelUsageSummary\"]\n        Metadata[\"RLMMetadata<br/>QueryMetadata\"]\n    end\n    \n    subgraph Support[\"Supporting Utilities\"]\n        Logger[\"RLMLogger<br/>[rlm/logger.py]\"]\n        Prompts[\"build_rlm_system_prompt<br/>[rlm/utils/prompts.py]\"]\n        Parsing[\"find_code_blocks<br/>find_final_answer<br/>[rlm/utils/parsing.py]\"]\n    end\n    \n    User --> CompletionAPI\n    CompletionAPI --> RLMClass\n    RLMClass --> SpawnContext\n    SpawnContext --> LMHandler\n    SpawnContext --> EnvRouter\n    RLMClass --> CompletionTurn\n    RLMClass --> Logger\n    RLMClass --> SetupPrompt\n    \n    SetupPrompt --> Prompts\n    CompletionTurn --> LMHandler\n    CompletionTurn --> BaseEnv\n    CompletionTurn --> Parsing\n    \n    LMHandler --> ClientRouter\n    ClientRouter --> BaseLM\n    \n    EnvRouter --> BaseEnv\n    BaseEnv --> NonIsolated\n    BaseEnv --> Isolated\n    \n    NonIsolated -.->|\"Direct socket\"| LMHandler\n    Isolated -.->|\"HTTP broker + polling\"| LMHandler\n    \n    RLMClass --> Types\n    RLMClass --> Usage\n    RLMClass --> Metadata\n    Logger --> Types\n```\n\n**Sources:** [rlm/core/rlm.py:32-400](), [rlm/clients/__init__.py:11-63](), [rlm/core/types.py:1-266](), README.md\n\n## Core Components Overview\n\nThe RLM system architecture consists of four primary subsystems that work together to enable recursive language model execution:\n\n| Component | Primary File(s) | Key Classes/Functions | Purpose |\n|-----------|----------------|----------------------|---------|\n| **Core Orchestration** | `rlm/core/rlm.py` | `RLM` | Orchestrates iterative LLM-code-LLM execution loops with per-completion resource spawning |\n| **Communication Layer** | `rlm/core/lm_handler.py` | `LMHandler` | Multi-threaded TCP server routing depth-based LLM requests from environments |\n| **LLM Backends** | `rlm/clients/*.py` | `BaseLM`, `get_client()`, `OpenAIClient`, `AnthropicClient`, `PortkeyClient` | Unified interface to multiple LLM providers with usage tracking |\n| **Execution Environments** | `rlm/environments/*.py` | `BaseEnv`, `get_environment()`, `NonIsolatedEnv`, `IsolatedEnv` | Code execution with socket/HTTP broker communication protocols |\n| **Data Types** | `rlm/core/types.py` | `RLMChatCompletion`, `REPLResult`, `RLMIteration`, `UsageSummary` | Structured data for execution traces and usage tracking |\n| **Support Systems** | `rlm/logger.py`, `rlm/utils/` | `RLMLogger`, `VerbosePrinter`, `find_code_blocks`, `build_rlm_system_prompt` | Logging, prompting, parsing, and result processing |\n\n**Sources:** [rlm/core/rlm.py:32-400](), [rlm/core/types.py:1-266](), [rlm/clients/__init__.py:11-63](), README.md\n\n## RLM Execution Flow\n\nThe following sequence diagram illustrates a complete RLM execution, showing how components interact during a single `completion()` call:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant RLM[\"RLM<br/>[rlm.py:192-295]\"]\n    participant SpawnCtx[\"_spawn_completion_context<br/>[rlm.py:125-179]\"]\n    participant LMHandler[\"LMHandler<br/>[lm_handler.py]\"]\n    participant Env[\"BaseEnv<br/>(LocalREPL/Docker/Modal/etc)\"]\n    participant Logger[\"RLMLogger\"]\n    \n    User->>RLM: \"completion(prompt)\"\n    activate RLM\n    \n    RLM->>SpawnCtx: \"Enter context manager\"\n    activate SpawnCtx\n    SpawnCtx->>SpawnCtx: \"get_client(backend, kwargs)\"\n    SpawnCtx->>LMHandler: \"LMHandler(client)\"\n    SpawnCtx->>LMHandler: \"start() - spawn TCP server\"\n    activate LMHandler\n    SpawnCtx->>SpawnCtx: \"get_environment(type, kwargs)\"\n    SpawnCtx->>Env: \"Initialize with lm_handler_address\"\n    activate Env\n    SpawnCtx-->>RLM: \"yield handler, environment\"\n    \n    RLM->>RLM: \"_setup_prompt(prompt)\"\n    RLM->>Logger: \"log_metadata(RLMMetadata)\"\n    \n    loop \"i in range(max_iterations)\"\n        RLM->>RLM: \"build_user_prompt(i, context_count)\"\n        RLM->>RLM: \"_completion_turn(prompt)\"\n        RLM->>LMHandler: \"completion(current_prompt)\"\n        LMHandler-->>RLM: \"response: str\"\n        \n        RLM->>RLM: \"find_code_blocks(response)\"\n        \n        loop \"for each code_block_str\"\n            RLM->>Env: \"execute_code(code_block_str)\"\n            \n            opt \"Code calls llm_query() or llm_query_batched()\"\n                Env->>LMHandler: \"Socket/HTTP: LMRequest\"\n                LMHandler->>LMHandler: \"Route by depth to client\"\n                LMHandler-->>Env: \"LMResponse with completion\"\n            end\n            \n            Env-->>RLM: \"REPLResult(stdout, stderr, locals, rlm_calls)\"\n        end\n        \n        RLM->>RLM: \"Create RLMIteration\"\n        RLM->>Logger: \"log(iteration)\"\n        RLM->>RLM: \"find_final_answer(response)\"\n        \n        alt \"final_answer is not None\"\n            RLM->>RLM: \"Get usage_summary from handler\"\n            RLM-->>User: \"RLMChatCompletion\"\n        else \"continue iteration\"\n            RLM->>RLM: \"format_iteration() -> messages\"\n            RLM->>RLM: \"message_history.extend(messages)\"\n        end\n    end\n    \n    alt \"max_iterations reached without final_answer\"\n        RLM->>RLM: \"_default_answer(message_history)\"\n        RLM->>LMHandler: \"completion(prompt)\"\n        LMHandler-->>RLM: \"forced answer\"\n    end\n    \n    RLM->>SpawnCtx: \"Exit context manager\"\n    deactivate SpawnCtx\n    SpawnCtx->>LMHandler: \"stop() - cleanup socket\"\n    deactivate LMHandler\n    SpawnCtx->>Env: \"cleanup() if not persistent\"\n    deactivate Env\n    \n    RLM-->>User: \"RLMChatCompletion(response, usage_summary)\"\n    deactivate RLM\n```\n\n**Sources:** [rlm/core/rlm.py:192-295](), [rlm/core/rlm.py:125-179](), [rlm/core/rlm.py:297-323]()\n\n## Iterative Execution Loop\n\nThe `RLM.completion()` method [rlm/core/rlm.py:192-295]() implements the core iterative execution loop. Each iteration consists of:\n\n1. **Prompt Construction** - Build current prompt from message history plus context-aware user prompt [rlm/core/rlm.py:229-232]()\n2. **LLM Query** - Request completion via `LMHandler.completion()` [rlm/core/rlm.py:308]()\n3. **Code Extraction** - Parse response for code blocks using `find_code_blocks()` [rlm/core/rlm.py:309]()\n4. **Code Execution** - Execute each code block in the REPL environment [rlm/core/rlm.py:312-314]()\n5. **Result Processing** - Capture stdout, stderr, locals, and nested LLM calls in `REPLResult`\n6. **Final Answer Detection** - Check for `FINAL()` or `FINAL_VAR()` markers via `find_final_answer()` [rlm/core/rlm.py:240]()\n7. **History Update** - Format iteration results and append to message history [rlm/core/rlm.py:271-274]()\n\n### Configuration Parameters\n\nThe following table shows key configuration parameters that control RLM execution:\n\n| Parameter | Type | Default | Description | Code Location |\n|-----------|------|---------|-------------|---------------|\n| `backend` | `ClientBackend` | `\"openai\"` | LLM provider (openai, anthropic, portkey, etc.) | [rlm.py:42]() |\n| `backend_kwargs` | `dict[str, Any]` | `None` | Arguments for LLM client (e.g., `model_name`, `api_key`) | [rlm.py:43]() |\n| `environment` | `EnvironmentType` | `\"local\"` | Execution environment (local, docker, modal, prime, daytona) | [rlm.py:44]() |\n| `environment_kwargs` | `dict[str, Any]` | `None` | Arguments for environment initialization | [rlm.py:45]() |\n| `depth` | `int` | `0` | Current recursion depth (0-indexed) | [rlm.py:46]() |\n| `max_depth` | `int` | `1` | Maximum allowed recursion depth | [rlm.py:47]() |\n| `max_iterations` | `int` | `30` | Maximum iterations before forcing answer | [rlm.py:48]() |\n| `custom_system_prompt` | `str` | `None` | Override default RLM system prompt | [rlm.py:49]() |\n| `other_backends` | `list[ClientBackend]` | `None` | Additional LLM backends for depth=1+ sub-calls | [rlm.py:50]() |\n| `other_backend_kwargs` | `list[dict]` | `None` | Kwargs for other_backends (ordered list) | [rlm.py:51]() |\n| `logger` | `RLMLogger` | `None` | Logger instance for trajectory recording (.jsonl files) | [rlm.py:52]() |\n| `verbose` | `bool` | `False` | Enable rich console output (Tokyo Night theme) | [rlm.py:53]() |\n| `persistent` | `bool` | `False` | Reuse environment across completion() calls for multi-turn | [rlm.py:54]() |\n\n### Loop Termination Conditions\n\nThe iteration loop [rlm/core/rlm.py:217-275]() terminates when:\n\n1. **Final Answer Found**: The `find_final_answer()` function detects a `FINAL()` or `FINAL_VAR()` marker in the LLM response [rlm/core/rlm.py:240-268]()\n2. **Max Iterations Reached**: After `max_iterations`, the system calls `_default_answer()` to force a final response [rlm/core/rlm.py:276-295]()\n3. **Fallback to Standard LLM**: If `depth >= max_depth`, the system falls back to a standard LLM completion via `_fallback_answer()` [rlm/core/rlm.py:210-212]()\n\n**Sources:** [rlm/core/rlm.py:40-54](), [rlm/core/rlm.py:192-295](), [rlm/core/rlm.py:217-275]()\n\n## Per-Completion Resource Spawning\n\nA key architectural pattern in RLM is that each `completion()` call spawns dedicated resources that are cleaned up when the call completes. This ensures isolation between different completion requests, except when `persistent=True` is set for multi-turn conversations.\n\n### Spawn Context Pattern\n\nThe `_spawn_completion_context()` context manager [rlm/core/rlm.py:125-179]() handles resource lifecycle:\n\n```mermaid\nstateDiagram-v2\n    [*] --> CreateClient: \"Enter context\"\n    CreateClient --> CreateHandler: \"get_client(backend, kwargs)\"\n    CreateHandler --> CreateOthers: \"LMHandler(client)\"\n    CreateOthers --> RegisterClients: \"Create other_backend clients\"\n    RegisterClients --> StartHandler: \"handler.register_client(name, client)\"\n    StartHandler --> CheckPersistent: \"handler.start() - TCP server\"\n    CheckPersistent --> ReuseEnv: \"if persistent and _persistent_env exists\"\n    CheckPersistent --> CreateEnv: \"else: get_environment(type, kwargs)\"\n    ReuseEnv --> UpdateAddress: \"environment.update_handler_address()\"\n    UpdateAddress --> AddContext: \"environment.add_context(prompt)\"\n    CreateEnv --> AddContext\n    AddContext --> Yield: \"Store in _persistent_env if persistent=True\"\n    Yield --> Executing: \"yield handler, environment\"\n    \n    Executing --> Cleanup: \"Exit context (finally block)\"\n    Cleanup --> StopHandler: \"handler.stop()\"\n    StopHandler --> CheckPersistentCleanup: \"Close socket, threads\"\n    CheckPersistentCleanup --> SkipCleanup: \"if persistent: keep environment\"\n    CheckPersistentCleanup --> CleanupEnv: \"else: environment.cleanup()\"\n    SkipCleanup --> [*]\n    CleanupEnv --> [*]: \"Destroy REPL state\"\n```\n\n**Sources:** [rlm/core/rlm.py:125-179](), [rlm/core/rlm.py:98-103]()\n\n### Component Initialization\n\nThe spawn context creates and configures the following components:\n\n```mermaid\ngraph TB\n    subgraph Client[\"1. Client Creation\"]\n        GetClient[\"get_client(backend, kwargs)<br/>[rlm/clients/__init__.py:11-63]\"]\n        PrimaryClient[\"primary_client: BaseLM<br/>(OpenAIClient/AnthropicClient/etc)\"]\n    end\n    \n    subgraph Handler[\"2. Handler Setup\"]\n        CreateHandler[\"LMHandler(primary_client)<br/>[lm_handler.py]\"]\n        OtherClients[\"Create other_backend_client<br/>if other_backends specified\"]\n        RegisterClients[\"handler.register_client(model_name, client)<br/>[rlm.py:147]\"]\n        StartServer[\"handler.start()<br/>Spawn TCP server thread<br/>[rlm.py:149]\"]\n    end\n    \n    subgraph Env[\"3. Environment Creation\"]\n        CheckPersist[\"if persistent and _persistent_env:<br/>reuse environment<br/>[rlm.py:152-162]\"]\n        PrepareKwargs[\"else: Prepare env_kwargs:<br/>lm_handler_address=(host,port)<br/>context_payload=prompt<br/>depth=self.depth+1<br/>[rlm.py:164-167]\"]\n        GetEnv[\"get_environment(type, kwargs)<br/>[rlm/environments/__init__.py]\"]\n        EnvInstance[\"environment: BaseEnv<br/>(LocalREPL/DockerREPL/Modal/etc)\"]\n    end\n    \n    GetClient --> PrimaryClient\n    PrimaryClient --> CreateHandler\n    CreateHandler --> OtherClients\n    OtherClients --> RegisterClients\n    RegisterClients --> StartServer\n    StartServer --> CheckPersist\n    CheckPersist --> PrepareKwargs\n    PrepareKwargs --> GetEnv\n    GetEnv --> EnvInstance\n```\n\nThe handler address `(host, port)` is passed to the environment [rlm/core/rlm.py:165]() so that code executed in the REPL can make recursive LLM calls back to the handler via socket or HTTP communication.\n\n**Sources:** [rlm/core/rlm.py:125-179](), [rlm/core/rlm.py:134-149](), [rlm/core/rlm.py:152-168]()\n\n### Message History Management\n\nThe `_setup_prompt()` method [rlm/core/rlm.py:180-191]() initializes the conversation history:\n\n1. **Extract Metadata**: Create `QueryMetadata` from the prompt [rlm/core/types.py:220-266]()\n2. **Build System Prompt**: Call `build_rlm_system_prompt()` with system prompt and metadata [rlm/utils/prompts.py]()\n3. **Return History**: Provide initial message list for iterative updates\n\nDuring each iteration, `format_iteration()` [rlm/utils/parsing.py]() converts the `RLMIteration` into message dictionaries that are appended to the history [rlm/core/rlm.py:271-274]().\n\nFor persistent environments, message history is stored via `environment.add_history()` [rlm/core/rlm.py:258-259]() after finding a final answer, enabling multi-turn conversations.\n\n**Sources:** [rlm/core/rlm.py:180-191](), [rlm/core/rlm.py:271-274](), [rlm/core/rlm.py:257-259](), [rlm/core/types.py:220-266]()\n\n## Multi-Backend Support\n\nThe RLM architecture supports multiple LLM backends simultaneously, allowing different models for root-level orchestration versus recursive sub-calls.\n\n### Backend Configuration\n\nThe system distinguishes between:\n\n| Backend Role | Configuration | Purpose | Code Location |\n|-------------|---------------|---------|---------------|\n| **Primary Backend** | `backend`, `backend_kwargs` | Root-level LLM queries at depth=0 via `LMHandler.completion()` | [rlm.py:42-43]() |\n| **Sub-Call Backend** | `other_backends[0]`, `other_backend_kwargs[0]` | Depth=1+ recursive LLM queries from REPL code | [rlm.py:50-51]() |\n| **Registered Clients** | N/A | Additional models registered by name, callable via `llm_query(model=...)` | [rlm.py:144-147]() |\n\nCurrently, only one additional backend is supported [rlm/core/rlm.py:80-86]():\n\n```python\nif other_backends is not None:\n    if len(other_backends) != 1:\n        raise ValueError(\n            \"We currently only support one additional backend for the recursive sub-calls!\"\n        )\n```\n\n### Backend Registration Flow\n\n```mermaid\nsequenceDiagram\n    participant Init[\"RLM.__init__\"]\n    participant Spawn[\"_spawn_completion_context\"]\n    participant GetClient[\"get_client()<br/>[clients/__init__.py:11-63]\"]\n    participant Handler[\"LMHandler\"]\n    \n    Init->>Init: \"Validate: len(other_backends) <= 1<br/>[rlm.py:80-86]\"\n    Init->>Init: \"Store: backend, other_backends,<br/>other_backend_kwargs\"\n    \n    Init->>Spawn: \"Enter context manager\"\n    Spawn->>GetClient: \"get_client(backend, backend_kwargs)\"\n    GetClient-->>Spawn: \"client: BaseLM\"\n    Spawn->>Spawn: \"Create other_backend_client if specified<br/>[rlm.py:137-139]\"\n    Spawn->>Handler: \"LMHandler(client, other_backend_client)\"\n    \n    loop \"for backend, kwargs in zip(other_backends, other_backend_kwargs)\"\n        Spawn->>GetClient: \"get_client(backend, kwargs)\"\n        GetClient-->>Spawn: \"other_client: BaseLM\"\n        Spawn->>Handler: \"register_client(model_name, other_client)<br/>[rlm.py:147]\"\n    end\n    \n    Spawn->>Handler: \"start() - spawn TCP server\"\n    Spawn-->>Init: \"yield handler, environment\"\n```\n\nWhen REPL code calls `llm_query(prompt, model=\"model_name\")`, the handler routes the request to the appropriate registered client based on the model name parameter.\n\n**Sources:** [rlm/core/rlm.py:40-54](), [rlm/core/rlm.py:80-86](), [rlm/core/rlm.py:137-147](), README.md\n\n## Environment Plugin Architecture\n\nThe RLM system uses a pluggable environment architecture that supports multiple execution backends with varying isolation levels. The key distinction is between **non-isolated** environments (same machine as LM, direct socket communication) and **isolated** environments (cloud sandboxes, HTTP broker pattern).\n\n### Environment Type Hierarchy\n\n```mermaid\ngraph TB\n    subgraph Abstract[\"Abstract Interfaces - rlm/environments/\"]\n        BaseEnv[\"BaseEnv<br/>execute_code(), cleanup()\"]\n        NonIsolated[\"NonIsolatedEnv<br/>extends BaseEnv<br/>Same machine execution\"]\n        Isolated[\"IsolatedEnv<br/>extends BaseEnv<br/>Cloud-based execution\"]\n        Persistent[\"SupportsPersistence<br/>Protocol: update_handler_address(),<br/>add_context(), get_context_count()\"]\n    end\n    \n    subgraph NonIso[\"Non-Isolated Implementations\"]\n        LocalREPL[\"LocalREPL<br/>Python exec()<br/>_SAFE_BUILTINS sandbox<br/>Direct socket\"]\n        DockerREPL[\"DockerREPL<br/>Docker container<br/>Volume mounts<br/>HTTP proxy via host.docker.internal\"]\n    end\n    \n    subgraph Iso[\"Isolated Implementations\"]\n        ModalREPL[\"ModalREPL<br/>Modal.Sandbox.create<br/>Encrypted tunnels<br/>HTTP broker + poller\"]\n        PrimeREPL[\"PrimeREPL<br/>Prime Sandboxes API<br/>Port exposure<br/>HTTP broker + poller\"]\n        DaytonaREPL[\"DaytonaREPL<br/>Daytona SDK<br/>Preview URLs<br/>HTTP broker + poller\"]\n    end\n    \n    subgraph Factory[\"Factory - rlm/environments/__init__.py\"]\n        GetEnv[\"get_environment(type, kwargs)<br/>Returns BaseEnv instance\"]\n    end\n    \n    BaseEnv --> NonIsolated\n    BaseEnv --> Isolated\n    NonIsolated --> LocalREPL\n    NonIsolated --> DockerREPL\n    Isolated --> ModalREPL\n    Isolated --> PrimeREPL\n    Isolated --> DaytonaREPL\n    \n    LocalREPL -.->|implements| Persistent\n    \n    GetEnv -.->|\"type='local'\"| LocalREPL\n    GetEnv -.->|\"type='docker'\"| DockerREPL\n    GetEnv -.->|\"type='modal'\"| ModalREPL\n    GetEnv -.->|\"type='prime'\"| PrimeREPL\n    GetEnv -.->|\"type='daytona'\"| DaytonaREPL\n```\n\n### Environment Features Comparison\n\n| Feature | LocalREPL | DockerREPL | ModalREPL | PrimeREPL | DaytonaREPL |\n|---------|-----------|------------|-----------|-----------|-------------|\n| **Isolation Level** | Low | Medium | High | High | High |\n| **Setup Required** | None | Docker installed | Modal account + `modal setup` | Prime account + API key | Daytona SDK |\n| **State Persistence** | In-memory + `/tmp/rlm_state.dill` | Dill to `/workspace/state.dill` | Cloud storage | Cloud storage | Cloud storage |\n| **Communication** | Direct socket to host | HTTP proxy via `host.docker.internal` | HTTP broker + encrypted ports | HTTP broker + expose | HTTP broker + preview URLs |\n| **Multi-Turn Support** | Yes (SupportsPersistence) | No | No | No | No |\n| **Use Case** | Local testing, benchmarks | Sandboxed local execution | Production, untrusted code | Production, untrusted code | Production with Daytona |\n\n**Sources:** [rlm/core/types.py:16](), README.md lines 79-115\n\n### Communication Patterns\n\nEnvironments use two distinct communication patterns to enable `llm_query()` calls from REPL code:\n\n#### Socket Protocol (Non-Isolated)\n\nUsed by `LocalREPL` and `DockerREPL` for direct TCP communication:\n\n```mermaid\nsequenceDiagram\n    participant Code[\"REPL Code\"]\n    participant Socket[\"Direct Socket\"]\n    participant Handler[\"LMHandler<br/>host:port\"]\n    \n    Code->>Socket: \"llm_query(prompt, model)\"\n    Socket->>Socket: \"Serialize LMRequest to JSON<br/>4-byte length prefix\"\n    Socket->>Handler: \"TCP socket.connect((host, port))\"\n    Handler->>Handler: \"Route by depth/model to client\"\n    Handler-->>Socket: \"LMResponse with completion\"\n    Socket->>Socket: \"Parse JSON response\"\n    Socket-->>Code: \"return response: str\"\n```\n\n#### HTTP Broker Pattern (Isolated)\n\nUsed by `ModalREPL`, `PrimeREPL`, `DaytonaREPL` for cloud-based execution:\n\n```mermaid\nsequenceDiagram\n    participant Code[\"REPL Code in Sandbox\"]\n    participant Broker[\"Flask HTTP Broker<br/>Inside sandbox\"]\n    participant Tunnel[\"Tunnel Layer<br/>(encrypted_ports/expose/preview)\"]\n    participant Poller[\"Poller Thread<br/>On host, 100ms interval\"]\n    participant Handler[\"LMHandler\"]\n    \n    Code->>Broker: \"POST /llm_query<br/>{prompt, model}\"\n    Broker->>Broker: \"pending_requests.append(request)\"\n    Broker-->>Code: \"Immediate ACK\"\n    \n    loop \"Every 100ms\"\n        Poller->>Tunnel: \"GET /pending_requests\"\n        Tunnel->>Broker: \"GET /pending_requests\"\n        Broker-->>Poller: \"[request1, request2, ...]\"\n        \n        loop \"for each request\"\n            Poller->>Handler: \"Forward LMRequest\"\n            Handler-->>Poller: \"LMResponse\"\n            Poller->>Tunnel: \"POST /responses<br/>{id: response}\"\n            Tunnel->>Broker: \"POST /responses\"\n        end\n    end\n    \n    Code->>Broker: \"GET /responses/{request_id}\"\n    Broker-->>Code: \"response: str\"\n```\n\n**Sources:** [rlm/environments/base_env.py](), README.md lines 79-97\n\n## Data Types and Communication Protocol\n\nThe RLM system uses structured data types to represent execution state and enable communication between components.\n\n### Core Data Types\n\nThe following types are defined in [rlm/core/types.py]():\n\n| Type | Key Fields | Purpose |\n|------|------------|---------|\n| `RLMIteration` | `prompt`, `response`, `code_blocks`, `final_answer`, `iteration_time` | Represents one iteration of the RLM loop |\n| `CodeBlock` | `code`, `result: REPLResult` | Stores code and its execution result |\n| `REPLResult` | `stdout`, `stderr`, `locals`, `rlm_calls`, `execution_time` | Output from REPL code execution |\n| `RLMChatCompletion` | `root_model`, `prompt`, `response`, `usage_summary`, `execution_time` | Final completion result |\n| `RLMMetadata` | `root_model`, `max_depth`, `max_iterations`, `backend`, `environment_type` | Configuration metadata for logging |\n| `UsageSummary` | `models: dict[str, ModelUsageSummary]` | Token usage across all models |\n| `ModelUsageSummary` | `num_calls`, `prompt_tokens`, `completion_tokens` | Per-model usage statistics |\n\n### Data Flow Through System\n\n```mermaid\ngraph LR\n    subgraph \"Input\"\n        Prompt[\"prompt: str or dict\"]\n    end\n    \n    subgraph \"Iteration Data [rlm.py:246-251]\"\n        Iteration[\"RLMIteration<br/>- prompt<br/>- response<br/>- code_blocks[]\"]\n        CodeBlock[\"CodeBlock<br/>- code<br/>- result\"]\n        REPLResult[\"REPLResult<br/>- stdout/stderr<br/>- locals<br/>- rlm_calls[]\"]\n    end\n    \n    subgraph \"Output\"\n        Completion[\"RLMChatCompletion<br/>- response<br/>- usage_summary<br/>- execution_time\"]\n    end\n    \n    subgraph \"Logging [rlm/logger.py]\"\n        Metadata[\"RLMMetadata\"]\n        Logger[\"RLMLogger<br/>writes .jsonl files\"]\n    end\n    \n    Prompt --> Iteration\n    Iteration --> CodeBlock\n    CodeBlock --> REPLResult\n    REPLResult --> Completion\n    \n    Metadata --> Logger\n    Iteration --> Logger\n    Logger --> JSONL[\"trajectory.jsonl\"]\n```\n\n**Sources:** [rlm/core/types.py](), [rlm/core/rlm.py:246-251](), [rlm/logger.py]()\n\n### Socket Communication Protocol\n\nEnvironments communicate with the `LMHandler` using a simple socket protocol defined in [rlm/core/comms_utils.py]():\n\n1. **Request Format**: `LMRequest(prompt, model)` serialized as JSON\n2. **Response Format**: `LMResponse(success, chat_completion, error)` serialized as JSON\n3. **Wire Protocol**: 4-byte length prefix (big-endian) + JSON payload\n\nThe `send_lm_request()` function [rlm/core/comms_utils.py]() handles this protocol for environments that need to make recursive calls.\n\n**Sources:** [rlm/core/comms_utils.py](), [rlm/environments/docker_repl.py:58-60]()\n\n## State Management and Reset\n\nThe `RLM_REPL` class maintains stateful execution across its lifetime. The `reset()` method [rlm/rlm_repl.py:127-131]() provides a mechanism to clear this state:\n\n```python\ndef reset(self):\n    \"\"\"Reset the (REPL) environment and message history.\"\"\"\n    self.repl_env = REPLEnv()\n    self.messages = []\n    self.query = None\n```\n\nThis creates a fresh `REPLEnv` instance and clears the conversation history, allowing the same `RLM_REPL` instance to be reused for multiple independent queries.\n\n**Sources:** [rlm/rlm_repl.py:127-131]()\n\n## Unimplemented Features\n\nThe `cost_summary()` method [rlm/rlm_repl.py:123-125]() is defined by the abstract interface but raises `NotImplementedError` in this minimal implementation. The README notes that cost tracking is excluded from this stripped-down version to maintain simplicity and ease of modification.\n\n**Sources:** [rlm/rlm_repl.py:123-125](), README.md\n\n---\n\n# Page: RLM Class\n\n# RLM Class\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [rlm/clients/__init__.py](rlm/clients/__init__.py)\n- [rlm/clients/azure_openai.py](rlm/clients/azure_openai.py)\n- [rlm/core/rlm.py](rlm/core/rlm.py)\n- [rlm/core/types.py](rlm/core/types.py)\n\n</details>\n\n\n\nThe `RLM` class is the main orchestrator and primary user-facing interface for the Recursive Language Model system. This document covers the class structure, initialization parameters, the iterative completion process, and internal execution flow.\n\nFor information about the LM Handler that manages LLM communication, see [LM Handler](#3.2). For details on REPL environments where code executes, see [REPL Environments](#3.3). For recursive LLM call mechanisms, see [Recursion and Sub-Calls](#3.4).\n\n---\n\n## Overview and Responsibilities\n\nThe `RLM` class serves as the central coordinator that:\n- Configures and initializes LLM backends and execution environments\n- Implements the iterative reasoning loop (up to `max_iterations`)\n- Spawns isolated `LMHandler` and environment instances per `completion()` call\n- Manages message history and prompt construction\n- Extracts code blocks from LLM responses and executes them\n- Detects final answers and returns results\n- Tracks usage statistics and execution time\n- Optionally logs execution trajectories\n\nEach `completion()` call creates fresh handler and environment instances, ensuring isolation between queries.\n\nSources: [rlm/core/rlm.py:32-38]()\n\n---\n\n## Class Structure and Methods\n\n```mermaid\nclassDiagram\n    class RLM {\n        +ClientBackend backend\n        +dict backend_kwargs\n        +EnvironmentType environment_type\n        +dict environment_kwargs\n        +int depth\n        +int max_depth\n        +int max_iterations\n        +str system_prompt\n        +RLMLogger logger\n        +VerbosePrinter verbose\n        \n        +__init__(backend, backend_kwargs, environment, ...)\n        +completion(prompt, root_prompt) RLMChatCompletion\n        -_spawn_completion_context(prompt) contextmanager\n        -_setup_prompt(prompt) list\n        -_completion_turn(prompt, handler, env) RLMIteration\n        -_default_answer(message_history, handler) str\n        -_fallback_answer(message) str\n    }\n    \n    class LMHandler {\n        +start()\n        +completion(prompt)\n        +get_usage_summary()\n        +stop()\n    }\n    \n    class BaseEnv {\n        +execute_code(code)\n        +cleanup()\n    }\n    \n    class RLMLogger {\n        +log_metadata(metadata)\n        +log(iteration)\n    }\n    \n    class BaseLM {\n        +completion(prompt)\n    }\n    \n    RLM ..> LMHandler : \"spawns per completion\"\n    RLM ..> BaseEnv : \"spawns per completion\"\n    RLM ..> RLMLogger : \"logs to\"\n    RLM ..> BaseLM : \"creates via get_client\"\n    LMHandler ..> BaseLM : \"wraps\"\n```\n\n**Diagram: RLM Class Structure and Dependencies**\n\nThe diagram illustrates the `RLM` class's main attributes and methods, plus its relationships with other core components. Note the spawning relationships: each `completion()` call creates dedicated handler and environment instances.\n\nSources: [rlm/core/rlm.py:32-284]()\n\n---\n\n## Initialization Parameters\n\nThe `RLM.__init__()` method accepts configuration for backends, environments, recursion depth, and logging.\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `backend` | `ClientBackend` | `\"openai\"` | LLM backend identifier (`\"openai\"`, `\"portkey\"`, `\"anthropic\"`, etc.) |\n| `backend_kwargs` | `dict[str, Any]` | `None` | Arguments passed to the backend client (e.g., `model_name`, `api_key`) |\n| `environment` | `EnvironmentType` | `\"local\"` | Execution environment type (`\"local\"`, `\"modal\"`, `\"prime\"`) |\n| `environment_kwargs` | `dict[str, Any]` | `None` | Arguments passed to the environment (e.g., timeouts, image specifications) |\n| `depth` | `int` | `0` | Current recursion depth (0-indexed) |\n| `max_depth` | `int` | `1` | Maximum recursion depth (currently only depth 1 supported) |\n| `max_iterations` | `int` | `30` | Maximum number of iterative reasoning steps |\n| `custom_system_prompt` | `str` | `None` | Override default system prompt |\n| `other_backends` | `list[ClientBackend]` | `None` | Additional backends for nested `llm_query()` calls |\n| `other_backend_kwargs` | `list[dict[str, Any]]` | `None` | Arguments for `other_backends` (ordered match) |\n| `logger` | `RLMLogger` | `None` | Logger instance for trajectory recording |\n| `verbose` | `bool` | `False` | Enable rich console output during execution |\n\n**Key Initialization Behavior:**\n\n- Configuration is stored for spawning per-completion instances (not created during `__init__`)\n- If `logger` is provided, metadata is logged via `RLMLogger.log_metadata()` [rlm/core/rlm.py:86-99]()\n- The `backend_kwargs` and `environment_kwargs` dicts are copied to prevent mutations [rlm/core/rlm.py:74]()\n- Sensitive keys (API keys, credentials) are filtered from metadata logs via `filter_sensitive_keys()` [rlm/core/rlm.py:92-94]()\n\n**Example Configuration:**\n\n```python\nrlm = RLM(\n    backend=\"openai\",\n    backend_kwargs={\"model_name\": \"gpt-4\", \"api_key\": \"...\"},\n    environment=\"local\",\n    environment_kwargs={},\n    max_iterations=20,\n    logger=RLMLogger(\"./logs/run.jsonl\"),\n    verbose=True\n)\n```\n\nSources: [rlm/core/rlm.py:40-99](), [rlm/core/types.py:5-6]()\n\n---\n\n## Completion Method\n\nThe `completion()` method is the main entry point for querying the RLM system. It replaces a standard LLM completion call with an iterative reasoning process.\n\n### Method Signature\n\n```python\ndef completion(\n    self, \n    prompt: str | dict[str, Any], \n    root_prompt: str | None = None\n) -> RLMChatCompletion\n```\n\n**Parameters:**\n- `prompt`: Context provided to the system. Can be a string or a dictionary of messages (OpenAI format).\n- `root_prompt`: Optional small prompt shown to the root LLM (e.g., the original user question). Common use case is passing the user's question while `prompt` contains larger context.\n\n**Returns:**\n- `RLMChatCompletion`: Object containing the final answer, usage statistics, and execution time.\n\n### Execution Flow\n\n```mermaid\nflowchart TD\n    Start[\"completion(prompt)\"] --> DepthCheck{\"depth >= max_depth?\"}\n    DepthCheck -->|Yes| Fallback[\"_fallback_answer()\"]\n    Fallback --> ReturnFallback[\"Return LLM response\"]\n    \n    DepthCheck -->|No| SpawnContext[\"_spawn_completion_context()\"]\n    SpawnContext --> CreateHandler[\"Create LMHandler + clients\"]\n    CreateHandler --> CreateEnv[\"Create BaseEnv instance\"]\n    CreateEnv --> SetupPrompt[\"_setup_prompt()\"]\n    \n    SetupPrompt --> IterLoop{\"i < max_iterations?\"}\n    \n    IterLoop -->|Yes| BuildPrompt[\"Build current_prompt with history\"]\n    BuildPrompt --> CompletionTurn[\"_completion_turn()\"]\n    CompletionTurn --> LLMCall[\"LMHandler.completion()\"]\n    LLMCall --> ExtractCode[\"find_code_blocks()\"]\n    ExtractCode --> ExecLoop[\"Execute each code block\"]\n    ExecLoop --> CheckAnswer{\"Final answer found?\"}\n    \n    CheckAnswer -->|Yes| LogIter[\"Log iteration\"]\n    LogIter --> ReturnFinal[\"Return RLMChatCompletion\"]\n    \n    CheckAnswer -->|No| FormatIter[\"format_iteration()\"]\n    FormatIter --> UpdateHistory[\"Extend message_history\"]\n    UpdateHistory --> IterLoop\n    \n    IterLoop -->|No| DefaultAnswer[\"_default_answer()\"]\n    DefaultAnswer --> ReturnDefault[\"Return RLMChatCompletion\"]\n    \n    ReturnFinal --> Cleanup[\"Cleanup handler & env\"]\n    ReturnDefault --> Cleanup\n    ReturnFallback --> End[\"End\"]\n    Cleanup --> End\n```\n\n**Diagram: Completion Method Execution Flow**\n\nThe flowchart shows the complete execution path, including depth checking, context spawning, iterative loop, and cleanup.\n\nSources: [rlm/core/rlm.py:147-224]()\n\n### Fallback and Default Behaviors\n\n**Fallback (_fallback_answer)**: When `depth >= max_depth`, the RLM degrades to a standard LLM call without iteration [rlm/core/rlm.py:278-284]().\n\n**Default Answer (_default_answer)**: When `max_iterations` is reached without finding a final answer, the system prompts the LLM to provide a final answer based on accumulated context [rlm/core/rlm.py:253-276]().\n\n---\n\n## Context Spawning and Lifecycle Management\n\nThe `_spawn_completion_context()` method is a context manager that creates and cleans up per-completion resources.\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant RLM_completion\n    participant spawn_context as \"_spawn_completion_context\"\n    participant get_client\n    participant LMHandler\n    participant get_environment\n    participant BaseEnv\n    \n    User->>RLM_completion: completion(prompt)\n    RLM_completion->>spawn_context: enter context\n    \n    spawn_context->>get_client: get_client(backend, kwargs)\n    get_client-->>spawn_context: BaseLM instance\n    \n    spawn_context->>LMHandler: __init__(client)\n    spawn_context->>LMHandler: register_client() for other_backends\n    spawn_context->>LMHandler: start() - spawn server\n    \n    spawn_context->>get_environment: get_environment(type, kwargs)\n    Note over spawn_context,get_environment: Pass handler address + context\n    get_environment-->>spawn_context: BaseEnv instance\n    \n    spawn_context-->>RLM_completion: yield (handler, env)\n    \n    RLM_completion->>RLM_completion: Execute iterative loop\n    \n    RLM_completion->>spawn_context: exit context\n    spawn_context->>LMHandler: stop()\n    spawn_context->>BaseEnv: cleanup()\n```\n\n**Diagram: Per-Completion Context Lifecycle**\n\nThis sequence diagram shows the creation and cleanup of isolated handler and environment instances for each completion call.\n\n**Key Operations:**\n\n1. **Client Creation**: Instantiate primary `BaseLM` client via `get_client()` [rlm/core/rlm.py:108]()\n2. **Handler Initialization**: Wrap client in `LMHandler` and register additional clients for sub-calls [rlm/core/rlm.py:109-115]()\n3. **Handler Start**: Spawn server thread via `lm_handler.start()` [rlm/core/rlm.py:117]()\n4. **Environment Setup**: Pass handler address and context to environment [rlm/core/rlm.py:120-122]()\n5. **Environment Creation**: Instantiate `BaseEnv` via `get_environment()` [rlm/core/rlm.py:125]()\n6. **Cleanup**: Always execute `lm_handler.stop()` and `environment.cleanup()` on context exit [rlm/core/rlm.py:131-133]()\n\nSources: [rlm/core/rlm.py:101-133]()\n\n---\n\n## Iterative Execution Loop\n\nThe core iteration logic resides in the `completion()` method's main loop and the `_completion_turn()` helper.\n\n### Completion Turn\n\nA single iteration consists of:\n\n```mermaid\nflowchart LR\n    Turn[\"_completion_turn()\"] --> BuildMsg[\"Construct current_prompt\"]\n    BuildMsg --> CallLLM[\"lm_handler.completion()\"]\n    CallLLM --> ParseCode[\"find_code_blocks()\"]\n    ParseCode --> ExecLoop[\"For each code block\"]\n    ExecLoop --> EnvExec[\"environment.execute_code()\"]\n    EnvExec --> CreateResult[\"Create CodeBlock + REPLResult\"]\n    CreateResult --> ReturnIter[\"Return RLMIteration\"]\n```\n\n**Diagram: Single Completion Turn Flow**\n\nEach turn produces an `RLMIteration` object containing the prompt, LLM response, executed code blocks, and optional final answer.\n\n**Step-by-Step:**\n\n1. **Prompt Construction**: Build message list from history + user prompt suffix [rlm/core/rlm.py:174]()\n2. **LLM Call**: Request completion from handler [rlm/core/rlm.py:237]()\n3. **Code Extraction**: Parse code blocks from response using `find_code_blocks()` [rlm/core/rlm.py:238]()\n4. **Code Execution**: Execute each block via `environment.execute_code()` [rlm/core/rlm.py:242]()\n5. **Result Aggregation**: Create `CodeBlock` instances with `REPLResult` [rlm/core/rlm.py:243]()\n6. **Iteration Object**: Return `RLMIteration` with timing [rlm/core/rlm.py:246-251]()\n\nSources: [rlm/core/rlm.py:226-251]()\n\n### Message History Management\n\nThe system maintains a growing message history across iterations:\n\n```python\n# Initial setup\nmessage_history = build_rlm_system_prompt(system_prompt, query_metadata)\n\n# Per iteration\ncurrent_prompt = message_history + [build_user_prompt(root_prompt, i)]\niteration = self._completion_turn(current_prompt, ...)\n\n# After iteration\nnew_messages = format_iteration(iteration)\nmessage_history.extend(new_messages)\n```\n\n**Key Functions:**\n- `build_rlm_system_prompt()`: Creates system message with metadata [rlm/core/rlm.py:141-143]()\n- `build_user_prompt()`: Adds iteration-specific user prompt [rlm/core/rlm.py:174]()\n- `format_iteration()`: Converts `RLMIteration` to message list [rlm/core/rlm.py:207]()\n\nThe formatted iteration includes the assistant's response and execution results, which inform the next iteration's context.\n\nSources: [rlm/core/rlm.py:135-210]()\n\n---\n\n## Final Answer Detection and Termination\n\nThe system terminates when a final answer is detected or `max_iterations` is reached.\n\n### Answer Detection\n\nAfter each iteration, the system checks for a final answer marker:\n\n```python\nfinal_answer = find_final_answer(iteration.response)\niteration.final_answer = final_answer\n\nif final_answer:\n    return RLMChatCompletion(...)\n```\n\nThe `find_final_answer()` function looks for specific markers in the LLM response (e.g., `FINAL ANSWER:` tags) [rlm/core/rlm.py:183-204]().\n\n### Return Value Construction\n\nThe `RLMChatCompletion` object includes:\n\n| Field | Source | Description |\n|-------|--------|-------------|\n| `root_model` | `backend_kwargs[\"model_name\"]` | Primary LLM model identifier |\n| `prompt` | Original input | User's initial prompt |\n| `response` | `final_answer` string | Extracted or generated final answer |\n| `usage_summary` | `lm_handler.get_usage_summary()` | Token counts per model |\n| `execution_time` | `time.perf_counter()` delta | Total execution time in seconds |\n\nSources: [rlm/core/rlm.py:194-224](), [rlm/core/types.py:81-107]()\n\n---\n\n## Integration with Other Components\n\nThe `RLM` class orchestrates multiple system components through well-defined interfaces.\n\n```mermaid\ngraph TB\n    RLM_class[\"RLM Class\"]\n    \n    subgraph \"Client Layer\"\n        get_client[\"get_client() factory\"]\n        BaseLM[\"BaseLM interface\"]\n        OpenAI[\"OpenAIClient\"]\n        Portkey[\"PortkeyClient\"]\n    end\n    \n    subgraph \"Handler Layer\"\n        LMHandler[\"LMHandler\"]\n        Socket[\"Socket Server\"]\n    end\n    \n    subgraph \"Environment Layer\"\n        get_environment[\"get_environment() factory\"]\n        BaseEnv[\"BaseEnv interface\"]\n        LocalREPL[\"LocalREPL\"]\n        ModalREPL[\"ModalREPL\"]\n    end\n    \n    subgraph \"Utilities\"\n        Parsing[\"Parsing Utils\"]\n        Prompts[\"Prompt Templates\"]\n        Filter[\"filter_sensitive_keys()\"]\n    end\n    \n    subgraph \"Logging\"\n        RLMLogger[\"RLMLogger\"]\n        VerbosePrinter[\"VerbosePrinter\"]\n    end\n    \n    RLM_class --> get_client\n    get_client --> BaseLM\n    BaseLM --> OpenAI\n    BaseLM --> Portkey\n    \n    RLM_class --> LMHandler\n    LMHandler --> Socket\n    \n    RLM_class --> get_environment\n    get_environment --> BaseEnv\n    BaseEnv --> LocalREPL\n    BaseEnv --> ModalREPL\n    \n    RLM_class --> Parsing\n    RLM_class --> Prompts\n    RLM_class --> Filter\n    \n    RLM_class --> RLMLogger\n    RLM_class --> VerbosePrinter\n    \n    LocalREPL -.-> Socket\n    ModalREPL -.-> Socket\n```\n\n**Diagram: RLM Integration Points**\n\nThe RLM class integrates with factory functions (`get_client`, `get_environment`), utility modules (parsing, prompts), and logging systems. Environments communicate back to handlers via sockets.\n\n### Import Dependencies\n\n```python\nfrom rlm.clients import BaseLM, get_client\nfrom rlm.core.lm_handler import LMHandler\nfrom rlm.core.types import (ClientBackend, EnvironmentType, RLMChatCompletion, ...)\nfrom rlm.environments import BaseEnv, get_environment\nfrom rlm.logger import RLMLogger, VerbosePrinter\nfrom rlm.utils.parsing import find_code_blocks, find_final_answer, format_iteration\nfrom rlm.utils.prompts import build_rlm_system_prompt, build_user_prompt\nfrom rlm.utils.rlm_utils import filter_sensitive_keys\n```\n\nSources: [rlm/core/rlm.py:1-29]()\n\n---\n\n## Data Flow Through RLM\n\nThe following table maps input to output through the RLM system:\n\n| Stage | Input | Processing | Output |\n|-------|-------|------------|--------|\n| **Initialization** | Config params | Store configuration, create logger metadata | `RLM` instance |\n| **Prompt Setup** | `prompt` string/dict | Parse metadata, build system message | `message_history` list |\n| **LLM Call** | Message list | `LMHandler.completion()` | Response string |\n| **Code Extraction** | Response string | `find_code_blocks()` | List of code strings |\n| **Code Execution** | Code string | `BaseEnv.execute_code()` | `REPLResult` |\n| **Iteration Packaging** | Response + code results | Create `RLMIteration` | Logged iteration object |\n| **History Update** | `RLMIteration` | `format_iteration()` | New messages for history |\n| **Final Answer** | All iterations | Extract or generate | `RLMChatCompletion` |\n\nSources: [rlm/core/rlm.py:147-251](), [rlm/core/types.py:110-170]()\n\n---\n\n## Configuration Examples\n\n### Basic Usage\n\n```python\nfrom rlm.core.rlm import RLM\n\nrlm = RLM(\n    backend=\"openai\",\n    backend_kwargs={\"model_name\": \"gpt-4\"},\n    environment=\"local\"\n)\n\nresult = rlm.completion(\"What is the sum of all prime numbers less than 100?\")\nprint(result.response)\n```\n\n### Multi-Backend Configuration\n\n```python\nrlm = RLM(\n    backend=\"openai\",\n    backend_kwargs={\"model_name\": \"gpt-4\"},\n    other_backends=[\"openai\", \"anthropic\"],\n    other_backend_kwargs=[\n        {\"model_name\": \"gpt-3.5-turbo\"},\n        {\"model_name\": \"claude-3-sonnet-20240229\"}\n    ],\n    environment=\"local\"\n)\n```\n\nThis allows code executed in environments to call `llm_query(model=\"gpt-3.5-turbo\")` or `llm_query(model=\"claude-3-sonnet-20240229\")` for sub-queries. See [Multi-Backend Configuration](#9.1) for details.\n\n### With Logging\n\n```python\nfrom rlm.logger import RLMLogger\n\nlogger = RLMLogger(\"trajectory.jsonl\")\nrlm = RLM(\n    backend=\"openai\",\n    backend_kwargs={\"model_name\": \"gpt-4\"},\n    environment=\"local\",\n    logger=logger,\n    verbose=True\n)\n\nresult = rlm.completion(\"Complex reasoning task...\")\n```\n\nThis logs each iteration's metadata and execution trace. See [RLM Logger](#6.1) for log format details.\n\nSources: [rlm/core/rlm.py:40-99]()\n\n---\n\n## Depth and Recursion\n\nThe `depth` and `max_depth` parameters control recursive RLM instantiation. Currently, only `max_depth=1` is supported, meaning:\n\n- **Depth 0** (root RLM): Executes iterative reasoning with code execution\n- **Depth 1** (reached via `llm_query`): Falls back to direct LLM completion without iteration\n\nWhen `depth >= max_depth`, the `_fallback_answer()` method is called, bypassing the iterative loop [rlm/core/rlm.py:166-167](), [rlm/core/rlm.py:278-284]().\n\nFuture versions may support deeper recursion where nested RLMs can themselves perform iterative reasoning. See [Recursion and Sub-Calls](#3.4) for more on how environments make nested LLM calls.\n\nSources: [rlm/core/rlm.py:46-47](), [rlm/core/rlm.py:61-62](), [rlm/core/rlm.py:166-167]()\n\n---\n\n## Timing and Usage Tracking\n\nThe RLM tracks execution time and token usage at multiple granularities:\n\n**Per-Iteration Timing:**\n```python\niter_start = time.perf_counter()\n# ... completion turn ...\niteration_time = time.perf_counter() - iter_start\n```\n\n**Total Execution Time:**\n```python\ntime_start = time.perf_counter()\n# ... all iterations ...\ntime_end = time.perf_counter()\nexecution_time = time_end - time_start\n```\n\n**Token Usage Aggregation:**\n\nThe `LMHandler` tracks usage per model via `get_usage_summary()`, returning a `UsageSummary` with per-model token counts [rlm/core/rlm.py:195](), [rlm/core/rlm.py:215]().\n\nThis data is included in the returned `RLMChatCompletion` and logged iterations for cost analysis. See [Communication and Data Types](#3.5) for usage data structures.\n\nSources: [rlm/core/rlm.py:163](), [rlm/core/rlm.py:236](), [rlm/core/rlm.py:245-246](), [rlm/core/types.py:56-74]()\n\n---\n\n## Summary\n\nThe `RLM` class is the central orchestrator that implements the iterative reasoning pattern:\n\n1. **Spawns** isolated handler and environment per completion\n2. **Iterates** up to `max_iterations`, prompting LLM → executing code → updating history\n3. **Detects** final answers or generates default response\n4. **Returns** structured result with usage and timing data\n5. **Cleans up** resources deterministically\n\nKey design principles:\n- **Isolation**: Each `completion()` call uses fresh handler/environment instances\n- **Flexibility**: Pluggable backends and environments via factory functions\n- **Observability**: Optional logging and verbose output for debugging\n- **Extensibility**: Multi-backend support for heterogeneous sub-calls\n\nSources: [rlm/core/rlm.py:32-284](), [rlm/core/types.py:1-236]()\n\n---\n\n# Page: LM Handler\n\n# LM Handler\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [rlm/clients/__init__.py](rlm/clients/__init__.py)\n- [rlm/clients/azure_openai.py](rlm/clients/azure_openai.py)\n- [rlm/core/comms_utils.py](rlm/core/comms_utils.py)\n- [rlm/core/lm_handler.py](rlm/core/lm_handler.py)\n- [rlm/core/types.py](rlm/core/types.py)\n\n</details>\n\n\n\nThe LM Handler component manages all language model communication in the RLM system. It provides a multi-threaded socket server that routes LLM completion requests from both the main RLM process and REPL environment subprocesses. The handler abstracts multiple LLM client backends, enabling concurrent request processing and multi-model support within a single RLM instance.\n\nFor information about the specific LLM client implementations, see [Client Architecture](#4.1). For details on how environments communicate with the handler, see [Communication and Data Types](#3.5).\n\n## Architecture Overview\n\nThe LM Handler uses a socket-based client-server architecture to enable inter-process communication between the RLM core, execution environments, and LLM APIs.\n\n```mermaid\ngraph TB\n    subgraph \"RLM Process\"\n        RLMCore[\"RLM.completion()\"]\n        LMHandler[\"LMHandler\"]\n        DefaultClient[\"BaseLM (default)\"]\n        ClientRegistry[\"clients: dict[str, BaseLM]\"]\n    end\n    \n    subgraph \"Socket Server\"\n        ThreadingLMServer[\"ThreadingLMServer<br/>(ThreadingTCPServer)\"]\n        LMRequestHandler[\"LMRequestHandler<br/>(StreamRequestHandler)\"]\n    end\n    \n    subgraph \"Environment Processes\"\n        LocalREPL[\"LocalREPL subprocess\"]\n        DockerREPL[\"DockerREPL container\"]\n        ModalREPL[\"ModalREPL sandbox\"]\n    end\n    \n    subgraph \"Protocol Layer\"\n        LMRequest[\"LMRequest<br/>(prompt/prompts + model)\"]\n        LMResponse[\"LMResponse<br/>(completion/error)\"]\n        SocketProtocol[\"socket_send/socket_recv<br/>(4-byte length + JSON)\"]\n    end\n    \n    RLMCore -->|\"direct call\"| LMHandler\n    LMHandler -->|\"start()\"| ThreadingLMServer\n    LMHandler -->|\"manages\"| ClientRegistry\n    LMHandler -->|\"default_client\"| DefaultClient\n    ClientRegistry -->|\"contains\"| DefaultClient\n    \n    ThreadingLMServer -->|\"spawns thread per request\"| LMRequestHandler\n    LMRequestHandler -->|\"calls get_client()\"| LMHandler\n    LMRequestHandler -->|\"calls completion()\"| ClientRegistry\n    \n    LocalREPL -->|\"socket connection\"| SocketProtocol\n    DockerREPL -->|\"socket connection\"| SocketProtocol\n    ModalREPL -->|\"socket connection\"| SocketProtocol\n    \n    SocketProtocol -->|\"receives\"| LMRequest\n    SocketProtocol -->|\"sends\"| LMResponse\n    LMRequest -->|\"handled by\"| LMRequestHandler\n    LMRequestHandler -->|\"returns\"| LMResponse\n```\n\n**Sources:** [rlm/core/lm_handler.py:1-186]()\n\n## Core Components\n\n### LMHandler Class\n\nThe `LMHandler` class [rlm/core/lm_handler.py:102-186]() is the main orchestrator that manages client registration, server lifecycle, and request routing.\n\n| Property/Method | Type | Description |\n|----------------|------|-------------|\n| `default_client` | `BaseLM` | Primary LLM client used when no model is specified |\n| `clients` | `dict[str, BaseLM]` | Registry mapping model names to client instances |\n| `host` | `str` | Server bind address (default: \"127.0.0.1\") |\n| `port` | `int` | Server port (0 = auto-assign) |\n| `start()` | `tuple[str, int]` | Starts socket server, returns (host, port) |\n| `stop()` | `None` | Shuts down socket server |\n| `register_client(model_name, client)` | `None` | Adds a client for a specific model |\n| `get_client(model)` | `BaseLM` | Returns client for model name, or default |\n| `completion(prompt, model)` | `str` | Direct completion call for main process |\n| `get_usage_summary()` | `UsageSummary` | Aggregates token usage across all clients |\n\n**Initialization:**\n```python\n# From rlm/core/lm_handler.py:110-123\ndef __init__(\n    self,\n    client: BaseLM,\n    host: str = \"127.0.0.1\",\n    port: int = 0,  # auto-assign available port\n):\n    self.default_client = client\n    self.clients: dict[str, BaseLM] = {}\n    self.host = host\n    self._server: ThreadingLMServer | None = None\n    self._thread: Thread | None = None\n    self._port = port\n    \n    self.register_client(client.model_name, client)\n```\n\nThe handler supports context manager usage [rlm/core/lm_handler.py:171-177](), automatically starting and stopping the server:\n\n```python\nwith LMHandler(client) as handler:\n    # Server runs in background thread\n    pass  # Server stops on exit\n```\n\n**Sources:** [rlm/core/lm_handler.py:102-186]()\n\n### ThreadingLMServer\n\nThe `ThreadingLMServer` [rlm/core/lm_handler.py:95-100]() extends Python's `ThreadingTCPServer` to handle concurrent requests:\n\n```python\nclass ThreadingLMServer(ThreadingTCPServer):\n    \"\"\"Multi-threaded TCP server for LM requests.\"\"\"\n    \n    daemon_threads = True\n    allow_reuse_address = True\n```\n\nKey characteristics:\n- **Daemon threads**: Worker threads terminate when main process exits\n- **Address reuse**: Allows immediate server restart on same port\n- **Thread-per-request**: Each connection spawns a new handler thread\n\n**Sources:** [rlm/core/lm_handler.py:95-100]()\n\n### LMRequestHandler\n\nThe `LMRequestHandler` [rlm/core/lm_handler.py:17-93]() processes individual socket connections by deserializing requests, routing to appropriate clients, and returning responses.\n\n```mermaid\ngraph TB\n    Handle[\"handle()\"]\n    SocketRecv[\"socket_recv(connection)\"]\n    ParseRequest[\"LMRequest.from_dict()\"]\n    CheckType{Is batched?}\n    HandleSingle[\"_handle_single()\"]\n    HandleBatched[\"_handle_batched()\"]\n    GetClient[\"handler.get_client(model)\"]\n    ClientCompletion[\"client.completion(prompt)\"]\n    ClientACompletion[\"client.acompletion(prompts)\"]\n    BuildResponse[\"LMResponse.success_response()\"]\n    SocketSend[\"socket_send(connection, response)\"]\n    ErrorResponse[\"LMResponse.error_response()\"]\n    \n    Handle --> SocketRecv\n    SocketRecv --> ParseRequest\n    ParseRequest --> CheckType\n    CheckType -->|\"request.is_batched\"| HandleBatched\n    CheckType -->|\"request.prompt\"| HandleSingle\n    \n    HandleSingle --> GetClient\n    GetClient --> ClientCompletion\n    ClientCompletion --> BuildResponse\n    \n    HandleBatched --> GetClient\n    GetClient --> ClientACompletion\n    ClientACompletion --> BuildResponse\n    \n    BuildResponse --> SocketSend\n    Handle -.->|\"on exception\"| ErrorResponse\n    ErrorResponse --> SocketSend\n```\n\n**Sources:** [rlm/core/lm_handler.py:17-93]()\n\n## Request Protocol\n\nThe handler uses a socket-based protocol with a 4-byte length prefix followed by JSON payload. The protocol is defined in the communication utilities module.\n\n### Request Format\n\nThe `LMRequest` class [rlm/core/comms_utils.py]() represents incoming requests:\n\n| Field | Type | Required | Description |\n|-------|------|----------|-------------|\n| `prompt` | `str` | Conditional | Single prompt (for non-batched requests) |\n| `prompts` | `list[str]` | Conditional | Multiple prompts (for batched requests) |\n| `model` | `str \\| None` | Optional | Model name (uses default if omitted) |\n| `is_batched` | `bool` | - | Computed property: `prompts is not None` |\n\n**Sources:** [rlm/core/lm_handler.py:28]()\n\n### Response Format\n\nThe `LMResponse` class encapsulates completion results or errors:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `completion` | `str \\| None` | Single completion result |\n| `chat_completions` | `list[RLMChatCompletion] \\| None` | Batched completion results |\n| `error` | `str \\| None` | Error message if request failed |\n\n**Factory Methods:**\n- `LMResponse.success_response(chat_completion)` [rlm/core/lm_handler.py:55-63]()\n- `LMResponse.batched_success_response(chat_completions)` [rlm/core/lm_handler.py:92]()\n- `LMResponse.error_response(message)` [rlm/core/lm_handler.py:24, 38, 43]()\n\n**Sources:** [rlm/core/lm_handler.py:24-44]()\n\n### Communication Flow\n\n```mermaid\nsequenceDiagram\n    participant Env as \"REPL Environment\"\n    participant Socket as \"TCP Socket\"\n    participant Handler as \"LMRequestHandler\"\n    participant LMH as \"LMHandler\"\n    participant Client as \"BaseLM Client\"\n    \n    Env->>Socket: \"socket_send(LMRequest)\"\n    Note over Socket: \"4-byte length + JSON\"\n    Socket->>Handler: \"connection established\"\n    Handler->>Handler: \"socket_recv()\"\n    Handler->>Handler: \"LMRequest.from_dict()\"\n    \n    Handler->>LMH: \"get_client(model)\"\n    LMH-->>Handler: \"BaseLM instance\"\n    \n    alt Single Request\n        Handler->>Client: \"completion(prompt)\"\n        Client-->>Handler: \"str content\"\n    else Batched Request\n        Handler->>Client: \"acompletion(prompt) x N\"\n        Note over Handler,Client: \"asyncio.gather()\"\n        Client-->>Handler: \"list[str] results\"\n    end\n    \n    Handler->>Handler: \"LMResponse.success_response()\"\n    Handler->>Socket: \"socket_send(LMResponse)\"\n    Socket->>Env: \"response received\"\n```\n\n**Sources:** [rlm/core/lm_handler.py:20-44]()\n\n## Client Management\n\nThe handler maintains a registry of LLM clients, enabling multi-model support within a single RLM instance.\n\n### Client Registration\n\nClients are registered by model name [rlm/core/lm_handler.py:125-127]():\n\n```python\ndef register_client(self, model_name: str, client: BaseLM) -> None:\n    \"\"\"Register a client for a specific model name.\"\"\"\n    self.clients[model_name] = client\n```\n\nThe default client is automatically registered during initialization [rlm/core/lm_handler.py:123]():\n\n```python\nself.register_client(client.model_name, client)\n```\n\n### Client Routing\n\nThe `get_client()` method [rlm/core/lm_handler.py:129-133]() implements fallback logic:\n\n1. If `model` is specified and exists in registry → return registered client\n2. Otherwise → return `default_client`\n\nThis enables environments to specify different models for recursive LLM calls (see [Recursion and Sub-Calls](#3.4)) while maintaining a sensible default.\n\n**Sources:** [rlm/core/lm_handler.py:125-133]()\n\n## Request Processing\n\nThe handler supports two request modes: single prompts and batched prompts.\n\n### Single Request Processing\n\nThe `_handle_single()` method [rlm/core/lm_handler.py:46-63]() processes one prompt:\n\n**Processing Steps:**\n1. **Client lookup**: `get_client(request.model)` retrieves appropriate client\n2. **Timing**: Start timer with `time.perf_counter()`\n3. **Completion**: Call `client.completion(request.prompt)`\n4. **Usage tracking**: Retrieve token counts with `client.get_last_usage()`\n5. **Response construction**: Build `RLMChatCompletion` with prompt, response, usage, and execution time\n\n**Data captured:**\n- Root model name (from request or client default)\n- Original prompt\n- Generated response\n- Token usage summary\n- Execution time in seconds\n\n**Sources:** [rlm/core/lm_handler.py:46-63]()\n\n### Batched Request Processing\n\nThe `_handle_batched()` method [rlm/core/lm_handler.py:65-92]() processes multiple prompts concurrently using async/await:\n\n**Processing Steps:**\n1. **Client lookup**: Single `get_client(request.model)` call for all prompts\n2. **Async execution**: Create tasks with `client.acompletion(prompt)` for each prompt\n3. **Concurrent execution**: `asyncio.gather(*tasks)` runs all requests in parallel\n4. **Usage aggregation**: Single `client.get_last_usage()` captures total usage\n5. **Response construction**: Build list of `RLMChatCompletion` objects\n\n**Timing behavior:**\n- Total execution time is measured across all requests\n- Per-prompt time is approximated as `total_time / len(prompts)` [rlm/core/lm_handler.py:87]()\n\n**Concurrency mechanism:**\n```python\n# From rlm/core/lm_handler.py:71-75\nasync def run_all():\n    tasks = [client.acompletion(prompt) for prompt in request.prompts]\n    return await asyncio.gather(*tasks)\n\nresults = asyncio.run(run_all())\n```\n\n**Sources:** [rlm/core/lm_handler.py:65-92]()\n\n## Server Lifecycle\n\nThe handler manages server startup and shutdown through explicit or context manager interfaces.\n\n### Starting the Server\n\nThe `start()` method [rlm/core/lm_handler.py:147-158]() initializes the socket server:\n\n1. **Idempotency check**: Return early if server already running [rlm/core/lm_handler.py:149-150]()\n2. **Server creation**: Instantiate `ThreadingLMServer` with (host, port) [rlm/core/lm_handler.py:152]()\n3. **Handler attachment**: Attach self to server for request routing [rlm/core/lm_handler.py:153]()\n4. **Thread spawn**: Start daemon thread running `serve_forever()` [rlm/core/lm_handler.py:155-156]()\n5. **Return address**: Return (host, port) tuple for client connections [rlm/core/lm_handler.py:158]()\n\n**Port auto-assignment:** When `port=0`, the OS assigns an available port. Retrieve the actual port with the `port` property [rlm/core/lm_handler.py:135-140]().\n\n**Sources:** [rlm/core/lm_handler.py:147-158]()\n\n### Stopping the Server\n\nThe `stop()` method [rlm/core/lm_handler.py:160-165]() cleanly shuts down:\n\n```python\ndef stop(self):\n    \"\"\"Stop the socket server.\"\"\"\n    if self._server:\n        self._server.shutdown()\n        self._server = None\n        self._thread = None\n```\n\nThe `shutdown()` call blocks until all worker threads complete their current requests.\n\n**Sources:** [rlm/core/lm_handler.py:160-165]()\n\n## Usage Tracking\n\nThe handler aggregates token usage across all registered clients through the `get_usage_summary()` method [rlm/core/lm_handler.py:179-185]():\n\n```python\ndef get_usage_summary(self) -> UsageSummary:\n    \"\"\"Get the usage summary for all clients, merged into a single dict.\"\"\"\n    merged = {}\n    for client in self.clients.values():\n        client_summary = client.get_usage_summary()\n        merged.update(client_summary.model_usage_summaries)\n    return UsageSummary(model_usage_summaries=merged)\n```\n\nThis enables cost tracking across multiple models and recursive LLM calls. The `UsageSummary` type contains a dictionary mapping model names to token counts (prompt, completion, total).\n\n**Sources:** [rlm/core/lm_handler.py:179-185]()\n\n## Integration with RLM Core\n\nThe `LMHandler` is instantiated by the `RLM` class and passed to execution environments:\n\n```mermaid\ngraph LR\n    RLMInit[\"RLM.__init__()\"]\n    CreateClient[\"get_client(backend)\"]\n    CreateHandler[\"LMHandler(client)\"]\n    HandlerStart[\"handler.start()\"]\n    CreateEnv[\"get_environment(type)\"]\n    PassAddress[\"env.__init__(handler.address)\"]\n    \n    RLMInit --> CreateClient\n    CreateClient --> CreateHandler\n    CreateHandler --> HandlerStart\n    HandlerStart --> CreateEnv\n    CreateEnv --> PassAddress\n```\n\n**Environment access:** Environments receive the handler's `(host, port)` address during initialization, enabling them to establish socket connections for recursive LLM calls. See [Recursion and Sub-Calls](#3.4) for details on how environments use this connection.\n\n**Direct access:** The RLM core calls `handler.completion()` [rlm/core/lm_handler.py:167-169]() directly without going through the socket layer, avoiding unnecessary serialization overhead.\n\n**Sources:** [rlm/core/lm_handler.py:102-186]()\n\n---\n\n# Page: REPL Environments\n\n# REPL Environments\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [Makefile](Makefile)\n- [README.md](README.md)\n- [rlm/environments/base_env.py](rlm/environments/base_env.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis document explains the environment abstraction layer in the RLM system. It covers the `BaseEnv` interface, the distinction between `IsolatedEnv` and `NonIsolatedEnv`, the core methods for setup and code execution, and the `SupportsPersistence` protocol. This page focuses on the **architectural design** and **interface contracts** of the environment system.\n\nFor detailed implementation guides for specific environments (`LocalREPL`, `DockerREPL`, `ModalREPL`, etc.), see [Section 5: Execution Environments](#5). For information about how environments communicate with the LM Handler, see [Section 3.5: Communication Protocols and Data Types](#3.5).\n\n---\n\n## Overview\n\nThe RLM system separates code execution from orchestration using an environment abstraction layer. The `BaseEnv` class [rlm/environments/base_env.py:7-29]() defines a uniform interface that all execution environments must implement, regardless of whether they run locally or in cloud-based sandboxes. This abstraction enables the RLM orchestrator to treat all environments identically while allowing each implementation to use platform-specific execution strategies.\n\n**Sources:** [rlm/environments/base_env.py:1-183](), [README.md:79-97]()\n\n---\n\n## Environment Type Hierarchy\n\nThe environment system is organized into three layers: an abstract base class, two category subclasses, and concrete implementations.\n\n### Class Hierarchy\n\n```mermaid\ngraph TB\n    BaseEnv[\"BaseEnv<br/>(Abstract Base Class)\"]\n    \n    NonIsolatedEnv[\"NonIsolatedEnv<br/>(Abstract Category)\"]\n    IsolatedEnv[\"IsolatedEnv<br/>(Abstract Category)\"]\n    \n    LocalREPL[\"LocalREPL\"]\n    DockerREPL[\"DockerREPL\"]\n    ModalREPL[\"ModalREPL\"]\n    PrimeREPL[\"PrimeREPL\"]\n    DaytonaREPL[\"DaytonaREPL\"]\n    \n    BaseEnv --> NonIsolatedEnv\n    BaseEnv --> IsolatedEnv\n    \n    NonIsolatedEnv --> LocalREPL\n    NonIsolatedEnv --> DockerREPL\n    \n    IsolatedEnv --> ModalREPL\n    IsolatedEnv --> PrimeREPL\n    IsolatedEnv --> DaytonaREPL\n    \n    LocalREPL -.->|implements| Persistent[\"SupportsPersistence<br/>(Protocol)\"]\n```\n\n**Sources:** [rlm/environments/base_env.py:7-74]()\n\n### BaseEnv Interface\n\nThe `BaseEnv` class [rlm/environments/base_env.py:7-29]() defines three abstract methods that all environments must implement:\n\n| Method | Purpose | Return Type |\n|--------|---------|-------------|\n| `setup()` | Initialize the execution environment (e.g., start containers, create sandboxes) | None |\n| `load_context(context_payload)` | Load context data into the environment's namespace | None |\n| `execute_code(code: str)` | Execute Python code and capture results | `REPLResult` |\n\nThe constructor accepts common parameters [rlm/environments/base_env.py:13-16]():\n- `persistent: bool` - Whether the environment persists between completions\n- `depth: int` - Maximum recursion depth for sub-LM calls\n- `**kwargs` - Environment-specific configuration\n\n**Sources:** [rlm/environments/base_env.py:7-29]()\n\n---\n\n## Isolated vs Non-Isolated Environments\n\nThe RLM system categorizes environments based on their isolation model, which determines both their security properties and communication patterns.\n\n### Environment Categories\n\n```mermaid\ngraph LR\n    subgraph NonIsolated[\"NonIsolatedEnv\"]\n        NI_Machine[\"Runs on same machine<br/>as RLM orchestrator\"]\n        NI_Comm[\"Direct socket<br/>communication\"]\n        NI_Access[\"Can access localhost<br/>services directly\"]\n    end\n    \n    subgraph Isolated[\"IsolatedEnv\"]\n        I_Machine[\"Runs on separate<br/>cloud machine\"]\n        I_Comm[\"HTTP broker pattern<br/>with polling\"]\n        I_Access[\"Complete network<br/>isolation\"]\n    end\n    \n    NI_Machine --> NI_Comm\n    NI_Comm --> NI_Access\n    \n    I_Machine --> I_Comm\n    I_Comm --> I_Access\n```\n\n**Sources:** [rlm/environments/base_env.py:31-74](), [README.md:79-97]()\n\n### NonIsolatedEnv\n\nThe `NonIsolatedEnv` class [rlm/environments/base_env.py:53-73]() represents environments that execute on the same physical or virtual machine as the RLM orchestrator. These environments:\n\n- Execute code in the same network context as the host\n- Can connect directly to `localhost` services (including the LM Handler)\n- Use TCP socket communication for sub-LM calls\n- Provide varying levels of process isolation (from none to containerization)\n- Are suitable for trusted code execution scenarios\n\n**Examples:** `LocalREPL` (Python `exec()` in same process), `DockerREPL` (containerized on same host)\n\n**Sources:** [rlm/environments/base_env.py:53-73]()\n\n### IsolatedEnv\n\nThe `IsolatedEnv` class [rlm/environments/base_env.py:31-50]() represents environments that execute on completely separate machines, typically cloud-based sandboxes. These environments:\n\n- Execute code in a separate network context from the host\n- Cannot directly access host services\n- Use HTTP broker pattern with polling for sub-LM calls\n- Provide complete isolation guarantees\n- Are suitable for untrusted code execution scenarios\n\n**Examples:** `ModalREPL` (Modal Sandboxes), `PrimeREPL` (Prime Intellect Sandboxes), `DaytonaREPL` (Daytona workspaces)\n\n**Sources:** [rlm/environments/base_env.py:31-50]()\n\n---\n\n## Core Method Contracts\n\n### setup()\n\nInitializes the execution environment before any code runs. Implementation responsibilities vary by environment type:\n\n| Environment Type | Typical Setup Tasks |\n|------------------|---------------------|\n| `LocalREPL` | Initialize namespace dictionaries, inject helper functions |\n| `DockerREPL` | Start Docker container, mount volumes, configure networking |\n| `ModalREPL` | Create Modal sandbox, establish encrypted tunnel, start broker server |\n| `PrimeREPL` | Create Prime sandbox, expose ports, launch broker server |\n\n**Sources:** [rlm/environments/base_env.py:19-20]()\n\n### load_context(context_payload)\n\nLoads context data into the environment's execution namespace. The `context_payload` parameter accepts three types:\n\n- `str` - Raw text context (e.g., documents, code files)\n- `dict` - Structured data (e.g., configuration, key-value data)\n- `list` - Sequential data (e.g., message histories, data records)\n\nAfter loading, the context must be accessible to executed code through global variables (typically named `context` or `context_0`, `context_1`, etc. for versioned contexts).\n\n**Sources:** [rlm/environments/base_env.py:23-24]()\n\n### execute_code(code: str)\n\nExecutes Python code in the environment and returns a `REPLResult` object containing:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `stdout` | `str` | Standard output captured during execution |\n| `stderr` | `str` | Standard error captured during execution |\n| `locals` | `dict[str, Any]` | Local variables after execution (serializable subset) |\n| `execution_time` | `float` | Time taken to execute the code (seconds) |\n| `rlm_calls` | `list[RLMChatCompletion]` | Sub-LM calls made during execution |\n\n**Sources:** [rlm/environments/base_env.py:27-28](), [rlm/core/types.py]()\n\n---\n\n## Environment Lifecycle in RLM Workflow\n\n### Integration with RLM Orchestrator\n\n```mermaid\ngraph TB\n    User[\"User Code<br/>rlm.completion()\"]\n    RLM[\"RLM Orchestrator<br/>rlm.core.rlm.RLM\"]\n    Factory[\"get_environment()<br/>Factory Function\"]\n    BaseEnv[\"BaseEnv Instance<br/>(LocalREPL, DockerREPL, etc.)\"]\n    Handler[\"LM Handler<br/>TCP Server\"]\n    \n    User -->|\"1. prompt + config\"| RLM\n    RLM -->|\"2. create environment\"| Factory\n    Factory -->|\"3. return instance\"| BaseEnv\n    RLM -->|\"4. setup()\"| BaseEnv\n    RLM -->|\"5. load_context()\"| BaseEnv\n    RLM -->|\"6. start handler\"| Handler\n    \n    Loop[\"Iteration Loop\"]\n    RLM -->|\"7. enter loop\"| Loop\n    Loop -->|\"8. get LM response\"| Handler\n    Loop -->|\"9. execute_code()\"| BaseEnv\n    BaseEnv -->|\"10. sub-LM calls\"| Handler\n    Handler -->|\"11. responses\"| BaseEnv\n    BaseEnv -->|\"12. REPLResult\"| Loop\n    Loop -->|\"13. check final_answer\"| RLM\n    \n    RLM -->|\"14. cleanup() (optional)\"| BaseEnv\n```\n\n**Sources:** [rlm/environments/base_env.py:7-74](), [README.md:66-77]()\n\n### Execution Sequence\n\n1. **Factory Creation**: The RLM orchestrator calls `get_environment(environment_type, **kwargs)` to instantiate the appropriate environment class\n2. **Setup Phase**: `setup()` initializes the environment (start containers, create sandboxes, etc.)\n3. **Context Loading**: `load_context(context_payload)` makes context data available to executed code\n4. **Handler Initialization**: LM Handler starts and communicates its address to the environment\n5. **Iteration Loop**: For each iteration:\n   - LM Handler provides code blocks from the LLM response\n   - `execute_code()` runs each code block\n   - Environment may make sub-LM calls back to the handler\n   - Results are aggregated into `REPLResult` objects\n6. **Cleanup Phase** (optional): Environment-specific cleanup (stop containers, destroy sandboxes)\n\n**Sources:** [rlm/environments/base_env.py:7-74]()\n\n---\n\n## SupportsPersistence Protocol\n\nThe `SupportsPersistence` protocol [rlm/environments/base_env.py:76-182]() defines an interface for environments that support multi-turn conversations with state preservation. It is implemented as a runtime-checkable protocol, allowing type-safe checking via `isinstance(env, SupportsPersistence)`.\n\n### Protocol Methods\n\n```mermaid\ngraph TB\n    Protocol[\"SupportsPersistence Protocol\"]\n    \n    UpdateAddr[\"update_handler_address()<br/>Store new handler address\"]\n    AddContext[\"add_context()<br/>Add versioned context<br/>context_0, context_1, ...\"]\n    GetContext[\"get_context_count()<br/>Return number of contexts\"]\n    AddHistory[\"add_history()<br/>Add versioned history<br/>history_0, history_1, ...\"]\n    GetHistory[\"get_history_count()<br/>Return number of histories\"]\n    \n    Protocol --> UpdateAddr\n    Protocol --> AddContext\n    Protocol --> GetContext\n    Protocol --> AddHistory\n    Protocol --> GetHistory\n```\n\n**Sources:** [rlm/environments/base_env.py:76-182]()\n\n### Method Specifications\n\n| Method | Parameters | Return | Purpose |\n|--------|-----------|--------|---------|\n| `update_handler_address` | `address: tuple[str, int]` | `None` | Update LM Handler location for sub-calls |\n| `add_context` | `context_payload, context_index: int \\| None` | `int` | Add context, return assigned index |\n| `get_context_count` | None | `int` | Return total number of contexts |\n| `add_history` | `message_history: list[dict], history_index: int \\| None` | `int` | Add conversation history, return index |\n| `get_history_count` | None | `int` | Return total number of histories |\n\n**Sources:** [rlm/environments/base_env.py:108-182]()\n\n### Versioning Behavior\n\nContexts and histories use numeric versioning [rlm/environments/base_env.py:88-96]():\n\n- **Versioned Names**: `context_0`, `context_1`, `context_2`, ..., `history_0`, `history_1`, ...\n- **Unversioned Aliases**: `context` → `context_0`, `history` → `history_0`\n- **Auto-Increment**: Passing `context_index=None` automatically assigns the next available index\n- **Explicit Index**: Passing a specific index allows overwriting or skipping indices\n\n**Example:**\n```python\nenv.add_context(\"first context\")    # Creates context_0 and context alias\nenv.add_context(\"second context\")   # Creates context_1\nenv.add_context(\"explicit\", context_index=5)  # Creates context_5\n```\n\n**Sources:** [rlm/environments/base_env.py:88-140]()\n\n### Implementation Requirements\n\nEnvironments implementing this protocol must:\n\n1. **Store Deep Copies**: The `add_history()` method must store a deep copy of the message list [rlm/environments/base_env.py:164-165]()\n2. **Maintain Aliases**: Always ensure unversioned names point to index 0 [rlm/environments/base_env.py:93-96]()\n3. **Persist State**: Preserve all contexts and histories across multiple `execute_code()` calls\n4. **Update Handler**: Store the latest handler address for `llm_query()` functions [rlm/environments/base_env.py:108-118]()\n\n**Reference Implementation**: `LocalREPL` [rlm/environments/local_repl.py]() provides a complete implementation of this protocol.\n\n**Sources:** [rlm/environments/base_env.py:76-182]()\n\n---\n\n## Factory Pattern and Environment Selection\n\n### get_environment Factory Function\n\nThe RLM system uses a factory function to instantiate environments based on string identifiers:\n\n```mermaid\ngraph LR\n    User[\"User: environment='local'\"]\n    Factory[\"get_environment()\"]\n    Mapping[\"Environment Registry\"]\n    Instance[\"Environment Instance\"]\n    \n    User --> Factory\n    Factory --> Mapping\n    Mapping -->|\"'local' → LocalREPL\"| Instance\n    Mapping -->|\"'docker' → DockerREPL\"| Instance\n    Mapping -->|\"'modal' → ModalREPL\"| Instance\n    Mapping -->|\"'prime' → PrimeREPL\"| Instance\n    Mapping -->|\"'daytona' → DaytonaREPL\"| Instance\n```\n\n**Sources:** [README.md:82-87]()\n\n### Usage Pattern\n\n```python\nfrom rlm import RLM\n\n# Factory is invoked internally\nrlm = RLM(\n    environment=\"local\",           # String identifier\n    environment_kwargs={            # Passed to __init__\n        \"persistent\": True,\n        \"depth\": 3,\n        # ... environment-specific kwargs\n    }\n)\n```\n\nThe factory pattern enables:\n- Runtime selection of execution environments\n- Configuration through string identifiers\n- Easy addition of new environment types\n- Separation of interface from implementation\n\n**Sources:** [README.md:82-87]()\n\n---\n\n## Communication Model Overview\n\nThe environment abstraction layer decouples communication logic from execution logic. Each environment category uses a different communication pattern to handle sub-LM calls made during code execution.\n\n### Communication Patterns by Environment Type\n\n| Environment Category | Communication Pattern | Implementation Location |\n|---------------------|----------------------|------------------------|\n| `NonIsolatedEnv` | Direct TCP Socket | Environment can connect to handler's localhost port |\n| `IsolatedEnv` | HTTP Broker + Polling | Broker server in sandbox, poller thread on host |\n\nFor detailed explanations of these communication protocols, including message formats, socket protocols, and broker architectures, see [Section 3.5: Communication Protocols and Data Types](#3.5).\n\n**Sources:** [rlm/environments/base_env.py:31-74]()\n\n---\n\n## Summary\n\nThe environment abstraction layer provides a clean separation between orchestration and execution in the RLM system. Key architectural principles:\n\n1. **Uniform Interface**: All environments implement `BaseEnv` with identical method signatures\n2. **Category-Based Design**: `IsolatedEnv` and `NonIsolatedEnv` encode fundamental differences in execution context\n3. **Protocol Extension**: `SupportsPersistence` allows optional features without breaking the base abstraction\n4. **Factory Pattern**: Runtime selection of environments through string identifiers\n5. **Communication Abstraction**: Each category handles sub-LM calls using appropriate networking strategies\n\nThis design enables the RLM orchestrator to remain environment-agnostic while supporting execution contexts ranging from same-process Python `exec()` to isolated cloud sandboxes.\n\n**Sources:** [rlm/environments/base_env.py:1-183](), [README.md:79-97]()\n\n---\n\n# Page: Recursion and Sub-Calls\n\n# Recursion and Sub-Calls\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [AGENTS.md](AGENTS.md)\n- [examples/lm_in_prime_repl.py](examples/lm_in_prime_repl.py)\n- [examples/prime_repl_example.py](examples/prime_repl_example.py)\n- [rlm/core/comms_utils.py](rlm/core/comms_utils.py)\n- [rlm/core/lm_handler.py](rlm/core/lm_handler.py)\n- [rlm/core/rlm.py](rlm/core/rlm.py)\n- [rlm/environments/constants.py](rlm/environments/constants.py)\n- [tests/test_imports.py](tests/test_imports.py)\n\n</details>\n\n\n\n## Overview\n\nThis page explains how the RLM system enables recursive LLM calls through the `depth` parameter and `llm_query()` functions. Code executed within REPL environments can make additional LLM queries back to the `LMHandler`, enabling language models to programmatically decompose problems and delegate sub-tasks.\n\nThe recursion mechanism consists of three key components:\n\n1. **Depth Parameter**: Controls recursion depth and enables depth-based routing between LLM backends\n2. **llm_query Functions**: `llm_query(prompt, model)` and `llm_query_batched(prompts, model)` exposed in REPL globals\n3. **LMHandler Routing**: Routes requests to different LLM clients based on depth level\n\n**Sources**: [rlm/core/rlm.py:46](), [rlm/core/rlm.py:62-63](), [rlm/core/lm_handler.py:135-150]()\n\n---\n\n## Depth Parameter and Routing\n\n### Depth Configuration\n\nThe `RLM` class uses `depth` to control the recursion level and route sub-calls to different LLM backends:\n\n| Parameter | Type | Default | Purpose |\n|-----------|------|---------|---------|\n| `depth` | `int` | `0` | Current recursion depth (0-indexed) |\n| `max_depth` | `int` | `1` | Maximum allowed depth |\n| `other_backends` | `list[ClientBackend]` | `None` | Additional backends for depth=1 calls |\n| `other_backend_kwargs` | `list[dict]` | `None` | Configuration for additional backends |\n\n**RLM Initialization** ([rlm/core/rlm.py:46](), [rlm/core/rlm.py:62-63]()):\n```python\ndef __init__(\n    self,\n    depth: int = 0,\n    max_depth: int = 1,\n    other_backends: list[ClientBackend] | None = None,\n    other_backend_kwargs: list[dict[str, Any]] | None = None,\n    ...\n):\n```\n\nWhen an environment is created, it receives `depth = RLM.depth + 1` ([rlm/core/rlm.py:167]()):\n```python\nenv_kwargs[\"depth\"] = self.depth + 1  # Environment depth is RLM depth + 1\n```\n\n**Sources**: [rlm/core/rlm.py:40-55](), [rlm/core/rlm.py:167]()\n\n### Depth-Based Routing in LMHandler\n\nThe `LMHandler` routes LLM requests based on depth, enabling hierarchical model selection:\n\n**Routing Logic** ([rlm/core/lm_handler.py:135-150]()):\n\n```mermaid\ngraph TB\n    Request[\"LMRequest<br/>(depth, model)\"]\n    GetClient[\"LMHandler.get_client()\"]\n    \n    CheckModel{\"model<br/>specified?\"}\n    CheckRegistered{\"model in<br/>clients?\"}\n    CheckDepth{\"depth == 1?\"}\n    CheckOther{\"other_backend_client<br/>exists?\"}\n    \n    UseRegistered[\"Use clients[model]\"]\n    UseOther[\"Use other_backend_client\"]\n    UseDefault[\"Use default_client\"]\n    \n    Request --> GetClient\n    GetClient --> CheckModel\n    \n    CheckModel -->|\"Yes\"| CheckRegistered\n    CheckModel -->|\"No\"| CheckDepth\n    \n    CheckRegistered -->|\"Yes\"| UseRegistered\n    CheckRegistered -->|\"No\"| CheckDepth\n    \n    CheckDepth -->|\"Yes\"| CheckOther\n    CheckDepth -->|\"No\"| UseDefault\n    \n    CheckOther -->|\"Yes\"| UseOther\n    CheckOther -->|\"No\"| UseDefault\n```\n\n**Implementation** ([rlm/core/lm_handler.py:135-150]()):\n```python\ndef get_client(self, model: str | None = None, depth: int = 0) -> BaseLM:\n    \"\"\"Get client by model name or depth, or return default.\n    \n    Routing logic:\n    - depth=0: use default_client (main backend)\n    - depth=1: use other_backend_client if it exists, otherwise default_client\n    - If model is specified and exists in clients, use that (overrides depth routing)\n    \"\"\"\n    if model and model in self.clients:\n        return self.clients[model]\n    \n    # Route based on depth\n    if depth == 1 and self.other_backend_client is not None:\n        return self.other_backend_client\n    \n    return self.default_client\n```\n\n**Sources**: [rlm/core/lm_handler.py:114-150](), [rlm/core/rlm.py:136-139]()\n\n### Depth in LMRequest\n\nAll LLM requests include a `depth` field that determines routing:\n\n**LMRequest Structure** ([rlm/core/comms_utils.py:22-58]()):\n\n| Field | Type | Purpose |\n|-------|------|---------|\n| `prompt` | `str \\| dict \\| None` | Single prompt |\n| `prompts` | `list[str \\| dict] \\| None` | Batched prompts |\n| `model` | `str \\| None` | Optional model specification |\n| `depth` | `int` | Recursion depth for routing (default 0) |\n\n**Example Flow**:\n1. Root RLM (depth=0) creates environment with depth=1\n2. Code in environment calls `llm_query(prompt)`\n3. Environment creates `LMRequest(prompt=prompt, depth=1)`\n4. `LMHandler.get_client()` routes to `other_backend_client` based on depth=1\n5. Response returned to executing code\n\n**Sources**: [rlm/core/comms_utils.py:22-58](), [rlm/core/lm_handler.py:46-65]()\n\n---\n\n## Recursion Architecture\n\nThe RLM system implements recursion through a communication-based architecture where REPL environments send LLM requests back to the `LMHandler` with depth information:\n\n**High-Level Architecture**:\n\n```mermaid\ngraph TB\n    subgraph \"RLM Process (depth=0)\"\n        RLM[\"RLM Core<br/>(depth=0)\"]\n        Handler[\"LMHandler\"]\n        DefaultClient[\"default_client<br/>(depth=0)\"]\n        OtherClient[\"other_backend_client<br/>(depth=1)\"]\n        \n        Handler --> DefaultClient\n        Handler --> OtherClient\n    end\n    \n    subgraph \"REPL Environment (depth=1)\"\n        Env[\"LocalREPL/DockerREPL\"]\n        Code[\"Executing Code<br/>exec()\"]\n        LLMQuery[\"llm_query()<br/>llm_query_batched()\"]\n        \n        Code --> LLMQuery\n    end\n    \n    subgraph \"External APIs\"\n        API1[\"LLM API<br/>(root model)\"]\n        API2[\"LLM API<br/>(sub-call model)\"]\n    end\n    \n    RLM -->|\"1. Request completion\"| Handler\n    Handler -->|\"2. Route by depth=0\"| DefaultClient\n    DefaultClient -->|\"3. API call\"| API1\n    API1 -->|\"4. Response with code\"| DefaultClient\n    DefaultClient -->|\"5. Return\"| RLM\n    \n    RLM -->|\"6. Execute code\"| Code\n    Code -->|\"7. Call llm_query()\"| LLMQuery\n    LLMQuery -->|\"8. LMRequest(depth=1)\"| Handler\n    Handler -->|\"9. Route by depth=1\"| OtherClient\n    OtherClient -->|\"10. API call\"| API2\n    API2 -->|\"11. Sub-response\"| OtherClient\n    OtherClient -->|\"12. Return\"| LLMQuery\n    LLMQuery -->|\"13. Return string\"| Code\n    Code -->|\"14. REPLResult\"| RLM\n```\n\n**Sources**: [rlm/core/rlm.py:133-149](), [rlm/core/lm_handler.py:135-150]()\n\n---\n\n## The `llm_query()` and `llm_query_batched()` Functions\n\n### Function Interface\n\nBoth functions are exposed as global functions within REPL environments, allowing LLM-generated code to make recursive LLM calls:\n\n| Function | Signature | Returns | Purpose |\n|----------|-----------|---------|---------|\n| `llm_query()` | `(prompt: str \\| dict, model: str \\| None = None)` | `str` | Single LLM query |\n| `llm_query_batched()` | `(prompts: list[str \\| dict], model: str \\| None = None)` | `list[str]` | Concurrent batch queries |\n\n**Parameters**:\n- `prompt` / `prompts`: Query prompt(s) as string or message list\n- `model`: Optional model name to override depth-based routing\n\n**Sources**: [rlm/core/comms_utils.py:22-58]()\n\n### Registration in Environment Globals\n\nThe functions are registered in each REPL environment's global namespace during `setup()`:\n\n**LocalREPL Registration** - Uses instance methods as closures:\n```python\n# From setup() method\nself.globals[\"llm_query\"] = self._llm_query\nself.globals[\"llm_query_batched\"] = self._llm_query_batched\n```\n\n**DockerREPL Registration** - Injects functions into container execution script:\n```python\n# Functions defined in exec_script.py\ndef llm_query(prompt, model=None):\n    # HTTP POST to proxy server\n    ...\n\n_globals = {\n    \"__builtins__\": __builtins__,\n    \"__name__\": \"__main__\",\n    \"llm_query\": llm_query,\n    \"llm_query_batched\": llm_query_batched,\n    ...\n}\n```\n\n**Sources**: [rlm/environments/local_repl.py:159-160](), [rlm/environments/docker_repl.py:146]()\n\n### Usage in Generated Code\n\nLLM-generated code can call these functions naturally:\n\n```python\n# Single query (uses depth-based routing)\nsummary = llm_query(\"Summarize the following text: \" + context[:10000])\n\n# Batched queries with explicit model\nquestions = [\"What is X?\", \"What is Y?\", \"What is Z?\"]\nanswers = llm_query_batched(questions, model=\"gpt-4\")\n\n# Message list format\nresponse = llm_query([\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What is 2+2?\"}\n])\n```\n\nThe `model` parameter overrides depth-based routing when specified, allowing fine-grained control over which LLM backend processes each sub-call.\n\n**Sources**: [rlm/core/lm_handler.py:135-150](), [rlm/core/comms_utils.py:22-58]()\n\n---\n\n## Communication Protocols\n\n### Socket Protocol Overview\n\nAll environment-to-LMHandler communication uses a 4-byte length-prefixed JSON protocol defined in `comms_utils.py`:\n\n**Protocol Format** ([rlm/core/comms_utils.py:146-176]()):\n```\n[4-byte big-endian length][UTF-8 JSON payload]\n```\n\n**Helper Functions**:\n- `socket_send(sock, data)`: Send length-prefixed JSON\n- `socket_recv(sock)`: Receive length-prefixed JSON\n- `socket_request(address, data)`: Open connection, send, receive, close\n- `send_lm_request(address, request)`: Typed LMRequest wrapper\n\n**Sources**: [rlm/core/comms_utils.py:146-197]()\n\n### Non-Isolated Environments: Direct Socket Communication\n\nNon-isolated environments like `LocalREPL` and `DockerREPL` use direct TCP socket connections to the `LMHandler`.\n\n**LocalREPL Socket Flow**:\n\n```mermaid\nsequenceDiagram\n    participant Code as \"Executing Code\"\n    participant LLMQuery as \"_llm_query()\"\n    participant Socket as \"socket_request()\"\n    participant Handler as \"LMHandler<br/>(TCP server)\"\n    participant Client as \"BaseLM Client\"\n    \n    Code->>LLMQuery: \"llm_query(prompt)\"\n    LLMQuery->>LLMQuery: \"Create LMRequest<br/>(depth=env.depth)\"\n    LLMQuery->>Socket: \"send_lm_request(address, request)\"\n    \n    Socket->>Handler: \"TCP: 4-byte length + JSON\"\n    Handler->>Handler: \"Parse LMRequest\"\n    Handler->>Handler: \"get_client(model, depth)\"\n    Handler->>Client: \"completion(prompt)\"\n    Client-->>Handler: \"response string\"\n    Handler->>Handler: \"Create LMResponse\"\n    Handler-->>Socket: \"TCP: 4-byte length + JSON\"\n    \n    Socket-->>LLMQuery: \"LMResponse object\"\n    LLMQuery->>LLMQuery: \"Track in _pending_llm_calls\"\n    LLMQuery-->>Code: \"return response.chat_completion.response\"\n```\n\n**LocalREPL Implementation** ([rlm/environments/local_repl.py:169-194]()):\n```python\ndef _llm_query(self, prompt: str | dict[str, Any], model: str | None = None) -> str:\n    if not self.lm_handler_address:\n        return \"Error: No LM handler configured\"\n    \n    try:\n        request = LMRequest(prompt=prompt, model=model, depth=self.depth)\n        response = send_lm_request(self.lm_handler_address, request)\n        \n        if not response.success:\n            return f\"Error: {response.error}\"\n        \n        # Track the call for REPLResult\n        self._pending_llm_calls.append(response.chat_completion)\n        return response.chat_completion.response\n    except Exception as e:\n        return f\"Error: LM query failed - {e}\"\n```\n\n**DockerREPL**: Uses HTTP proxy on host that forwards to socket (see page 5.3 for details).\n\n**Sources**: [rlm/environments/local_repl.py:169-194](), [rlm/core/comms_utils.py:204-224](), [rlm/core/lm_handler.py:17-97]()\n\n### Isolated Environments: HTTP Broker Pattern\n\nIsolated environments (Modal, Prime, Daytona) cannot directly connect to the host's TCP server. They use an HTTP broker pattern with polling. See page 5.4 (Cloud Sandbox Environments) for details on this pattern.\n\n**Key Difference**: Instead of direct socket connections, isolated environments:\n1. POST requests to a broker server running inside the sandbox\n2. Host polls the broker for pending requests\n3. Host forwards to `LMHandler` via socket\n4. Host posts responses back to broker\n\n**Sources**: [rlm/environments/modal_repl.py](), [rlm/environments/prime_repl.py](), [AGENTS.md:257-319]()\n\n---\n\n## Complete Execution Flow with Depth Routing\n\n### End-to-End Call Sequence\n\nThe following diagram shows a complete recursive call with depth-based routing:\n\n**Execution Flow with Depth Routing**:\n\n```mermaid\nsequenceDiagram\n    participant RLM as \"RLM(depth=0)\"\n    participant Handler as \"LMHandler\"\n    participant Default as \"default_client<br/>(depth=0)\"\n    participant Other as \"other_backend_client<br/>(depth=1)\"\n    participant Env as \"LocalREPL(depth=1)\"\n    participant Code as \"exec(code)\"\n    \n    Note over RLM: \"Root-level completion\"\n    RLM->>Handler: \"completion(prompt)\"\n    Handler->>Handler: \"get_client(depth=0)\"\n    Handler->>Default: \"completion(prompt)\"\n    Default-->>Handler: \"response with code blocks\"\n    Handler-->>RLM: \"response\"\n    \n    Note over RLM: \"Extract and execute code\"\n    RLM->>RLM: \"find_code_blocks(response)\"\n    RLM->>Env: \"execute_code(code_block)\"\n    \n    Env->>Env: \"Clear _pending_llm_calls\"\n    Env->>Code: \"exec(code, globals, locals)\"\n    \n    Note over Code: \"Code execution begins\"\n    Code->>Code: \"Calls llm_query(prompt)\"\n    \n    Note over Code,Handler: \"Sub-call with depth=1\"\n    Code->>Env: \"_llm_query(prompt)\"\n    Env->>Env: \"Create LMRequest(depth=1)\"\n    Env->>Handler: \"send_lm_request(address, request)\"\n    \n    Handler->>Handler: \"get_client(model=None, depth=1)\"\n    Handler->>Other: \"completion(prompt)\"\n    Other-->>Handler: \"sub-response\"\n    Handler->>Handler: \"Create LMResponse\"\n    Handler-->>Env: \"LMResponse\"\n    \n    Env->>Env: \"Track: _pending_llm_calls.append(...)\"\n    Env-->>Code: \"return response string\"\n    \n    Note over Code: \"Continue execution\"\n    Code-->>Env: \"Execution completes\"\n    \n    Env->>Env: \"Capture stdout/stderr/locals\"\n    Env->>Env: \"Copy _pending_llm_calls\"\n    Env-->>RLM: \"REPLResult(rlm_calls=[...])\"\n    \n    RLM->>RLM: \"Create CodeBlock(code, result)\"\n    RLM->>RLM: \"Check for final_answer\"\n```\n\n**Key Observations**:\n1. Root RLM completion uses `depth=0` → routes to `default_client`\n2. Environment receives `depth=1` during creation\n3. Sub-calls from code use `depth=1` → routes to `other_backend_client`\n4. All sub-calls tracked in `REPLResult.rlm_calls`\n\n**Sources**: [rlm/core/rlm.py:297-322](), [rlm/core/lm_handler.py:46-65](), [rlm/core/lm_handler.py:135-150](), [rlm/environments/local_repl.py:261-292]()\n\n### LocalREPL Implementation Details\n\n**Key Implementation** ([rlm/environments/local_repl.py:169-194]()):\n\n```python\ndef _llm_query(self, prompt: str | dict[str, Any], model: str | None = None) -> str:\n    \"\"\"Make a single LLM query via the handler.\"\"\"\n    if not self.lm_handler_address:\n        return \"Error: No LM handler configured\"\n    \n    try:\n        # Create request with environment's depth\n        request = LMRequest(prompt=prompt, model=model, depth=self.depth)\n        \n        # Send via socket protocol\n        response = send_lm_request(self.lm_handler_address, request)\n        \n        if not response.success:\n            return f\"Error: {response.error}\"\n        \n        # Track for REPLResult.rlm_calls\n        self._pending_llm_calls.append(response.chat_completion)\n        return response.chat_completion.response\n    except Exception as e:\n        return f\"Error: LM query failed - {e}\"\n```\n\n**Batched Implementation** ([rlm/environments/local_repl.py:195-223]()):\n```python\ndef _llm_query_batched(\n    self, prompts: list[str | dict[str, Any]], model: str | None = None\n) -> list[str]:\n    \"\"\"Make batched concurrent LLM queries.\"\"\"\n    if not self.lm_handler_address:\n        return [f\"Error: No LM handler configured\"] * len(prompts)\n    \n    try:\n        # Send batched request with environment's depth\n        responses = send_lm_request_batched(\n            self.lm_handler_address, prompts, model=model, depth=self.depth\n        )\n        \n        results = []\n        for response in responses:\n            if not response.success:\n                results.append(f\"Error: {response.error}\")\n            else:\n                self._pending_llm_calls.append(response.chat_completion)\n                results.append(response.chat_completion.response)\n        \n        return results\n    except Exception as e:\n        return [f\"Error: Batched LM query failed - {e}\"] * len(prompts)\n```\n\n**Sources**: [rlm/environments/local_repl.py:169-223]()\n\n---\n\n## Tracking Recursive Calls\n\n### REPLResult Structure\n\nAll recursive LLM calls made during code execution are tracked in the `REPLResult.rlm_calls` field:\n\n**REPLResult Fields**:\n\n| Field | Type | Purpose |\n|-------|------|---------|\n| `stdout` | `str` | Captured standard output |\n| `stderr` | `str` | Captured standard error |\n| `locals` | `dict[str, Any]` | Namespace variables after execution |\n| `execution_time` | `float` | Time taken to execute code (seconds) |\n| `rlm_calls` | `list[RLMChatCompletion]` | All recursive LLM calls with full metadata |\n\n**Sources**: [rlm/core/types.py]()\n\n### Call Tracking Implementation\n\n**LocalREPL Tracking Flow** ([rlm/environments/local_repl.py:261-292]()):\n\n```python\ndef execute_code(self, code: str) -> REPLResult:\n    \"\"\"Execute code and track all sub-LLM calls.\"\"\"\n    start_time = time.perf_counter()\n    \n    # Clear tracking list at start\n    self._pending_llm_calls = []\n    \n    # Execute code (may call llm_query multiple times)\n    # ... exec(code, self.globals, self.locals) ...\n    \n    # Each llm_query call appends to _pending_llm_calls\n    \n    # Return result with all tracked calls\n    return REPLResult(\n        stdout=captured_stdout,\n        stderr=captured_stderr,\n        locals=self.locals.copy(),\n        execution_time=time.perf_counter() - start_time,\n        rlm_calls=self._pending_llm_calls.copy(),  # Complete trace\n    )\n```\n\n**Append on Each Sub-Call** ([rlm/environments/local_repl.py:187-189]()):\n```python\n# In _llm_query method\nself._pending_llm_calls.append(response.chat_completion)\n```\n\n**Sources**: [rlm/environments/local_repl.py:261-292](), [rlm/environments/local_repl.py:187-189]()\n\n### Hierarchical Call Structure\n\nThe RLM creates a hierarchical structure of calls:\n\n**Data Structure Hierarchy**:\n\n```mermaid\ngraph TB\n    RLMCompletion[\"RLMChatCompletion<br/>(root completion)\"]\n    Iteration1[\"RLMIteration 1\"]\n    Iteration2[\"RLMIteration 2\"]\n    \n    CB1[\"CodeBlock 1<br/>(code + result)\"]\n    CB2[\"CodeBlock 2<br/>(code + result)\"]\n    CB3[\"CodeBlock 3<br/>(code + result)\"]\n    \n    Result1[\"REPLResult 1<br/>(rlm_calls=[...])\"]\n    Result2[\"REPLResult 2<br/>(rlm_calls=[...])\"]\n    Result3[\"REPLResult 3<br/>(rlm_calls=[...])\"]\n    \n    SubCall1[\"RLMChatCompletion<br/>(sub-call 1)\"]\n    SubCall2[\"RLMChatCompletion<br/>(sub-call 2)\"]\n    SubCall3[\"RLMChatCompletion<br/>(sub-call 3)\"]\n    \n    RLMCompletion --> Iteration1\n    RLMCompletion --> Iteration2\n    \n    Iteration1 --> CB1\n    Iteration1 --> CB2\n    Iteration2 --> CB3\n    \n    CB1 --> Result1\n    CB2 --> Result2\n    CB3 --> Result3\n    \n    Result1 --> SubCall1\n    Result1 --> SubCall2\n    Result2 --> SubCall3\n```\n\n**Type Composition** (from `rlm/core/types.py`):\n- `RLMChatCompletion` contains execution metadata\n- `RLMIteration` contains list of `CodeBlock`\n- `CodeBlock` contains code string and `REPLResult`\n- `REPLResult` contains `rlm_calls: list[RLMChatCompletion]`\n\nThis enables complete traceability: every sub-LLM call is associated with the code block that invoked it, which is associated with the iteration that generated it.\n\n**Sources**: [rlm/core/types.py](), [rlm/core/rlm.py:307-322]()\n\n---\n\n## Depth Configuration Examples\n\n### Example 1: Using Different Models by Depth\n\nConfigure the RLM to use a powerful model for root reasoning and a cheaper model for sub-calls:\n\n```python\nfrom rlm import RLM\n\nrlm = RLM(\n    backend=\"openai\",\n    backend_kwargs={\n        \"api_key\": \"...\",\n        \"model_name\": \"gpt-4\",  # Root model (depth=0)\n    },\n    other_backends=[\"openai\"],\n    other_backend_kwargs=[{\n        \"api_key\": \"...\",\n        \"model_name\": \"gpt-3.5-turbo\",  # Sub-call model (depth=1)\n    }],\n    max_depth=1,  # Allow one level of recursion\n)\n\n# Root call uses gpt-4\n# Any llm_query() from code uses gpt-3.5-turbo\nresult = rlm.completion(\"Analyze this dataset and summarize each section...\")\n```\n\n**Sources**: [rlm/core/rlm.py:40-93](), [rlm/core/lm_handler.py:114-150]()\n\n### Example 2: Prime REPL with Recursive Calls\n\nUsing `PrimeREPL` to execute code in a cloud sandbox with sub-LLM calls ([examples/lm_in_prime_repl.py]()):\n\n```python\nfrom rlm.clients.portkey import PortkeyClient\nfrom rlm.core.lm_handler import LMHandler\nfrom rlm.environments.prime_repl import PrimeREPL\n\n# Setup client and handler\nclient = PortkeyClient(api_key=\"...\", model_name=\"@openai/gpt-5-nano\")\n\nwith LMHandler(client=client) as handler:\n    # Create Prime sandbox with handler connection\n    with PrimeREPL(\n        name=\"rlm-demo\",\n        docker_image=\"python:3.11-slim\",\n        lm_handler_address=handler.address,\n        context_payload=\"Test context data\",\n    ) as repl:\n        \n        # Execute code that makes sub-calls\n        code = '''\nresponse = llm_query(\"What is 2 + 2?\")\nprint(f\"LLM says: {response}\")\n\n# Access context\nprint(f\"Context: {context}\")\n'''\n        \n        result = repl.execute_code(code)\n        print(f\"Output: {result.stdout}\")\n        print(f\"Sub-calls made: {len(result.rlm_calls)}\")\n```\n\n**Key Points**:\n- Prime sandbox runs in cloud, isolated from host\n- `llm_query()` works seamlessly via HTTP broker pattern\n- All sub-calls tracked in `result.rlm_calls`\n\n**Sources**: [examples/lm_in_prime_repl.py:1-77](), [examples/prime_repl_example.py:1-31]()\n\n### Example 3: Batched Recursive Calls\n\nEfficiently process multiple sub-queries in parallel:\n\n```python\ncode = '''\n# Decompose into sub-questions\nquestions = [\n    \"What is the population of Tokyo?\",\n    \"What is the population of New York?\",\n    \"What is the population of London?\",\n]\n\n# Batched concurrent queries (uses async under the hood)\nanswers = llm_query_batched(questions)\n\n# Process results\nfor q, a in zip(questions, answers):\n    print(f\"{q} -> {a}\")\n'''\n\nresult = env.execute_code(code)\nprint(f\"Made {len(result.rlm_calls)} sub-calls\")\n# Output: Made 3 sub-calls\n```\n\nThe `llm_query_batched()` function sends all requests concurrently via `asyncio.gather()`, significantly reducing total latency compared to sequential calls.\n\n**Sources**: [rlm/environments/local_repl.py:195-223](), [rlm/core/lm_handler.py:67-96]()\n\n---\n\n## Error Handling\n\n### Error Handling Strategy\n\nAll `llm_query()` functions return error messages as strings rather than raising exceptions, enabling LLM-generated code to handle errors programmatically.\n\n**LocalREPL Error Cases** ([rlm/environments/local_repl.py:169-194]()):\n\n| Error Type | Check | Return Value |\n|------------|-------|--------------|\n| No handler configured | `if not self.lm_handler_address` | `\"Error: No LM handler configured\"` |\n| Request failed | `if not response.success` | `f\"Error: {response.error}\"` |\n| Exception during call | `except Exception as e` | `f\"Error: LM query failed - {e}\"` |\n\n**Implementation** ([rlm/environments/local_repl.py:169-194]()):\n```python\ndef _llm_query(self, prompt: str | dict[str, Any], model: str | None = None) -> str:\n    \"\"\"Returns error string instead of raising exception.\"\"\"\n    if not self.lm_handler_address:\n        return \"Error: No LM handler configured\"\n    \n    try:\n        request = LMRequest(prompt=prompt, model=model, depth=self.depth)\n        response = send_lm_request(self.lm_handler_address, request)\n        \n        if not response.success:\n            return f\"Error: {response.error}\"  # Handler-side error\n        \n        self._pending_llm_calls.append(response.chat_completion)\n        return response.chat_completion.response\n    except Exception as e:\n        return f\"Error: LM query failed - {e}\"  # Network/socket error\n```\n\n**Benefits**:\n- Code execution continues even if a sub-call fails\n- LLM can detect errors via string matching\n- Enables retry logic and fallback strategies in generated code\n\n**Sources**: [rlm/environments/local_repl.py:169-194]()\n\n### Batched Request Error Handling\n\nBatched requests handle errors per-prompt, returning error strings in place of failed responses ([rlm/environments/local_repl.py:195-223]()):\n\n```python\ndef _llm_query_batched(\n    self, prompts: list[str | dict[str, Any]], model: str | None = None\n) -> list[str]:\n    \"\"\"Returns list of responses/errors, one per prompt.\"\"\"\n    if not self.lm_handler_address:\n        return [f\"Error: No LM handler configured\"] * len(prompts)\n    \n    try:\n        responses = send_lm_request_batched(\n            self.lm_handler_address, prompts, model=model, depth=self.depth\n        )\n        \n        results = []\n        for response in responses:\n            if not response.success:\n                results.append(f\"Error: {response.error}\")  # Per-prompt error\n            else:\n                self._pending_llm_calls.append(response.chat_completion)\n                results.append(response.chat_completion.response)\n        \n        return results\n    except Exception as e:\n        return [f\"Error: Batched LM query failed - {e}\"] * len(prompts)\n```\n\nThis ensures the result list always has the same length as the input prompt list, maintaining alignment.\n\n**Sources**: [rlm/environments/local_repl.py:195-223]()\n\n---\n\n## Practical Considerations\n\n### Model Specification\n\nThe `model` parameter in both `llm_query()` and `llm_query_batched()` allows code to specify which LLM backend to use:\n\n```python\n# Use default model\nresult1 = llm_query(\"Question 1\")\n\n# Specify different model\nresult2 = llm_query(\"Complex question\", model=\"gpt-4\")\n```\n\nThis enables flexible model selection within recursive calls, allowing cheaper models for simple tasks and more capable models for complex sub-queries.\n\n**Sources**: [rlm/environments/local_repl.py:169-223]()\n\n### Batched Requests for Efficiency\n\nThe `llm_query_batched()` function enables concurrent processing of multiple prompts:\n\n**Benefits**:\n- Parallel API requests reduce total latency\n- Efficient for decomposing tasks into independent sub-questions\n- Returns results in same order as input prompts\n\n**Implementation** ([rlm/environments/local_repl.py:195-223]()):\n```python\nresponses = send_lm_request_batched(self.lm_handler_address, prompts, model=model)\n\nresults = []\nfor response in responses:\n    if not response.success:\n        results.append(f\"Error: {response.error}\")\n    else:\n        self._pending_llm_calls.append(response.chat_completion)\n        results.append(response.chat_completion.response)\n```\n\n**Sources**: [rlm/environments/local_repl.py:195-223](), [rlm/core/comms_utils.py]()\n\n### Thread Safety\n\n**LocalREPL** uses a lock for thread-safe output capture ([rlm/environments/local_repl.py:242-249]()):\n```python\nwith self._lock:\n    old_stdout, old_stderr = sys.stdout, sys.stderr\n    stdout_buf, stderr_buf = io.StringIO(), io.StringIO()\n    ...\n```\n\n**DockerREPL** uses locks for shared state access ([rlm/environments/docker_repl.py:64-65]()):\n```python\nwith self.lock:\n    self.pending_calls.append(response.chat_completion)\n```\n\nThis ensures correct behavior when multiple threads make concurrent recursive calls.\n\n**Sources**: [rlm/environments/local_repl.py:132](), [rlm/environments/local_repl.py:242-249](), [rlm/environments/docker_repl.py:30]()\n\n---\n\n## Summary\n\nThe RLM system implements recursion through:\n\n1. **`llm_query()` function**: Exposed in REPL globals for code to call [rlm/repl.py:169-174]()\n2. **`Sub_RLM` class**: Minimal LLM client for depth=1 recursion [rlm/repl.py:15-52]()\n3. **Depth limitation**: Current implementation stops at depth=1 by design [README.md:9]()\n4. **Extension path**: Replace `Sub_RLM` with `RLM_REPL` for arbitrary depth [README.md:9-11]()\n\nThe architecture separates root-level orchestration (`RLM_REPL`) from sub-query execution (`Sub_RLM`), enabling flexible model selection and controlled recursion depth.\n\n**Sources**: [rlm/rlm_repl.py:1-136](), [rlm/repl.py:1-360](), [README.md:1-32]()\n\n---\n\n# Page: Communication Protocols and Data Types\n\n# Communication and Data Types\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [rlm/clients/__init__.py](rlm/clients/__init__.py)\n- [rlm/clients/azure_openai.py](rlm/clients/azure_openai.py)\n- [rlm/core/comms_utils.py](rlm/core/comms_utils.py)\n- [rlm/core/lm_handler.py](rlm/core/lm_handler.py)\n- [rlm/core/types.py](rlm/core/types.py)\n- [rlm/environments/modal_repl.py](rlm/environments/modal_repl.py)\n- [rlm/environments/prime_repl.py](rlm/environments/prime_repl.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis document describes the communication protocol and data structures that enable inter-process communication and data exchange within the RLM system. It covers:\n\n- The socket-based protocol used for communication between REPL environments and the LM Handler\n- Request and response message formats (`LMRequest`, `LMResponse`)\n- Core data types for representing execution results, LLM calls, and system metadata\n- Helper functions for socket communication\n\nFor information about how the LM Handler uses this protocol to route requests, see [LM Handler](#3.2). For details on how REPL environments make LLM calls using this protocol, see [REPL Environments](#3.3) and [Recursion and Sub-Calls](#3.4).\n\n---\n\n## Socket-Based Communication Protocol\n\nThe RLM system uses a simple, reliable socket-based protocol for communication between REPL environments and the LM Handler. This enables environments running in separate processes, containers, or cloud sandboxes to make LLM calls back to the handler.\n\n### Protocol Specification\n\nThe protocol uses a **4-byte big-endian length prefix** followed by a **UTF-8 encoded JSON payload**:\n\n```\n[4 bytes: message length][N bytes: JSON payload]\n```\n\nThis design ensures that the receiver knows exactly how many bytes to read for each message, preventing partial reads and message boundary issues.\n\n**Protocol Diagram**\n\n```mermaid\nsequenceDiagram\n    participant Env as \"REPL Environment\"\n    participant Socket as \"TCP Socket\"\n    participant Handler as \"LM Handler\"\n    \n    Env->>Socket: \"Pack 4-byte length (big-endian)\"\n    Env->>Socket: \"Send JSON payload (UTF-8)\"\n    Socket->>Handler: \"Receive 4-byte length\"\n    Handler->>Socket: \"Read N bytes of payload\"\n    Handler->>Handler: \"Parse JSON\"\n    Handler->>Handler: \"Process request\"\n    Handler->>Socket: \"Pack 4-byte length\"\n    Handler->>Socket: \"Send JSON response\"\n    Socket->>Env: \"Receive length + payload\"\n    Env->>Env: \"Parse response\"\n```\n\n**Sources:** [rlm/core/comms_utils.py:1-10](), [rlm/core/comms_utils.py:143-174]()\n\n### Socket Helper Functions\n\nThe protocol is implemented through three core helper functions:\n\n| Function | Purpose | Location |\n|----------|---------|----------|\n| `socket_send(sock, data)` | Send a length-prefixed JSON message | [rlm/core/comms_utils.py:143-149]() |\n| `socket_recv(sock)` | Receive a length-prefixed JSON message | [rlm/core/comms_utils.py:152-173]() |\n| `socket_request(address, data, timeout)` | Open connection, send request, receive response, close | [rlm/core/comms_utils.py:176-193]() |\n\nThe `socket_recv` function handles partial reads by looping until the full payload is received. If the connection closes before the message is complete, it raises a `ConnectionError`.\n\n**Sources:** [rlm/core/comms_utils.py:143-194]()\n\n---\n\n## Message Data Types\n\n### LMRequest\n\nThe `LMRequest` class represents a request from a REPL environment to the LM Handler. It supports both single and batched requests.\n\n**Structure:**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `prompt` | `str \\| dict \\| None` | Single prompt for non-batched requests |\n| `prompts` | `list[str \\| dict] \\| None` | Multiple prompts for batched requests |\n| `model` | `str \\| None` | Optional model override |\n\n**Key Methods:**\n- `is_batched`: Property that returns `True` if `prompts` field is populated\n- `to_dict()`: Serializes to dictionary, excluding `None` values\n- `from_dict(data)`: Deserializes from dictionary\n\n**Sources:** [rlm/core/comms_utils.py:21-56]()\n\n### LMResponse\n\nThe `LMResponse` class represents a response from the LM Handler back to the environment. It supports both single and batched responses, as well as error reporting.\n\n**Structure:**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `error` | `str \\| None` | Error message if request failed |\n| `chat_completion` | `RLMChatCompletion \\| None` | Single completion for non-batched responses |\n| `chat_completions` | `list[RLMChatCompletion] \\| None` | Multiple completions for batched responses |\n\n**Key Methods:**\n- `success`: Property that returns `True` if no error occurred\n- `is_batched`: Property that returns `True` if response is batched\n- `success_response(completion)`: Factory for successful single response\n- `batched_success_response(completions)`: Factory for successful batched response\n- `error_response(error)`: Factory for error response\n\nThe `to_dict()` method ensures mutual exclusivity—only one of `error`, `chat_completion`, or `chat_completions` is non-`None` in the serialized output.\n\n**Sources:** [rlm/core/comms_utils.py:58-136]()\n\n### Typed Request Functions\n\nTwo high-level functions provide typed interfaces for making requests:\n\n**`send_lm_request(address, request, timeout)`**\n- Sends a single `LMRequest` and returns an `LMResponse`\n- Handles connection errors by returning error responses\n- Used by environments for single LLM calls\n\n**`send_lm_request_batched(address, prompts, model, timeout)`**\n- Sends multiple prompts in a single request\n- Returns a list of `LMResponse` objects, one per prompt, in order\n- If the batched request fails, returns error responses for all prompts\n- Used by environments for batched LLM calls\n\n**Sources:** [rlm/core/comms_utils.py:201-254]()\n\n---\n\n## Core Data Types\n\nThe RLM system uses a hierarchical set of data types to represent execution state, LLM interactions, and system configuration.\n\n### Data Type Hierarchy Diagram\n\n```mermaid\ngraph TB\n    RLMIteration[\"RLMIteration<br/>prompt, response, code_blocks\"]\n    CodeBlock[\"CodeBlock<br/>code, result\"]\n    REPLResult[\"REPLResult<br/>stdout, stderr, locals, llm_calls\"]\n    RLMChatCompletion[\"RLMChatCompletion<br/>prompt, response, usage, time\"]\n    UsageSummary[\"UsageSummary<br/>model_usage_summaries\"]\n    ModelUsageSummary[\"ModelUsageSummary<br/>calls, input_tokens, output_tokens\"]\n    \n    RLMIteration -->|\"contains list of\"| CodeBlock\n    CodeBlock -->|\"has\"| REPLResult\n    REPLResult -->|\"tracks\"| RLMChatCompletion\n    RLMChatCompletion -->|\"has\"| UsageSummary\n    UsageSummary -->|\"maps model to\"| ModelUsageSummary\n    \n    style RLMIteration fill:#f9f9f9,stroke:#333,stroke-width:2px\n    style RLMChatCompletion fill:#f9f9f9,stroke:#333,stroke-width:2px\n```\n\n**Sources:** [rlm/core/types.py:1-236]()\n\n### RLMChatCompletion\n\nRepresents a single LLM call, including the prompt, response, token usage, and execution time. This type is used both for top-level RLM iterations and for nested LLM calls made from within code.\n\n**Fields:**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `root_model` | `str` | The model that handled this request |\n| `prompt` | `str \\| dict` | The input prompt (string or messages format) |\n| `response` | `str` | The LLM's response text |\n| `usage_summary` | `UsageSummary` | Token usage statistics |\n| `execution_time` | `float` | Time taken in seconds |\n\n**Methods:**\n- `to_dict()`: Serializes to dictionary for JSON logging\n- `from_dict(data)`: Deserializes from dictionary\n\n**Sources:** [rlm/core/types.py:81-107]()\n\n### REPLResult\n\nCaptures the complete result of executing a code block in a REPL environment, including stdout/stderr output, local variable state, execution time, and any nested LLM calls made during execution.\n\n**Fields:**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `stdout` | `str` | Standard output from code execution |\n| `stderr` | `str` | Standard error from code execution |\n| `locals` | `dict` | Local variable namespace after execution |\n| `execution_time` | `float` | Time taken to execute code |\n| `rlm_calls` | `list[RLMChatCompletion]` | Nested LLM calls made during execution |\n\nThe `locals` dictionary is serialized using `_serialize_value()` to handle non-JSON-serializable Python objects like modules and functions.\n\n**Sources:** [rlm/core/types.py:110-142]()\n\n### CodeBlock\n\nRepresents a single code block extracted from an LLM response, along with its execution result.\n\n**Fields:**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `code` | `str` | The Python code to execute |\n| `result` | `REPLResult` | The execution result |\n\n**Sources:** [rlm/core/types.py:145-151]()\n\n### RLMIteration\n\nRepresents one complete iteration of the RLM loop: the prompt sent to the LLM, the response received, the extracted code blocks, and their execution results.\n\n**Fields:**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `prompt` | `str \\| dict` | The prompt sent to the LLM |\n| `response` | `str` | The LLM's raw response |\n| `code_blocks` | `list[CodeBlock]` | Extracted and executed code blocks |\n| `final_answer` | `str \\| None` | Final answer if one was extracted |\n| `iteration_time` | `float \\| None` | Total time for this iteration |\n\nThis type is serialized to JSONL format by the logger for trajectory visualization.\n\n**Sources:** [rlm/core/types.py:154-169]()\n\n---\n\n## Usage Tracking Data Types\n\n### ModelUsageSummary\n\nTracks token usage for a single model across multiple calls.\n\n**Fields:**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `total_calls` | `int` | Number of calls made to this model |\n| `total_input_tokens` | `int` | Total input tokens consumed |\n| `total_output_tokens` | `int` | Total output tokens generated |\n\n**Sources:** [rlm/core/types.py:33-52]()\n\n### UsageSummary\n\nAggregates usage statistics across all models used in an RLM session.\n\n**Fields:**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `model_usage_summaries` | `dict[str, ModelUsageSummary]` | Maps model names to their usage summaries |\n\nThis enables tracking costs when different models are used for different sub-calls (e.g., using GPT-4 for main reasoning and GPT-3.5 for nested calls).\n\n**Sources:** [rlm/core/types.py:55-74]()\n\n---\n\n## Metadata Data Types\n\n### RLMMetadata\n\nCaptures the configuration used for an RLM session. This is logged at the start of each trajectory for reproducibility.\n\n**Fields:**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `root_model` | `str` | The primary model used |\n| `max_depth` | `int` | Maximum recursion depth |\n| `max_iterations` | `int` | Maximum iterations per completion |\n| `backend` | `str` | Primary backend (e.g., \"openai\", \"portkey\") |\n| `backend_kwargs` | `dict` | Backend-specific configuration |\n| `environment_type` | `str` | Environment type (e.g., \"local\", \"modal\") |\n| `environment_kwargs` | `dict` | Environment-specific configuration |\n| `other_backends` | `list[str] \\| None` | Additional backends available for sub-calls |\n\nNon-serializable values in `backend_kwargs` and `environment_kwargs` are converted to string representations using `_serialize_value()`.\n\n**Sources:** [rlm/core/types.py:177-202]()\n\n### QueryMetadata\n\nProvides context length information about prompts, useful for debugging and optimization.\n\n**Fields:**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `context_lengths` | `list[int]` | Length of each message/chunk in the prompt |\n| `context_total_length` | `int` | Total prompt length |\n| `context_type` | `str` | Type of prompt structure (\"str\", \"dict\", \"list\") |\n\nThe constructor analyzes the prompt structure and computes these statistics automatically.\n\n**Sources:** [rlm/core/types.py:210-235]()\n\n---\n\n## Value Serialization\n\nThe `_serialize_value(value)` helper function converts Python values to JSON-serializable representations. This is critical for logging local variable state and configuration parameters.\n\n**Serialization Rules:**\n\n| Python Type | Serialization |\n|-------------|---------------|\n| Primitives (`None`, `bool`, `int`, `float`, `str`) | Unchanged |\n| `list`, `tuple` | Recursively serialize elements |\n| `dict` | Recursively serialize values, stringify keys |\n| `ModuleType` | `\"<module 'module_name'>\"` |\n| Callable | `\"<function_name>\"` or `\"<type_name>\"` |\n| Other | `repr(value)` or `\"<type_name>\"` |\n\nThis ensures that the `REPLResult.locals` dictionary can be safely serialized even if it contains complex Python objects like imported modules, functions, or class instances.\n\n**Sources:** [rlm/core/types.py:9-25]()\n\n---\n\n## Communication Flow Diagram\n\nThe following diagram shows how these data types flow through the system during a typical RLM execution:\n\n```mermaid\nsequenceDiagram\n    participant RLM as \"RLM Core\"\n    participant Env as \"REPL Environment\"\n    participant Handler as \"LM Handler\"\n    participant Client as \"LLM Client\"\n    \n    Note over RLM: \"Iteration Loop\"\n    \n    RLM->>Handler: \"LMRequest(prompt, model)\"\n    Handler->>Client: \"Forward request\"\n    Client-->>Handler: \"LLM response\"\n    Handler-->>RLM: \"LMResponse(chat_completion)\"\n    \n    Note over RLM: \"Extract code blocks\"\n    \n    RLM->>Env: \"execute_code(code)\"\n    \n    opt Code calls llm_query\n        Env->>Env: \"Build LMRequest\"\n        Env->>Handler: \"socket_request(LMRequest)\"\n        Handler->>Client: \"Nested LLM call\"\n        Client-->>Handler: \"Nested response\"\n        Handler-->>Env: \"LMResponse(chat_completion)\"\n    end\n    \n    Env-->>RLM: \"REPLResult(stdout, stderr, locals, rlm_calls)\"\n    \n    RLM->>RLM: \"Create CodeBlock(code, result)\"\n    RLM->>RLM: \"Create RLMIteration(prompt, response, code_blocks)\"\n    RLM->>Logger: \"log(RLMIteration)\"\n    \n    Note over Logger: \"Serialize to .jsonl\"\n```\n\n**Sources:** [rlm/core/comms_utils.py:1-254](), [rlm/core/types.py:1-236]()\n\n---\n\n## Type Constants\n\nThe `types.py` module defines two type aliases for configuration:\n\n```python\nClientBackend = Literal[\"openai\", \"portkey\", \"openrouter\", \"vllm\", \"litellm\", \"anthropic\"]\nEnvironmentType = Literal[\"local\", \"prime\", \"modal\"]\n```\n\nThese provide type hints and validation for backend and environment selection throughout the codebase.\n\n**Sources:** [rlm/core/types.py:5-6]()\n\n---\n\n# Page: LLM Client Backends\n\n# LLM Client Backends\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [rlm/clients/__init__.py](rlm/clients/__init__.py)\n- [rlm/clients/azure_openai.py](rlm/clients/azure_openai.py)\n- [rlm/clients/openai.py](rlm/clients/openai.py)\n- [rlm/clients/portkey.py](rlm/clients/portkey.py)\n- [rlm/core/types.py](rlm/core/types.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis document describes the LLM client backend system in RLM, which provides a unified interface for making completion requests to various LLM providers. The client layer abstracts differences between provider APIs (OpenAI, Anthropic, Portkey, etc.) behind the `BaseLM` interface, enabling the RLM system to work with any supported backend through a consistent API.\n\nFor information about how clients are selected and routed during recursive calls, see [Multi-Backend Configuration](#8.1). For details on how clients interact with the execution environment layer, see [REPL Environments](#5).\n\n---\n\n## Overview\n\nThe LLM client backends form the **LM Backend Layer** in RLM's architecture. This layer sits between the Core Orchestration Layer (which includes the `RLM` class and `LMHandler`) and the external LLM provider APIs. All clients implement the `BaseLM` abstract interface, which defines two core methods: `completion()` and `acompletion()` for synchronous and asynchronous completions respectively.\n\nThe `get_client()` factory function [rlm/clients/__init__.py:11-63]() provides runtime selection of backends based on the `ClientBackend` literal type, which enumerates all supported providers.\n\n```mermaid\ngraph TB\n    subgraph \"Core Orchestration\"\n        RLM[\"RLM<br/>(rlm.core.rlm.RLM)\"]\n        LMHandler[\"LMHandler<br/>(TCP Server)\"]\n    end\n    \n    subgraph \"LM Backend Layer\"\n        GetClient[\"get_client()<br/>(Factory)\"]\n        BaseLM[\"BaseLM<br/>(Abstract Interface)\"]\n        \n        OpenAIClient[\"OpenAIClient\"]\n        PortkeyClient[\"PortkeyClient\"]\n        AnthropicClient[\"AnthropicClient\"]\n        AzureOpenAIClient[\"AzureOpenAIClient\"]\n        GeminiClient[\"GeminiClient\"]\n        LiteLLMClient[\"LiteLLMClient\"]\n        \n        BaseLM --> OpenAIClient\n        BaseLM --> PortkeyClient\n        BaseLM --> AnthropicClient\n        BaseLM --> AzureOpenAIClient\n        BaseLM --> GeminiClient\n        BaseLM --> LiteLLMClient\n    end\n    \n    subgraph \"External APIs\"\n        OpenAIAPI[\"OpenAI API<br/>(api.openai.com)\"]\n        OpenRouter[\"OpenRouter<br/>(openrouter.ai)\"]\n        VercelGateway[\"Vercel AI Gateway<br/>(ai-gateway.vercel.sh)\"]\n        PrimeAPI[\"Prime Intellect<br/>(api.pinference.ai)\"]\n        PortkeyAPI[\"Portkey Gateway<br/>(api.portkey.ai)\"]\n        AnthropicAPI[\"Anthropic API\"]\n        AzureAPI[\"Azure OpenAI\"]\n        GeminiAPI[\"Google Gemini\"]\n        Other[\"Other Providers<br/>(via LiteLLM)\"]\n    end\n    \n    RLM --> GetClient\n    LMHandler --> GetClient\n    GetClient --> BaseLM\n    \n    OpenAIClient --> OpenAIAPI\n    OpenAIClient --> OpenRouter\n    OpenAIClient --> VercelGateway\n    OpenAIClient --> PrimeAPI\n    PortkeyClient --> PortkeyAPI\n    AnthropicClient --> AnthropicAPI\n    AzureOpenAIClient --> AzureAPI\n    GeminiClient --> GeminiAPI\n    LiteLLMClient --> Other\n```\n\n**Sources:** [rlm/clients/__init__.py:11-63](), [rlm/core/types.py:5-15]()\n\n---\n\n## Supported Backends\n\nThe `ClientBackend` literal type [rlm/core/types.py:5-15]() enumerates all supported backend identifiers:\n\n| Backend ID | Client Class | Primary Use Case | API Compatibility |\n|------------|--------------|------------------|-------------------|\n| `openai` | `OpenAIClient` | OpenAI models | OpenAI API |\n| `openrouter` | `OpenAIClient` | OpenRouter gateway | OpenAI-compatible |\n| `vercel` | `OpenAIClient` | Vercel AI Gateway | OpenAI-compatible |\n| `vllm` | `OpenAIClient` | Local vLLM server | OpenAI-compatible |\n| `portkey` | `PortkeyClient` | Portkey gateway | Portkey API |\n| `anthropic` | `AnthropicClient` | Claude models | Anthropic API |\n| `azure_openai` | `AzureOpenAIClient` | Azure-hosted OpenAI | Azure OpenAI API |\n| `gemini` | `GeminiClient` | Google Gemini models | Google Generative AI API |\n| `litellm` | `LiteLLMClient` | Universal proxy | 100+ providers via LiteLLM |\n\nThe `OpenAIClient` is the most versatile, supporting any OpenAI-compatible API through the `base_url` parameter. This includes Prime Intellect's inference API [rlm/clients/openai.py:17](), which requires special `extra_body` configuration for usage tracking [rlm/clients/openai.py:66-67]().\n\n**Sources:** [rlm/core/types.py:5-15](), [rlm/clients/__init__.py:11-63](), [rlm/clients/openai.py:20-51]()\n\n---\n\n## Client Factory Pattern\n\nThe `get_client()` factory function [rlm/clients/__init__.py:11-63]() routes backend identifiers to concrete implementations:\n\n```mermaid\ngraph LR\n    Backend[\"ClientBackend<br/>(Literal Type)\"]\n    GetClient[\"get_client()<br/>(backend, backend_kwargs)\"]\n    \n    OpenAI[\"backend == 'openai'<br/>→ OpenAIClient\"]\n    VLLM[\"backend == 'vllm'<br/>→ OpenAIClient<br/>(+ base_url validation)\"]\n    OpenRouter[\"backend == 'openrouter'<br/>→ OpenAIClient<br/>(base_url preset)\"]\n    Vercel[\"backend == 'vercel'<br/>→ OpenAIClient<br/>(base_url preset)\"]\n    Portkey[\"backend == 'portkey'<br/>→ PortkeyClient\"]\n    Anthropic[\"backend == 'anthropic'<br/>→ AnthropicClient\"]\n    Azure[\"backend == 'azure_openai'<br/>→ AzureOpenAIClient\"]\n    Gemini[\"backend == 'gemini'<br/>→ GeminiClient\"]\n    LiteLLM[\"backend == 'litellm'<br/>→ LiteLLMClient\"]\n    \n    Backend --> GetClient\n    GetClient --> OpenAI\n    GetClient --> VLLM\n    GetClient --> OpenRouter\n    GetClient --> Vercel\n    GetClient --> Portkey\n    GetClient --> Anthropic\n    GetClient --> Azure\n    GetClient --> Gemini\n    GetClient --> LiteLLM\n```\n\nThe factory performs backend-specific logic:\n- **OpenRouter/Vercel**: Pre-sets `base_url` to provider endpoints [rlm/clients/__init__.py:37-42]()\n- **vLLM**: Validates that `base_url` is provided for local server [rlm/clients/__init__.py:26-28]()\n- **All others**: Directly instantiates the appropriate client class\n\n**Sources:** [rlm/clients/__init__.py:11-63]()\n\n---\n\n## BaseLM Interface\n\nAll clients implement the `BaseLM` abstract base class, which defines the contract for completion operations and usage tracking. The interface includes:\n\n```mermaid\nclassDiagram\n    class BaseLM {\n        <<abstract>>\n        +model_name: str\n        +last_prompt_tokens: int\n        +last_completion_tokens: int\n        +completion(prompt, model) str\n        +acompletion(prompt, model) str*\n        +get_usage_summary() UsageSummary\n        +get_last_usage() ModelUsageSummary\n    }\n    \n    class OpenAIClient {\n        +client: openai.OpenAI\n        +async_client: openai.AsyncOpenAI\n        +model_call_counts: dict\n        +model_input_tokens: dict\n        +model_output_tokens: dict\n        +_track_cost(response, model)\n    }\n    \n    class PortkeyClient {\n        +client: Portkey\n        +async_client: AsyncPortkey\n        +model_call_counts: dict\n        +model_input_tokens: dict\n        +model_output_tokens: dict\n        +_track_cost(response, model)\n    }\n    \n    class AzureOpenAIClient {\n        +client: openai.AzureOpenAI\n        +async_client: openai.AsyncAzureOpenAI\n        +azure_deployment: str\n        +model_call_counts: dict\n        +_track_cost(response, model)\n    }\n    \n    BaseLM <|-- OpenAIClient\n    BaseLM <|-- PortkeyClient\n    BaseLM <|-- AzureOpenAIClient\n```\n\n### Core Methods\n\n**`completion(prompt, model=None)`** [rlm/clients/openai.py:53-73]()\n- Accepts `str` or `list[dict[str, Any]]` for prompts (single message or full conversation)\n- Returns the completion as a string\n- Converts string prompts to message format: `[{\"role\": \"user\", \"content\": prompt}]`\n- Tracks usage via `_track_cost()` after each call\n\n**`acompletion(prompt, model=None)`** [rlm/clients/openai.py:75-97]()\n- Async version of `completion()`, same signature and behavior\n- Uses async client instances (e.g., `openai.AsyncOpenAI`)\n\n**`get_usage_summary()`** [rlm/clients/openai.py:114-122]()\n- Returns `UsageSummary` with per-model token counts aggregated across all calls\n- Used by `LMHandler` to track total usage across the entire RLM execution\n\n**`get_last_usage()`** [rlm/clients/openai.py:124-129]()\n- Returns `ModelUsageSummary` for the most recent completion\n- Used to attribute token costs to specific `RLMChatCompletion` objects\n\n**Sources:** [rlm/clients/openai.py:53-129](), [rlm/clients/portkey.py:34-94](), [rlm/clients/azure_openai.py:72-142]()\n\n---\n\n## Usage Tracking System\n\nAll clients maintain per-model usage statistics using `defaultdict` counters [rlm/clients/openai.py:48-51](). After each completion, the `_track_cost()` method updates:\n\n```mermaid\ngraph TB\n    Completion[\"completion() or<br/>acompletion()\"]\n    APICall[\"API Request<br/>(e.g., client.chat.completions.create)\"]\n    Response[\"Response<br/>(contains usage data)\"]\n    TrackCost[\"_track_cost(response, model)\"]\n    \n    UpdateCounters[\"Update Per-Model Counters:<br/>- model_call_counts[model]<br/>- model_input_tokens[model]<br/>- model_output_tokens[model]<br/>- model_total_tokens[model]\"]\n    \n    UpdateLast[\"Update Last Call:<br/>- last_prompt_tokens<br/>- last_completion_tokens\"]\n    \n    Completion --> APICall\n    APICall --> Response\n    Response --> TrackCost\n    TrackCost --> UpdateCounters\n    TrackCost --> UpdateLast\n```\n\n### Token Tracking Data Flow\n\n1. **During Completion**: `_track_cost()` extracts `usage` from the API response [rlm/clients/openai.py:102-108]()\n2. **Per-Model Aggregation**: Counters accumulate across all calls to each model [rlm/clients/openai.py:100-108]()\n3. **Last Call Tracking**: `last_prompt_tokens` and `last_completion_tokens` store the most recent call for handler access [rlm/clients/openai.py:111-112]()\n4. **Summary Generation**: `get_usage_summary()` converts counters to `UsageSummary` objects [rlm/clients/openai.py:114-122]()\n\nThis system enables:\n- **Cost Attribution**: Each `RLMChatCompletion` records its token usage via `get_last_usage()` [rlm/core/types.py:91-117]()\n- **Total Cost Tracking**: The `RLM` orchestrator aggregates usage across all depths and iterations\n- **Per-Model Breakdown**: Usage reports show which models consumed how many tokens\n\n**Sources:** [rlm/clients/openai.py:48-51, 99-129](), [rlm/core/types.py:43-85]()\n\n---\n\n## Environment Variable Configuration\n\nClients load API keys from environment variables with provider-specific defaults:\n\n| Client | Environment Variable | Default Constant | File Location |\n|--------|---------------------|------------------|---------------|\n| OpenAI | `OPENAI_API_KEY` | `DEFAULT_OPENAI_API_KEY` | [rlm/clients/openai.py:14]() |\n| OpenRouter | `OPENROUTER_API_KEY` | `DEFAULT_OPENROUTER_API_KEY` | [rlm/clients/openai.py:15]() |\n| Vercel | `AI_GATEWAY_API_KEY` | `DEFAULT_VERCEL_API_KEY` | [rlm/clients/openai.py:16]() |\n| Azure OpenAI | `AZURE_OPENAI_API_KEY` | `DEFAULT_AZURE_OPENAI_API_KEY` | [rlm/clients/azure_openai.py:14]() |\n| Azure OpenAI | `AZURE_OPENAI_ENDPOINT` | - | [rlm/clients/azure_openai.py:37]() |\n| Azure OpenAI | `AZURE_OPENAI_API_VERSION` | `\"2024-02-01\"` | [rlm/clients/azure_openai.py:40]() |\n| Azure OpenAI | `AZURE_OPENAI_DEPLOYMENT` | - | [rlm/clients/azure_openai.py:43]() |\n\nThe `OpenAIClient` constructor performs conditional API key selection based on `base_url` [rlm/clients/openai.py:34-40]():\n- If `base_url` is OpenAI's endpoint → use `OPENAI_API_KEY`\n- If `base_url` is OpenRouter's endpoint → use `OPENROUTER_API_KEY`\n- If `base_url` is Vercel's endpoint → use `AI_GATEWAY_API_KEY`\n\nAll clients call `load_dotenv()` at module import to load `.env` files [rlm/clients/openai.py:11]().\n\n**Sources:** [rlm/clients/openai.py:11-17, 34-40](), [rlm/clients/azure_openai.py:11-43](), [rlm/clients/__init__.py:8]()\n\n---\n\n## Prompt Format Handling\n\nAll clients normalize prompts to the OpenAI message format. The `completion()` method accepts two formats:\n\n1. **String Prompt**: Converted to `[{\"role\": \"user\", \"content\": prompt}]` [rlm/clients/openai.py:54-55]()\n2. **Message List**: Passed directly to the API [rlm/clients/openai.py:56-59]()\n\n```mermaid\ngraph LR\n    StrPrompt[\"str:<br/>'What is 2+2?'\"]\n    ListPrompt[\"list[dict]:<br/>[{'role':'user','content':'...'}]\"]\n    \n    Convert[\"Convert to:<br/>[{'role':'user','content':prompt}]\"]\n    Validate[\"Validate:<br/>all items are dict\"]\n    \n    Messages[\"messages:<br/>list[dict]\"]\n    APICall[\"API Call:<br/>chat.completions.create(messages=...)\"]\n    \n    StrPrompt --> Convert\n    ListPrompt --> Validate\n    Convert --> Messages\n    Validate --> Messages\n    Messages --> APICall\n```\n\nInvalid prompt types raise `ValueError` [rlm/clients/openai.py:59](). This normalization ensures consistent behavior across all backends, even though some (like Anthropic) use different native formats.\n\n**Sources:** [rlm/clients/openai.py:53-59, 78-83](), [rlm/clients/portkey.py:34-40](), [rlm/clients/azure_openai.py:72-78]()\n\n---\n\n## Special Handling: Prime Intellect API\n\nThe Prime Intellect API requires additional configuration for usage tracking. When `base_url` is set to Prime's endpoint [rlm/clients/openai.py:17](), the client adds `extra_body` parameters:\n\n```python\nextra_body = {}\nif self.client.base_url == DEFAULT_PRIME_INTELLECT_BASE_URL:\n    extra_body[\"usage\"] = {\"include\": True}\n```\n\nThis ensures that token usage data is included in the response, enabling cost tracking [rlm/clients/openai.py:66-70](). The same logic applies to async completions [rlm/clients/openai.py:89-91]().\n\n**Sources:** [rlm/clients/openai.py:17, 66-70, 89-91]()\n\n---\n\n## Client Instantiation Flow\n\nThe following diagram shows how clients are instantiated and configured through the RLM system:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant RLM as \"RLM.__init__()\"\n    participant GetClient as \"get_client()\"\n    participant Client as \"OpenAIClient()\"\n    participant OpenAI as \"openai.OpenAI\"\n    \n    User->>RLM: backend=\"openai\"<br/>backend_kwargs={...}\n    RLM->>GetClient: backend, backend_kwargs\n    \n    alt backend == \"openai\"\n        GetClient->>Client: **backend_kwargs\n        Client->>Client: Load env vars<br/>(OPENAI_API_KEY, etc.)\n        Client->>Client: Select api_key based on base_url\n        Client->>OpenAI: api_key=..., base_url=...\n        OpenAI-->>Client: client instance\n        Client->>OpenAI: AsyncOpenAI(...)\n        OpenAI-->>Client: async_client instance\n        Client->>Client: Initialize usage counters<br/>(defaultdict)\n        Client-->>GetClient: OpenAIClient instance\n    else backend == \"portkey\"\n        GetClient->>Client: Create PortkeyClient\n    else backend == \"anthropic\"\n        GetClient->>Client: Create AnthropicClient\n    end\n    \n    GetClient-->>RLM: BaseLM instance\n    RLM->>RLM: Store as self.client\n```\n\n**Sources:** [rlm/clients/__init__.py:11-63](), [rlm/clients/openai.py:25-51]()\n\n---\n\n## Error Handling\n\nClients perform validation at multiple points:\n\n1. **Model Name Required**: If no `model` parameter is provided and `model_name` was not set during initialization, raise `ValueError` [rlm/clients/openai.py:62-63]()\n2. **Prompt Type Validation**: Invalid prompt types (not `str` or `list[dict]`) raise `ValueError` [rlm/clients/openai.py:59]()\n3. **Usage Data Missing**: If the API response lacks usage data, raise `ValueError` to prevent silent tracking failures [rlm/clients/openai.py:103-104]()\n4. **Backend-Specific Validation**: The `vllm` backend requires `base_url` to be set [rlm/clients/__init__.py:26-28]()\n5. **Azure Required Fields**: `AzureOpenAIClient` validates that `azure_endpoint` is provided [rlm/clients/azure_openai.py:45-49]()\n\n**Sources:** [rlm/clients/openai.py:59, 62-63, 103-104](), [rlm/clients/__init__.py:26-28](), [rlm/clients/azure_openai.py:45-49]()\n\n---\n\n## Summary\n\nThe LLM client backend system provides:\n\n- **Unified Interface**: `BaseLM` abstracts provider differences behind consistent `completion()` and `acompletion()` methods\n- **Factory Pattern**: `get_client()` enables runtime backend selection with type safety via `ClientBackend` literal\n- **Usage Tracking**: Per-model token counting with aggregation for cost monitoring and attribution\n- **Flexible Configuration**: Environment variable defaults with override support for API keys and endpoints\n- **OpenAI Compatibility**: `OpenAIClient` supports any OpenAI-compatible API through `base_url` parameter\n- **Gateway Support**: Native integration with Portkey, OpenRouter, Vercel AI Gateway, and Prime Intellect\n\nFor details on specific client implementations, see:\n- [Client Architecture](#4.1) for `BaseLM` interface details\n- [OpenAI and Compatible Clients](#4.2) for `OpenAIClient` specifics\n- [Portkey Client](#4.3) for Portkey integration\n- [Other Backends](#4.4) for Anthropic, Azure, Gemini, and LiteLLM\n\n**Sources:** [rlm/clients/__init__.py:11-63](), [rlm/clients/openai.py:20-130](), [rlm/core/types.py:5-15, 43-85]()\n\n---\n\n# Page: Client Architecture\n\n# Client Architecture\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [AGENTS.md](AGENTS.md)\n- [examples/lm_in_prime_repl.py](examples/lm_in_prime_repl.py)\n- [examples/prime_repl_example.py](examples/prime_repl_example.py)\n- [rlm/clients/__init__.py](rlm/clients/__init__.py)\n- [rlm/clients/azure_openai.py](rlm/clients/azure_openai.py)\n- [rlm/clients/openai.py](rlm/clients/openai.py)\n- [rlm/clients/portkey.py](rlm/clients/portkey.py)\n- [rlm/core/types.py](rlm/core/types.py)\n- [rlm/environments/constants.py](rlm/environments/constants.py)\n- [tests/test_imports.py](tests/test_imports.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis document explains the pluggable client architecture for LLM backends in RLM. It covers the `BaseLM` abstract interface, the `get_client` factory function, and how clients are instantiated and configured. For details on specific client implementations (OpenAI, Portkey, Anthropic, etc.), see sections [4.2](#4.2), [4.3](#4.3), and [4.4](#4.4). For information on how these clients integrate with the LM Handler communication layer, see [3.2](#3.2).\n\n## Overview\n\nThe RLM system uses a plugin-based architecture to support multiple LLM backends. This design allows users to seamlessly switch between providers (OpenAI, Anthropic, Portkey, etc.) or use local inference servers (vLLM, OpenRouter) without changing their code. The architecture consists of two main components:\n\n1. **BaseLM Interface**: An abstract base class defining the contract all clients must implement\n2. **get_client Factory**: A routing function that instantiates the appropriate client based on configuration\n\nThis separation enables extensibility—adding a new LLM provider only requires implementing the `BaseLM` interface and adding a routing case to the factory.\n\n**Sources:** [rlm/clients/base_lm.py:1-34](), [rlm/clients/__init__.py:1-51]()\n\n## BaseLM Abstract Interface\n\n### Interface Definition\n\nThe `BaseLM` class defines the standard interface that all LLM clients must implement. It provides four core methods:\n\n| Method | Return Type | Purpose |\n|--------|-------------|---------|\n| `completion(prompt)` | `str` | Synchronous LLM completion |\n| `acompletion(prompt)` | `str` | Asynchronous LLM completion |\n| `get_usage_summary()` | `UsageSummary` | Cumulative usage statistics across all calls |\n| `get_last_usage()` | `UsageSummary` | Usage statistics from most recent call |\n\nThe base class constructor takes a `model_name` parameter and arbitrary keyword arguments, which are stored for use by concrete implementations:\n\n```python\ndef __init__(self, model_name: str, **kwargs):\n    self.model_name = model_name\n    self.kwargs = kwargs\n```\n\n**Diagram: BaseLM Interface Contract**\n\n```mermaid\nclassDiagram\n    class BaseLM {\n        <<abstract>>\n        +str model_name\n        +dict kwargs\n        +__init__(model_name, **kwargs)\n        +completion(prompt)* str\n        +acompletion(prompt)* str\n        +get_usage_summary()* UsageSummary\n        +get_last_usage()* UsageSummary\n    }\n    \n    class OpenAIClient {\n        +completion(prompt) str\n        +acompletion(prompt) str\n        +get_usage_summary() UsageSummary\n        +get_last_usage() UsageSummary\n    }\n    \n    class PortkeyClient {\n        +completion(prompt) str\n        +acompletion(prompt) str\n        +get_usage_summary() UsageSummary\n        +get_last_usage() UsageSummary\n    }\n    \n    class AnthropicClient {\n        +completion(prompt) str\n        +acompletion(prompt) str\n        +get_usage_summary() UsageSummary\n        +get_last_usage() UsageSummary\n    }\n    \n    class LiteLLMClient {\n        +completion(prompt) str\n        +acompletion(prompt) str\n        +get_usage_summary() UsageSummary\n        +get_last_usage() UsageSummary\n    }\n    \n    BaseLM <|-- OpenAIClient\n    BaseLM <|-- PortkeyClient\n    BaseLM <|-- AnthropicClient\n    BaseLM <|-- LiteLLMClient\n```\n\n**Sources:** [rlm/clients/base_lm.py:7-34]()\n\n### Prompt Format Flexibility\n\nThe `completion` and `acompletion` methods accept `prompt` as either a `str` or `dict[str, Any]`. This flexibility allows clients to:\n\n- Accept simple string prompts for basic use cases\n- Support structured message formats (e.g., OpenAI's chat messages format)\n- Pass provider-specific parameters as dictionary keys\n\nConcrete implementations are responsible for handling both formats appropriately for their backend.\n\n**Sources:** [rlm/clients/base_lm.py:18-23]()\n\n### Usage Tracking\n\nThe usage tracking methods enable cost monitoring and debugging:\n\n- `get_usage_summary()` returns cumulative token counts and costs across all calls\n- `get_last_usage()` returns statistics from the most recent completion\n\nThe `UsageSummary` type (defined in `rlm.core.types`) tracks per-model usage, allowing the system to monitor costs when multiple models are used within a single RLM execution.\n\n**Sources:** [rlm/clients/base_lm.py:26-33]()\n\n## Client Factory Pattern\n\n### get_client Factory Function\n\nThe `get_client` function implements a factory pattern that routes backend identifiers to concrete client implementations. It takes two parameters:\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| `backend` | `ClientBackend` | String identifier for the LLM backend |\n| `backend_kwargs` | `dict[str, Any]` | Configuration parameters passed to the client constructor |\n\n**Diagram: get_client Factory Routing**\n\n```mermaid\nflowchart TD\n    GetClient[\"get_client(backend, backend_kwargs)\"]\n    \n    GetClient -->|\"backend == 'openai'\"| OpenAI[\"OpenAIClient(**backend_kwargs)\"]\n    GetClient -->|\"backend == 'vllm'\"| VLLM[\"OpenAIClient(**backend_kwargs)<br/>+ validate base_url\"]\n    GetClient -->|\"backend == 'portkey'\"| Portkey[\"PortkeyClient(**backend_kwargs)\"]\n    GetClient -->|\"backend == 'openrouter'\"| OpenRouter[\"OpenAIClient(**backend_kwargs)<br/>+ set base_url\"]\n    GetClient -->|\"backend == 'litellm'\"| LiteLLM[\"LiteLLMClient(**backend_kwargs)\"]\n    GetClient -->|\"backend == 'anthropic'\"| Anthropic[\"AnthropicClient(**backend_kwargs)\"]\n    GetClient -->|\"else\"| Error[\"ValueError: Unknown backend\"]\n    \n    OpenAI --> Return[\"Return BaseLM instance\"]\n    VLLM --> Return\n    Portkey --> Return\n    OpenRouter --> Return\n    LiteLLM --> Return\n    Anthropic --> Return\n```\n\n**Sources:** [rlm/clients/__init__.py:11-50]()\n\n### Supported Backends\n\nThe factory currently supports six backend types:\n\n1. **`openai`**: Standard OpenAI API client\n2. **`vllm`**: Local vLLM inference server (requires `base_url` in kwargs)\n3. **`portkey`**: Portkey.ai gateway for observability and routing\n4. **`openrouter`**: OpenRouter API (automatically sets `base_url`)\n5. **`litellm`**: LiteLLM unified API client\n6. **`anthropic`**: Anthropic Claude API\n\nNote that `vllm` and `openrouter` both instantiate `OpenAIClient` with different configurations, demonstrating the adapter pattern where a single client class can serve multiple compatible backends.\n\n**Sources:** [rlm/clients/__init__.py:19-46]()\n\n### Lazy Imports\n\nThe factory uses lazy imports to avoid loading all client dependencies at startup:\n\n```python\nif backend == \"openai\":\n    from rlm.clients.openai import OpenAIClient\n    return OpenAIClient(**backend_kwargs)\n```\n\nThis pattern allows the system to function even if optional dependencies (e.g., Portkey SDK) are not installed, as long as users don't attempt to use those backends.\n\n**Sources:** [rlm/clients/__init__.py:19-22]()\n\n### Backend-Specific Configuration\n\nSome backends perform configuration adjustments before instantiation:\n\n- **vLLM**: Validates that `base_url` is present in `backend_kwargs` (required to connect to local server)\n- **OpenRouter**: Automatically sets `base_url` to `https://openrouter.ai/api/v1` if not provided\n\nThis preprocessing in the factory ensures clients receive correct configuration without requiring users to specify boilerplate parameters.\n\n**Sources:** [rlm/clients/__init__.py:23-38]()\n\n## Configuration and Instantiation\n\n### Client Instantiation Flow\n\n**Diagram: Client Configuration and Instantiation Flow**\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant RLM as \"RLM.__init__\"\n    participant Factory as \"get_client\"\n    participant Client as \"Concrete Client\"\n    participant DotEnv as \"load_dotenv()\"\n    \n    User->>RLM: \"RLM(backend='openai',<br/>backend_kwargs={...})\"\n    \n    RLM->>Factory: \"get_client(backend, backend_kwargs)\"\n    \n    Note over Factory: Load environment variables\n    Factory->>DotEnv: \"load_dotenv()\"\n    DotEnv-->>Factory: \"API keys loaded\"\n    \n    Note over Factory: Route to implementation\n    Factory->>Factory: \"if backend == 'openai':<br/>    from rlm.clients.openai...\"\n    \n    Factory->>Client: \"OpenAIClient(**backend_kwargs)\"\n    \n    Note over Client: Initialize with model_name,<br/>api_key from env or kwargs\n    Client->>Client: \"super().__init__(model_name, **kwargs)\"\n    \n    Client-->>Factory: \"client instance\"\n    Factory-->>RLM: \"BaseLM instance\"\n    RLM-->>User: \"RLM instance\"\n```\n\n**Sources:** [rlm/clients/__init__.py:1-51]()\n\n### Environment Variable Loading\n\nThe `get_client` module automatically loads environment variables using `dotenv`:\n\n```python\nfrom dotenv import load_dotenv\nload_dotenv()\n```\n\nThis occurs at import time, ensuring API keys and other secrets are available when clients are instantiated. Clients typically look for environment variables like:\n\n- `OPENAI_API_KEY` for OpenAI client\n- `PORTKEY_API_KEY` for Portkey client\n- `OPENROUTER_API_KEY` for OpenRouter\n- `ANTHROPIC_API_KEY` for Anthropic client\n\nUsers can override these by explicitly passing `api_key` in `backend_kwargs`.\n\n**Sources:** [rlm/clients/__init__.py:3-8](), [examples/lm_in_repl.py:36-39]()\n\n### Backend Configuration Examples\n\n**OpenAI:**\n```python\nbackend_kwargs = {\n    \"model_name\": \"gpt-4\",\n    \"api_key\": \"sk-...\",  # Optional, defaults to OPENAI_API_KEY env var\n}\n```\n\n**vLLM (local server):**\n```python\nbackend_kwargs = {\n    \"model_name\": \"meta-llama/Llama-2-7b\",\n    \"base_url\": \"http://localhost:8000/v1\",  # Required\n}\n```\n\n**OpenRouter:**\n```python\nbackend_kwargs = {\n    \"model_name\": \"anthropic/claude-3-opus\",\n    # base_url automatically set to https://openrouter.ai/api/v1\n}\n```\n\n**Portkey:**\n```python\nbackend_kwargs = {\n    \"model_name\": \"@openai/gpt-4\",\n    \"api_key\": \"pk-...\",  # Portkey API key\n}\n```\n\n**Sources:** [rlm/clients/__init__.py:19-46]()\n\n## Integration with RLM System\n\n### Client Usage in RLM Core\n\nThe RLM core instantiates clients through the factory and passes them to the `LMHandler`:\n\n```python\n# In RLM.__init__\nclient = get_client(backend=self.backend, backend_kwargs=self.backend_kwargs)\nself.lm_handler = LMHandler(client=client)\n```\n\nThis separation allows the handler to remain backend-agnostic, operating solely through the `BaseLM` interface.\n\n**Sources:** Referenced in context, see [3.1](#3.1) and [3.2](#3.2) for full details\n\n### Client Usage in Recursive Calls\n\nWhen REPL environments make recursive LLM calls via `llm_query()`, they send requests to the `LMHandler`, which forwards them to the client:\n\n```python\n# Inside REPL environment\nresponse = llm_query(\"What is 2 + 2?\")\n```\n\nThe handler receives this request through socket communication and calls:\n\n```python\nresponse = self.client.completion(prompt)\n```\n\nThis enables nested LLM calls to use the same backend configured at the RLM level, maintaining consistency throughout the execution tree.\n\n**Sources:** [examples/lm_in_repl.py:27-32]()\n\n### Extending with Custom Clients\n\nTo add a new LLM backend:\n\n1. **Implement `BaseLM`**: Create a new client class that implements all abstract methods\n2. **Update factory**: Add a routing case in `get_client` for the new backend identifier\n3. **Update type definitions**: Add the backend string to the `ClientBackend` type enum\n\nExample skeleton:\n\n```python\n# rlm/clients/custom.py\nfrom rlm.clients.base_lm import BaseLM\nfrom rlm.core.types import UsageSummary\n\nclass CustomClient(BaseLM):\n    def completion(self, prompt: str | dict) -> str:\n        # Implementation\n        pass\n    \n    async def acompletion(self, prompt: str | dict) -> str:\n        # Implementation\n        pass\n    \n    def get_usage_summary(self) -> UsageSummary:\n        # Implementation\n        pass\n    \n    def get_last_usage(self) -> UsageSummary:\n        # Implementation\n        pass\n```\n\nThen update the factory in [rlm/clients/__init__.py:11-50]():\n\n```python\nelif backend == \"custom\":\n    from rlm.clients.custom import CustomClient\n    return CustomClient(**backend_kwargs)\n```\n\n**Sources:** [rlm/clients/base_lm.py:7-34](), [rlm/clients/__init__.py:11-50]()\n\n## Summary\n\nThe client architecture provides a clean abstraction layer between the RLM system and LLM providers. Key design patterns include:\n\n- **Abstract interface (`BaseLM`)**: Ensures all clients provide consistent completion and usage tracking methods\n- **Factory pattern (`get_client`)**: Centralizes routing logic and enables lazy loading\n- **Configuration flexibility**: Supports environment variables and explicit kwargs\n- **Adapter reuse**: Single client classes can serve multiple compatible backends (e.g., `OpenAIClient` for OpenAI, vLLM, and OpenRouter)\n\nThis architecture enables RLM to support a diverse ecosystem of LLM providers while maintaining a simple, uniform interface for the core system.\n\n**Sources:** [rlm/clients/base_lm.py:1-34](), [rlm/clients/__init__.py:1-51]()\n\n---\n\n# Page: OpenAI and Compatible Clients\n\n# OpenAI Client\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [rlm/clients/__init__.py](rlm/clients/__init__.py)\n- [rlm/clients/azure_openai.py](rlm/clients/azure_openai.py)\n- [rlm/clients/openai.py](rlm/clients/openai.py)\n- [rlm/clients/portkey.py](rlm/clients/portkey.py)\n- [rlm/core/types.py](rlm/core/types.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThe `OpenAIClient` is a versatile LLM client implementation that supports multiple OpenAI-compatible backends within the RLM system. It provides a unified interface for chat completion requests across OpenAI's official API, OpenRouter, and local vLLM servers, while implementing comprehensive usage tracking and cost monitoring.\n\nThis document covers the `OpenAIClient` class located in [rlm/clients/openai.py](). For information about the abstract client interface it implements, see [Client Architecture](#4.1). For information about how clients are instantiated and used by the RLM system, see [RLM Class](#3.1) and [LM Handler](#3.2).\n\n**Sources**: [rlm/clients/openai.py:1-108]()\n\n## Class Overview\n\nThe `OpenAIClient` class implements the `BaseLM` interface and serves as a flexible adapter for OpenAI-compatible APIs. It encapsulates the official `openai` Python client library and extends it with per-model usage tracking, automatic API key detection, and support for both synchronous and asynchronous operations.\n\n**Diagram: OpenAIClient Class Structure**\n```mermaid\ngraph TB\n    BaseLM[\"BaseLM<br/>(abstract interface)\"]\n    OpenAIClient[\"OpenAIClient\"]\n    \n    BaseLM --> OpenAIClient\n    \n    subgraph \"Constructor Parameters\"\n        APIKey[\"api_key: str | None\"]\n        ModelName[\"model_name: str | None\"]\n        BaseURL[\"base_url: str | None\"]\n        Kwargs[\"**kwargs\"]\n    end\n    \n    subgraph \"Instance Attributes\"\n        Client[\"client: openai.OpenAI\"]\n        Model[\"model_name: str\"]\n        CallCounts[\"model_call_counts: dict[str, int]\"]\n        InputTokens[\"model_input_tokens: dict[str, int]\"]\n        OutputTokens[\"model_output_tokens: dict[str, int]\"]\n        TotalTokens[\"model_total_tokens: dict[str, int]\"]\n    end\n    \n    subgraph \"Public Methods\"\n        Completion[\"completion(prompt, model)\"]\n        ACompletion[\"acompletion(prompt, model)\"]\n        GetUsage[\"get_usage_summary()\"]\n        GetLast[\"get_last_usage()\"]\n    end\n    \n    APIKey --> OpenAIClient\n    ModelName --> OpenAIClient\n    BaseURL --> OpenAIClient\n    Kwargs --> OpenAIClient\n    \n    OpenAIClient --> Client\n    OpenAIClient --> Model\n    OpenAIClient --> CallCounts\n    OpenAIClient --> InputTokens\n    OpenAIClient --> OutputTokens\n    OpenAIClient --> TotalTokens\n    \n    OpenAIClient --> Completion\n    OpenAIClient --> ACompletion\n    OpenAIClient --> GetUsage\n    OpenAIClient --> GetLast\n```\n\nThe class maintains comprehensive state including an authenticated `openai.OpenAI` client instance, per-model usage statistics, and the selected model name. This design enables cost tracking across multiple models and provides visibility into token consumption patterns.\n\n**Sources**: [rlm/clients/openai.py:18-46](), [rlm/clients/base_lm.py]()\n\n## Initialization\n\nThe `OpenAIClient` constructor accepts multiple parameters to support different backend configurations:\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `api_key` | `str \\| None` | `None` | API key for authentication. Auto-detected based on `base_url` if not provided. |\n| `model_name` | `str \\| None` | `None` | Default model identifier for completions. |\n| `base_url` | `str \\| None` | `None` | API endpoint URL. Defaults to OpenAI's official API. |\n| `**kwargs` | | | Additional parameters passed to `BaseLM` constructor. |\n\n### Intelligent API Key Detection\n\nThe initialization implements automatic API key selection based on the `base_url` parameter:\n\n**Diagram: OpenAIClient Initialization and API Key Selection**\n```mermaid\nsequenceDiagram\n    participant Caller\n    participant Init as \"__init__\"\n    participant Env as \"Environment Variables\"\n    participant OpenAI as \"openai.OpenAI\"\n    \n    Caller->>Init: OpenAIClient(api_key, model_name, base_url)\n    \n    alt api_key provided\n        Init->>Init: Use provided api_key\n    else api_key is None\n        alt base_url is OpenAI or None\n            Init->>Env: Read OPENAI_API_KEY\n            Env-->>Init: Return key\n        else base_url is OpenRouter\n            Init->>Env: Read OPENROUTER_API_KEY\n            Env-->>Init: Return key\n        else base_url is custom (vLLM)\n            Init->>Init: Use None (no auth needed)\n        end\n    end\n    \n    Init->>OpenAI: openai.OpenAI(api_key, base_url)\n    OpenAI-->>Init: client instance\n    \n    Init->>Init: Initialize usage tracking dicts\n    Init-->>Caller: OpenAIClient instance\n```\n\nThe key selection logic at [rlm/clients/openai.py:32-36]() checks the `base_url` to determine which environment variable to use:\n- OpenAI API (`https://api.openai.com/v1` or `None`): Uses `OPENAI_API_KEY`\n- OpenRouter (`https://openrouter.ai/api/v1`): Uses `OPENROUTER_API_KEY`\n- Custom URL (e.g., local vLLM): Allows `None` for unauthenticated access\n\n### Usage Tracking Initialization\n\nThe constructor initializes per-model usage tracking dictionaries at [rlm/clients/openai.py:42-46]():\n- `model_call_counts`: Number of completion requests per model\n- `model_input_tokens`: Input tokens consumed per model\n- `model_output_tokens`: Output tokens generated per model\n- `model_total_tokens`: Total tokens per model\n\nThese dictionaries use `defaultdict(int)` to automatically handle new models without explicit initialization.\n\n**Sources**: [rlm/clients/openai.py:23-46](), [rlm/clients/openai.py:11-15]()\n\n## Completion Methods\n\nThe `OpenAIClient` provides both synchronous and asynchronous completion methods that implement the `BaseLM` interface.\n\n### Synchronous Completion\n\n**Method Signature:**\n```python\ndef completion(\n    self,\n    prompt: str | list[dict[str, Any]],\n    model: str | None = None\n) -> str\n```\n\n**Parameters:**\n\n| Parameter | Type | Description |\n|-----------|------|---------|\n| `prompt` | `str \\| list[dict[str, Any]]` | Either a plain string or a list of message dictionaries. |\n| `model` | `str \\| None` | Model identifier. Overrides the instance's `model_name` if provided. |\n\n### Asynchronous Completion\n\n**Method Signature:**\n```python\nasync def acompletion(\n    self,\n    prompt: str | list[dict[str, Any]],\n    model: str | None = None\n) -> str\n```\n\nBoth methods share identical parameter semantics and prompt handling logic.\n\n### Prompt Format Handling\n\nBoth methods implement prompt normalization at [rlm/clients/openai.py:49-54]() (sync) and [rlm/clients/openai.py:67-72]() (async):\n\n**Diagram: Prompt Normalization and Completion Flow**\n```mermaid\ngraph TD\n    Input[\"prompt parameter\"]\n    \n    CheckType{\"prompt type?\"}\n    \n    Input --> CheckType\n    \n    CheckType -->|str| ConvertStr[\"messages = [{'role': 'user', 'content': prompt}]\"]\n    CheckType -->|list[dict]| ValidateList[\"Validate all items are dict\"]\n    \n    ValidateList -->|Valid| UseList[\"messages = prompt\"]\n    ValidateList -->|Invalid| RaiseError[\"Raise ValueError\"]\n    \n    ConvertStr --> CheckModel[\"Check model parameter\"]\n    UseList --> CheckModel\n    \n    CheckModel -->|model provided| UseProvided[\"Use provided model\"]\n    CheckModel -->|model is None| UseDefault[\"Use self.model_name\"]\n    \n    UseProvided --> ValidateModel{\"model exists?\"}\n    UseDefault --> ValidateModel\n    \n    ValidateModel -->|No| RaiseModelError[\"Raise ValueError:<br/>'Model name is required'\"]\n    ValidateModel -->|Yes| APICall[\"client.chat.completions.create()\"]\n    \n    APICall --> TrackCost[\"_track_cost(response, model)\"]\n    TrackCost --> Extract[\"response.choices[0].message.content\"]\n    Extract --> Return[\"Return string\"]\n```\n\nThe normalization logic handles two input formats:\n1. **String**: Automatically wrapped in a user message dictionary\n2. **List of dictionaries**: Validated and used directly\n\nIf the `model` parameter is not provided, the method uses `self.model_name`. If neither is available, a `ValueError` is raised at [rlm/clients/openai.py:58]() and [rlm/clients/openai.py:76]().\n\n### API Call and Response Extraction\n\nThe actual API call occurs at [rlm/clients/openai.py:60]() (sync) and [rlm/clients/openai.py:78]() (async):\n\n```python\n# Synchronous\nresponse = self.client.chat.completions.create(model=model, messages=messages)\n\n# Asynchronous  \nresponse = await self.client.chat.completions.create(model=model, messages=messages)\n```\n\nBoth methods:\n1. Call the OpenAI chat completions endpoint with the formatted messages\n2. Track token usage via `_track_cost()` at [rlm/clients/openai.py:61]() and [rlm/clients/openai.py:79]()\n3. Extract the completion text at [rlm/clients/openai.py:62]() and [rlm/clients/openai.py:80]()\n4. Return the plain string content\n\n**Sources**: [rlm/clients/openai.py:48-80]()\n\n## Usage Tracking\n\nThe `OpenAIClient` implements comprehensive usage tracking to monitor token consumption and API costs across multiple models.\n\n### Cost Tracking Method\n\nThe private `_track_cost()` method at [rlm/clients/openai.py:82-90]() records usage statistics for each completion:\n\n```python\ndef _track_cost(self, response: openai.ChatCompletion, model: str):\n    self.model_call_counts[model] += 1\n    self.model_input_tokens[model] += response.usage.prompt_tokens\n    self.model_output_tokens[model] += response.usage.completion_tokens\n    self.model_total_tokens[model] += response.usage.total_tokens\n    \n    # Track last call for handler to read\n    self.last_prompt_tokens = response.usage.prompt_tokens\n    self.last_completion_tokens = response.usage.completion_tokens\n```\n\nThe method:\n1. Increments the call count for the model\n2. Accumulates input, output, and total tokens per model\n3. Stores the most recent call's token counts in instance attributes for immediate access\n\n### Usage Summary\n\nThe `get_usage_summary()` method at [rlm/clients/openai.py:92-100]() returns a complete `UsageSummary` object containing per-model statistics:\n\n**Diagram: Usage Tracking Data Flow**\n```mermaid\ngraph LR\n    Completion[\"completion() or acompletion()\"]\n    \n    Completion --> APIResponse[\"OpenAI API Response\"]\n    APIResponse --> TrackCost[\"_track_cost(response, model)\"]\n    \n    TrackCost --> UpdateDicts[\"Update per-model dicts:<br/>- model_call_counts<br/>- model_input_tokens<br/>- model_output_tokens<br/>- model_total_tokens\"]\n    \n    TrackCost --> UpdateLast[\"Update last call attrs:<br/>- last_prompt_tokens<br/>- last_completion_tokens\"]\n    \n    UpdateDicts --> GetSummary[\"get_usage_summary()\"]\n    GetSummary --> UsageSummary[\"UsageSummary:<br/>model_usage_summaries dict\"]\n    \n    UpdateLast --> GetLast[\"get_last_usage()\"]\n    GetLast --> ModelUsage[\"ModelUsageSummary:<br/>last call only\"]\n```\n\n**Return Type:**\n```python\nUsageSummary(\n    model_usage_summaries={\n        \"gpt-4\": ModelUsageSummary(\n            total_calls=5,\n            total_input_tokens=2000,\n            total_output_tokens=500\n        ),\n        \"gpt-3.5-turbo\": ModelUsageSummary(\n            total_calls=10,\n            total_input_tokens=1000,\n            total_output_tokens=300\n        )\n    }\n)\n```\n\n### Last Call Usage\n\nThe `get_last_usage()` method at [rlm/clients/openai.py:102-107]() returns a `ModelUsageSummary` for just the most recent completion, useful for per-request monitoring.\n\n**Sources**: [rlm/clients/openai.py:82-107](), [rlm/core/types.py]()\n\n## Backend Configuration\n\nThe `OpenAIClient` supports three distinct backend configurations through the `base_url` parameter.\n\n### Supported Backends\n\n| Backend | `base_url` Value | API Key Source | Use Case |\n|---------|------------------|----------------|----------|\n| OpenAI | `https://api.openai.com/v1` or `None` | `OPENAI_API_KEY` | Official OpenAI models (GPT-4, GPT-3.5, etc.) |\n| OpenRouter | `https://openrouter.ai/api/v1` | `OPENROUTER_API_KEY` | Access to multiple model providers through unified API |\n| vLLM | Custom (e.g., `http://localhost:8000/v1`) | Not required | Local inference with vLLM-served models |\n\n### Configuration Examples\n\n**Diagram: Backend Configuration Patterns**\n```mermaid\ngraph TB\n    User[\"User Configuration\"]\n    \n    subgraph \"OpenAI Backend\"\n        OpenAIConfig[\"backend='openai'<br/>backend_kwargs={'model_name': 'gpt-4'}\"]\n        OpenAIClient1[\"OpenAIClient(api_key=OPENAI_API_KEY,<br/>base_url=None,<br/>model_name='gpt-4')\"]\n        OpenAIAPI[\"OpenAI API<br/>api.openai.com\"]\n        \n        OpenAIConfig --> OpenAIClient1\n        OpenAIClient1 --> OpenAIAPI\n    end\n    \n    subgraph \"OpenRouter Backend\"\n        RouterConfig[\"backend='openai'<br/>backend_kwargs={'base_url': 'https://openrouter.ai/api/v1',<br/>'model_name': 'anthropic/claude-3-opus'}\"]\n        RouterClient[\"OpenAIClient(api_key=OPENROUTER_API_KEY,<br/>base_url='https://openrouter.ai/api/v1',<br/>model_name='anthropic/claude-3-opus')\"]\n        RouterAPI[\"OpenRouter API<br/>openrouter.ai\"]\n        \n        RouterConfig --> RouterClient\n        RouterClient --> RouterAPI\n    end\n    \n    subgraph \"vLLM Backend\"\n        VLLMConfig[\"backend='openai'<br/>backend_kwargs={'base_url': 'http://localhost:8000/v1',<br/>'model_name': 'meta-llama/Llama-2-70b'}\"]\n        VLLMClient[\"OpenAIClient(api_key=None,<br/>base_url='http://localhost:8000/v1',<br/>model_name='meta-llama/Llama-2-70b')\"]\n        VLLMServer[\"Local vLLM Server<br/>localhost:8000\"]\n        \n        VLLMConfig --> VLLMClient\n        VLLMClient --> VLLMServer\n    end\n    \n    User --> OpenAIConfig\n    User --> RouterConfig\n    User --> VLLMConfig\n```\n\n**Usage Examples:**\n\n```python\nfrom rlm import completion\nfrom rlm.client_backend import ClientBackend\n\n# OpenAI (default)\nresult = completion(\n    prompt=\"What is 2+2?\",\n    backend=ClientBackend.OPENAI,\n    backend_kwargs={\"model_name\": \"gpt-4\"}\n)\n\n# OpenRouter\nresult = completion(\n    prompt=\"What is 2+2?\",\n    backend=ClientBackend.OPENAI,\n    backend_kwargs={\n        \"base_url\": \"https://openrouter.ai/api/v1\",\n        \"model_name\": \"anthropic/claude-3-opus\"\n    }\n)\n\n# vLLM local server\nresult = completion(\n    prompt=\"What is 2+2?\",\n    backend=ClientBackend.OPENAI,\n    backend_kwargs={\n        \"base_url\": \"http://localhost:8000/v1\",\n        \"model_name\": \"meta-llama/Llama-2-70b\"\n    }\n)\n```\n\n### Environment Variable Configuration\n\nThe `.env` file should contain API keys for the backends you intend to use:\n\n```\nOPENAI_API_KEY=sk-...\nOPENROUTER_API_KEY=sk-or-...\n```\n\nThe `load_dotenv()` call at [rlm/clients/openai.py:11]() automatically loads these variables at module import time.\n\n**Sources**: [rlm/clients/openai.py:11-39](), [rlm/client_backend.py]()\n\n## Integration with RLM System\n\nThe `OpenAIClient` is instantiated by the `get_client()` factory function and used throughout the RLM execution pipeline.\n\n**Diagram: OpenAIClient Integration in RLM Execution**\n```mermaid\nsequenceDiagram\n    participant User\n    participant RLM as \"RLM Core\"\n    participant GetClient as \"get_client()\"\n    participant OpenAIClient\n    participant Handler as \"LMHandler\"\n    participant API as \"OpenAI-Compatible API\"\n    \n    User->>RLM: completion(prompt, backend='openai')\n    RLM->>GetClient: get_client(ClientBackend.OPENAI, kwargs)\n    GetClient->>OpenAIClient: OpenAIClient(**kwargs)\n    OpenAIClient-->>GetClient: client instance\n    GetClient-->>RLM: client instance\n    \n    RLM->>Handler: start(client)\n    Handler->>Handler: Spawn server thread\n    \n    loop Iteration Loop\n        RLM->>Handler: Request completion\n        Handler->>OpenAIClient: completion(prompt, model)\n        OpenAIClient->>OpenAIClient: Normalize prompt\n        OpenAIClient->>API: chat.completions.create()\n        API-->>OpenAIClient: Response\n        OpenAIClient->>OpenAIClient: _track_cost()\n        OpenAIClient-->>Handler: completion text\n        Handler-->>RLM: completion text\n        \n        RLM->>RLM: Execute code in environment\n        \n        opt Nested LLM Call\n            Note over RLM: Code calls llm_query()\n            RLM->>Handler: Nested completion request\n            Handler->>OpenAIClient: completion()\n            OpenAIClient->>API: chat.completions.create()\n            API-->>OpenAIClient: Response\n            OpenAIClient-->>Handler: completion text\n            Handler-->>RLM: completion text\n        end\n    end\n    \n    RLM->>OpenAIClient: get_usage_summary()\n    OpenAIClient-->>RLM: UsageSummary\n    RLM-->>User: RLMChatCompletion with usage stats\n```\n\n### Client Instantiation\n\nThe RLM system instantiates `OpenAIClient` through the `get_client()` factory at initialization time. The factory:\n1. Routes the `ClientBackend.OPENAI` enum to `OpenAIClient`\n2. Passes `backend_kwargs` to the constructor\n3. Returns the configured client instance\n\nSee [Client Architecture](#4.1) for details on the factory pattern.\n\n### Request Routing\n\nThe `LMHandler` (see [LM Handler](#3.2)) manages all communication with the client:\n1. Receives completion requests from the RLM core or REPL environments\n2. Calls `client.completion()` or `client.acompletion()`\n3. Returns responses to the caller\n4. Queries `get_usage_summary()` at the end of execution\n\n### Multi-Model Support\n\nThe `OpenAIClient` can handle requests for different models in a single session. Each completion call can specify a different `model` parameter, and usage is tracked separately per model. This enables patterns like:\n- Using GPT-4 for complex reasoning tasks\n- Using GPT-3.5-turbo for simple queries to reduce costs\n- Switching between OpenAI and OpenRouter models dynamically\n\n**Sources**: [rlm/clients/openai.py:48-100](), [rlm/clients/get_client.py](), [rlm/lm_handler.py]()\n\n## Error Scenarios\n\nThe `OpenAIClient` handles several error scenarios:\n\n| Scenario | Detection Point | Error Type | Message |\n|----------|----------------|------------|---------|\n| Missing API key | Initialization | `ValueError` | \"OpenAI API key is required...\" |\n| API request failure | Completion call | `RuntimeError` | \"Error generating completion: {details}\" |\n| Invalid message format | Completion call | `RuntimeError` | \"Error generating completion: {details}\" |\n| Network errors | Completion call | `RuntimeError` | \"Error generating completion: {details}\" |\n\nAll errors during completion are caught at [rlm/utils/llm.py:43-44]() and wrapped in `RuntimeError` with contextual information. This design provides consistent error handling to callers without exposing the underlying OpenAI client's exception hierarchy.\n\n**Sources**: [rlm/utils/llm.py:14-16](), [rlm/utils/llm.py:43-44]()\n\n---\n\n# Page: Portkey Client\n\n# Portkey Client\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [rlm/clients/openai.py](rlm/clients/openai.py)\n- [rlm/clients/portkey.py](rlm/clients/portkey.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThe `PortkeyClient` provides an LLM backend implementation for the [Portkey AI Gateway](https://portkey.ai), enabling RLM to route requests through Portkey's unified API for model orchestration, caching, and observability. This client implements the `BaseLM` interface to integrate with the RLM system's multi-backend architecture.\n\nFor information about the client abstraction layer and factory system, see [Client Architecture](#4.1). For other LLM backend implementations, see [OpenAI and Compatible Clients](#4.2) and [Other Backends](#4.4).\n\nSources: [rlm/clients/portkey.py:1-95]()\n\n---\n\n## Client System Integration\n\nThe `PortkeyClient` class integrates into the RLM system through the `BaseLM` abstract base class, providing standardized interfaces for synchronous and asynchronous completions, as well as usage tracking.\n\n```mermaid\ngraph TB\n    subgraph \"Factory and Registration\"\n        GetClient[\"get_client()\"]\n        ClientBackend[\"ClientBackend literal<br/>'portkey'\"]\n    end\n    \n    subgraph \"Base Abstraction\"\n        BaseLM[\"BaseLM<br/>abstract base class\"]\n        Methods[\"Methods:<br/>- completion()<br/>- acompletion()<br/>- get_usage_summary()<br/>- get_last_usage()\"]\n        \n        BaseLM --> Methods\n    end\n    \n    subgraph \"PortkeyClient Implementation\"\n        PortkeyClient[\"PortkeyClient\"]\n        SyncClient[\"self.client<br/>Portkey instance\"]\n        AsyncClient[\"self.async_client<br/>AsyncPortkey instance\"]\n        \n        PortkeyClient --> SyncClient\n        PortkeyClient --> AsyncClient\n    end\n    \n    subgraph \"Usage Tracking State\"\n        CallCounts[\"model_call_counts<br/>dict[str, int]\"]\n        InputTokens[\"model_input_tokens<br/>dict[str, int]\"]\n        OutputTokens[\"model_output_tokens<br/>dict[str, int]\"]\n        TotalTokens[\"model_total_tokens<br/>dict[str, int]\"]\n        \n        PortkeyClient --> CallCounts\n        PortkeyClient --> InputTokens\n        PortkeyClient --> OutputTokens\n        PortkeyClient --> TotalTokens\n    end\n    \n    subgraph \"External Service\"\n        PortkeyAPI[\"Portkey API<br/>api.portkey.ai/v1\"]\n    end\n    \n    ClientBackend --> GetClient\n    GetClient --> BaseLM\n    BaseLM --> PortkeyClient\n    \n    SyncClient --> PortkeyAPI\n    AsyncClient --> PortkeyAPI\n```\n\n**Integration Points:**\n\n| Component | Implementation | Purpose |\n|-----------|---------------|---------|\n| `BaseLM` parent class | [rlm/clients/portkey.py:11]() | Provides abstract interface for all LLM clients |\n| `Portkey` SDK client | [rlm/clients/portkey.py:24]() | Synchronous chat completion requests |\n| `AsyncPortkey` SDK client | [rlm/clients/portkey.py:25]() | Asynchronous chat completion requests |\n| Per-model tracking dicts | [rlm/clients/portkey.py:29-32]() | Accumulate usage statistics per model |\n| Factory registration | Via `ClientBackend` literal | Selected using `backend=\"portkey\"` parameter |\n\nSources: [rlm/clients/portkey.py:7-33]()\n\n---\n\n## Client Configuration\n\n### Initialization Parameters\n\nThe `PortkeyClient` constructor accepts the following parameters:\n\n| Parameter | Type | Required | Default | Description |\n|-----------|------|----------|---------|-------------|\n| `api_key` | `str` | Yes | None | Portkey API key for authentication |\n| `model_name` | `str \\| None` | No | `None` | Default model to use for completions |\n| `base_url` | `str \\| None` | No | `\"https://api.portkey.ai/v1\"` | Portkey API endpoint URL |\n| `**kwargs` | `Any` | No | - | Additional arguments passed to `BaseLM` |\n\n### Initialization Flow\n\n```mermaid\ngraph TB\n    Init[\"__init__()\"]\n    SuperInit[\"super().__init__()<br/>Pass model_name and kwargs\"]\n    CreateSync[\"Create Portkey instance<br/>api_key + base_url\"]\n    CreateAsync[\"Create AsyncPortkey instance<br/>api_key + base_url\"]\n    StoreModel[\"Store model_name\"]\n    InitTracking[\"Initialize tracking dicts<br/>defaultdict(int)\"]\n    \n    Init --> SuperInit\n    SuperInit --> CreateSync\n    CreateSync --> CreateAsync\n    CreateAsync --> StoreModel\n    StoreModel --> InitTracking\n```\n\n**Constructor Implementation:**\n\n```python\ndef __init__(\n    self,\n    api_key: str,\n    model_name: str | None = None,\n    base_url: str | None = \"https://api.portkey.ai/v1\",\n    **kwargs,\n):\n    super().__init__(model_name=model_name, **kwargs)\n    self.client = Portkey(api_key=api_key, base_url=base_url)\n    self.async_client = AsyncPortkey(api_key=api_key, base_url=base_url)\n    self.model_name = model_name\n    \n    # Per-model usage tracking\n    self.model_call_counts: dict[str, int] = defaultdict(int)\n    self.model_input_tokens: dict[str, int] = defaultdict(int)\n    self.model_output_tokens: dict[str, int] = defaultdict(int)\n    self.model_total_tokens: dict[str, int] = defaultdict(int)\n```\n\n**Key Differences from OpenAI Client:**\n\n- The `api_key` parameter is **required** (no environment variable fallback)\n- No automatic API key detection based on `base_url`\n- Single endpoint URL (no special-casing for different providers)\n\nSources: [rlm/clients/portkey.py:16-32]()\n\n---\n\n## Completion Methods\n\n### Synchronous Completion\n\nThe `completion()` method implements the synchronous chat completion interface defined by `BaseLM`.\n\n```mermaid\ngraph TB\n    Input[\"completion() called\"]\n    ValidatePrompt[\"Validate prompt type<br/>str or list[dict]\"]\n    ConvertStr[\"If str: wrap in messages list<br/>[{'role': 'user', 'content': prompt}]\"]\n    ConvertList[\"If list: use as messages\"]\n    ResolveModel[\"Resolve model:<br/>parameter or self.model_name\"]\n    CheckModel[\"Validate model is not None\"]\n    CreateCompletion[\"self.client.chat.completions.create()<br/>model + messages\"]\n    TrackUsage[\"_track_cost()<br/>Update tracking dicts\"]\n    Return[\"Return response.choices[0].message.content\"]\n    \n    Input --> ValidatePrompt\n    ValidatePrompt --> ConvertStr\n    ValidatePrompt --> ConvertList\n    ConvertStr --> ResolveModel\n    ConvertList --> ResolveModel\n    ResolveModel --> CheckModel\n    CheckModel --> CreateCompletion\n    CreateCompletion --> TrackUsage\n    TrackUsage --> Return\n```\n\n**Method Signature and Implementation:**\n\n[rlm/clients/portkey.py:34-51]() defines the synchronous completion method:\n\n```python\ndef completion(self, prompt: str | list[dict[str, Any]], model: str | None = None) -> str:\n    if isinstance(prompt, str):\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n    elif isinstance(prompt, list) and all(isinstance(item, dict) for item in prompt):\n        messages = prompt\n    else:\n        raise ValueError(f\"Invalid prompt type: {type(prompt)}\")\n    \n    model = model or self.model_name\n    if not model:\n        raise ValueError(\"Model name is required for Portkey client.\")\n    \n    response = self.client.chat.completions.create(\n        model=model,\n        messages=messages,\n    )\n    self._track_cost(response, model)\n    return response.choices[0].message.content\n```\n\n**Prompt Format Support:**\n\n| Input Type | Conversion | Example |\n|------------|------------|---------|\n| `str` | Wrapped as user message | `\"What is 2+2?\"` → `[{\"role\": \"user\", \"content\": \"What is 2+2?\"}]` |\n| `list[dict]` | Used directly | `[{\"role\": \"system\", \"content\": \"...\"}, {\"role\": \"user\", \"content\": \"...\"}]` |\n\nSources: [rlm/clients/portkey.py:34-51]()\n\n### Asynchronous Completion\n\nThe `acompletion()` method provides async/await support for non-blocking LLM requests.\n\n[rlm/clients/portkey.py:53-67]() implements the asynchronous variant:\n\n```python\nasync def acompletion(self, prompt: str | dict[str, Any], model: str | None = None) -> str:\n    if isinstance(prompt, str):\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n    elif isinstance(prompt, list) and all(isinstance(item, dict) for item in prompt):\n        messages = prompt\n    else:\n        raise ValueError(f\"Invalid prompt type: {type(prompt)}\")\n    \n    model = model or self.model_name\n    if not model:\n        raise ValueError(\"Model name is required for Portkey client.\")\n    \n    response = await self.async_client.chat.completions.create(model=model, messages=messages)\n    self._track_cost(response, model)\n    return response.choices[0].message.content\n```\n\n**Key Implementation Details:**\n\n- Uses `self.async_client` (AsyncPortkey instance)\n- Identical prompt validation and model resolution logic\n- Returns `str` (completion text content)\n- Calls the same `_track_cost()` method for usage tracking\n\nSources: [rlm/clients/portkey.py:53-67]()\n\n---\n\n## Usage Tracking System\n\nThe `PortkeyClient` maintains per-model usage statistics using dictionary-based accumulators. This enables cost monitoring and resource usage analysis across multiple models accessed through the Portkey gateway.\n\n```mermaid\ngraph TB\n    subgraph \"Tracking Data Structures\"\n        CallCounts[\"model_call_counts<br/>dict[str, int]<br/>Total API calls per model\"]\n        InputTokens[\"model_input_tokens<br/>dict[str, int]<br/>Prompt tokens per model\"]\n        OutputTokens[\"model_output_tokens<br/>dict[str, int]<br/>Completion tokens per model\"]\n        TotalTokens[\"model_total_tokens<br/>dict[str, int]<br/>Total tokens per model\"]\n        \n        LastPrompt[\"last_prompt_tokens<br/>int<br/>Most recent call\"]\n        LastCompletion[\"last_completion_tokens<br/>int<br/>Most recent call\"]\n    end\n    \n    subgraph \"Tracking Methods\"\n        TrackCost[\"_track_cost()<br/>Update all counters\"]\n        GetSummary[\"get_usage_summary()<br/>Return UsageSummary\"]\n        GetLast[\"get_last_usage()<br/>Return ModelUsageSummary\"]\n    end\n    \n    subgraph \"Output Types\"\n        ModelSummary[\"ModelUsageSummary<br/>- total_calls<br/>- total_input_tokens<br/>- total_output_tokens\"]\n        UsageSummary[\"UsageSummary<br/>- model_usage_summaries<br/>  dict[str, ModelUsageSummary]\"]\n    end\n    \n    Completion[\"completion() / acompletion()\"]\n    Response[\"ChatCompletions response\"]\n    \n    Completion --> Response\n    Response --> TrackCost\n    \n    TrackCost --> CallCounts\n    TrackCost --> InputTokens\n    TrackCost --> OutputTokens\n    TrackCost --> TotalTokens\n    TrackCost --> LastPrompt\n    TrackCost --> LastCompletion\n    \n    CallCounts --> GetSummary\n    InputTokens --> GetSummary\n    OutputTokens --> GetSummary\n    \n    GetSummary --> UsageSummary\n    UsageSummary --> ModelSummary\n    \n    LastPrompt --> GetLast\n    LastCompletion --> GetLast\n    GetLast --> ModelSummary\n```\n\n### Internal Tracking Method\n\n[rlm/clients/portkey.py:69-77]() implements the `_track_cost()` method:\n\n```python\ndef _track_cost(self, response: ChatCompletions, model: str):\n    self.model_call_counts[model] += 1\n    self.model_input_tokens[model] += response.usage.prompt_tokens\n    self.model_output_tokens[model] += response.usage.completion_tokens\n    self.model_total_tokens[model] += response.usage.total_tokens\n    \n    # Track last call for handler to read\n    self.last_prompt_tokens = response.usage.prompt_tokens\n    self.last_completion_tokens = response.usage.completion_tokens\n```\n\n**Tracking Behavior:**\n\n| Counter | Scope | Update Logic |\n|---------|-------|--------------|\n| `model_call_counts[model]` | Per-model | Incremented by 1 per call |\n| `model_input_tokens[model]` | Per-model | Accumulated from `response.usage.prompt_tokens` |\n| `model_output_tokens[model]` | Per-model | Accumulated from `response.usage.completion_tokens` |\n| `model_total_tokens[model]` | Per-model | Accumulated from `response.usage.total_tokens` |\n| `last_prompt_tokens` | Global | Overwritten with latest value |\n| `last_completion_tokens` | Global | Overwritten with latest value |\n\n### Usage Summary Retrieval\n\n[rlm/clients/portkey.py:79-87]() implements `get_usage_summary()`:\n\n```python\ndef get_usage_summary(self) -> UsageSummary:\n    model_summaries = {}\n    for model in self.model_call_counts:\n        model_summaries[model] = ModelUsageSummary(\n            total_calls=self.model_call_counts[model],\n            total_input_tokens=self.model_input_tokens[model],\n            total_output_tokens=self.model_output_tokens[model],\n        )\n    return UsageSummary(model_usage_summaries=model_summaries)\n```\n\n**Returns:** A `UsageSummary` object containing per-model statistics for all models used by this client instance.\n\n### Last Call Usage\n\n[rlm/clients/portkey.py:89-94]() implements `get_last_usage()`:\n\n```python\ndef get_last_usage(self) -> ModelUsageSummary:\n    return ModelUsageSummary(\n        total_calls=1,\n        total_input_tokens=self.last_prompt_tokens,\n        total_output_tokens=self.last_completion_tokens,\n    )\n```\n\n**Returns:** A `ModelUsageSummary` object containing statistics from the most recent completion call. This is used by the `LMHandler` to attribute token usage to specific sub-calls.\n\nSources: [rlm/clients/portkey.py:69-94]()\n\n---\n\n## Code Entity Reference\n\n### Class: PortkeyClient\n\n**Location:** [rlm/clients/portkey.py:11-94]()\n\n**Inheritance:** `BaseLM` → `PortkeyClient`\n\n**Instance Variables:**\n\n| Variable | Type | Purpose |\n|----------|------|---------|\n| `client` | `Portkey` | Synchronous Portkey SDK client |\n| `async_client` | `AsyncPortkey` | Asynchronous Portkey SDK client |\n| `model_name` | `str \\| None` | Default model identifier |\n| `model_call_counts` | `dict[str, int]` | Per-model API call counter |\n| `model_input_tokens` | `dict[str, int]` | Per-model prompt token accumulator |\n| `model_output_tokens` | `dict[str, int]` | Per-model completion token accumulator |\n| `model_total_tokens` | `dict[str, int]` | Per-model total token accumulator |\n| `last_prompt_tokens` | `int` | Most recent call's prompt tokens |\n| `last_completion_tokens` | `int` | Most recent call's completion tokens |\n\n**Public Methods:**\n\n| Method | Signature | Returns | Purpose |\n|--------|-----------|---------|---------|\n| `__init__` | `(api_key: str, model_name: str \\| None, base_url: str \\| None, **kwargs)` | `None` | Initialize client with Portkey credentials |\n| `completion` | `(prompt: str \\| list[dict], model: str \\| None)` | `str` | Synchronous chat completion |\n| `acompletion` | `(prompt: str \\| list[dict], model: str \\| None)` | `str` | Asynchronous chat completion |\n| `get_usage_summary` | `()` | `UsageSummary` | Aggregate usage across all models |\n| `get_last_usage` | `()` | `ModelUsageSummary` | Usage from most recent call |\n\n**Private Methods:**\n\n| Method | Signature | Purpose |\n|--------|-----------|---------|\n| `_track_cost` | `(response: ChatCompletions, model: str)` | Update usage tracking dictionaries |\n\n### Dependencies\n\n| Import | Source | Purpose |\n|--------|--------|---------|\n| `Portkey` | `portkey_ai` | Synchronous Portkey SDK client |\n| `AsyncPortkey` | `portkey_ai` | Asynchronous Portkey SDK client |\n| `ChatCompletions` | `portkey_ai.api_resources.types.chat_complete_type` | Response type for completions |\n| `BaseLM` | `rlm.clients.base_lm` | Abstract base class for LLM clients |\n| `ModelUsageSummary` | `rlm.core.types` | Per-model usage statistics type |\n| `UsageSummary` | `rlm.core.types` | Aggregated usage statistics type |\n| `defaultdict` | `collections` | Auto-initializing dictionary for counters |\n\nSources: [rlm/clients/portkey.py:1-8]()\n\n---\n\n# Page: Other Backends\n\n# Other Backends\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [AGENTS.md](AGENTS.md)\n- [examples/lm_in_prime_repl.py](examples/lm_in_prime_repl.py)\n- [examples/prime_repl_example.py](examples/prime_repl_example.py)\n- [rlm/clients/__init__.py](rlm/clients/__init__.py)\n- [rlm/clients/azure_openai.py](rlm/clients/azure_openai.py)\n- [rlm/core/types.py](rlm/core/types.py)\n- [rlm/environments/constants.py](rlm/environments/constants.py)\n- [tests/test_imports.py](tests/test_imports.py)\n\n</details>\n\n\n\nThis page documents additional LLM client backends beyond OpenAI and Portkey: the `AnthropicClient` for Claude models and the `LiteLLMClient` for unified access to 100+ LLM providers. For information about the OpenAI backend (including OpenRouter and vLLM), see [OpenAI Client](#4.2). For information about the Portkey backend, see [Portkey Client](#4.3). For the overall client architecture and `BaseLM` interface, see [Client Architecture](#4.1).\n\n## Overview\n\nThe RLM system supports two additional backend types through the `get_client` factory function:\n\n- **anthropic**: Direct integration with Anthropic's Claude models\n- **litellm**: Unified interface to 100+ LLM providers through the LiteLLM library\n\nBoth backends implement the `BaseLM` interface and are instantiated via the `get_client` factory, allowing seamless switching between providers without changing application code.\n\n**Sources:** [rlm/clients/__init__.py:39-46]()\n\n## Backend Registration in Factory\n\nThe `get_client` factory function routes backend strings to their corresponding client implementations. The Anthropic and LiteLLM backends are registered alongside other providers:\n\n```mermaid\ngraph LR\n    Factory[\"get_client()\"]\n    \n    Factory -->|\"backend='anthropic'\"| AnthropicClient[\"AnthropicClient\"]\n    Factory -->|\"backend='litellm'\"| LiteLLMClient[\"LiteLLMClient\"]\n    Factory -->|\"backend='openai'\"| OpenAIClient[\"OpenAIClient\"]\n    Factory -->|\"backend='portkey'\"| PortkeyClient[\"PortkeyClient\"]\n    \n    AnthropicClient --> BaseLM[\"BaseLM interface\"]\n    LiteLLMClient --> BaseLM\n    OpenAIClient --> BaseLM\n    PortkeyClient --> BaseLM\n    \n    BaseLM --> LMHandler[\"LMHandler\"]\n```\n\n**Diagram: Backend Client Registration and Routing**\n\nThe factory performs lazy imports to avoid loading dependencies for unused backends, instantiating only the requested client class with the provided `backend_kwargs`.\n\n**Sources:** [rlm/clients/__init__.py:11-50]()\n\n## Anthropic Client\n\nThe `AnthropicClient` provides direct access to Anthropic's Claude models (Claude 3.5 Sonnet, Claude 3 Opus, etc.) without intermediary gateways.\n\n### Configuration\n\nThe Anthropic backend is selected by passing `backend=\"anthropic\"` to the RLM initialization:\n\n```python\nimport rlm\n\nresult = rlm.completion(\n    prompt=\"Your prompt here\",\n    backend=\"anthropic\",\n    backend_kwargs={\n        \"api_key\": \"sk-ant-...\",  # Or use ANTHROPIC_API_KEY env var\n        \"model_name\": \"claude-3-5-sonnet-20241022\",\n        # Additional Anthropic-specific parameters\n    }\n)\n```\n\nThe client is imported from `rlm.clients.anthropic` and instantiated with the provided `backend_kwargs` dictionary.\n\n**Sources:** [rlm/clients/__init__.py:43-46]()\n\n### Backend Parameters\n\nThe `AnthropicClient` accepts standard Anthropic API parameters through `backend_kwargs`:\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| `api_key` | str | Anthropic API key (can also use `ANTHROPIC_API_KEY` env var) |\n| `model_name` | str | Model identifier (e.g., \"claude-3-5-sonnet-20241022\") |\n| `max_tokens` | int | Maximum tokens in response |\n| `temperature` | float | Sampling temperature (0.0-1.0) |\n| `top_p` | float | Nucleus sampling parameter |\n| `timeout` | float | Request timeout in seconds |\n\n### Usage in REPL Environments\n\nThe Anthropic backend integrates with all REPL environments through the `LMHandler`. Code running in environments can make recursive calls to Claude models via `llm_query`:\n\n```python\n# Inside REPL environment\nresponse = llm_query(\"Analyze this data: \" + str(data))\n```\n\nThe environment forwards the request to the `LMHandler`, which routes it to the `AnthropicClient` instance.\n\n**Sources:** [rlm/clients/__init__.py:43-46](), [rlm/environments/base_env.py:6-27]()\n\n## LiteLLM Client\n\nThe `LiteLLMClient` provides a unified interface to 100+ LLM providers (OpenAI, Anthropic, Cohere, Replicate, Hugging Face, etc.) through the LiteLLM library. This backend enables rapid experimentation with different models without changing client code.\n\n### Configuration\n\nThe LiteLLM backend is selected by passing `backend=\"litellm\"`:\n\n```python\nimport rlm\n\nresult = rlm.completion(\n    prompt=\"Your prompt here\",\n    backend=\"litellm\",\n    backend_kwargs={\n        \"model\": \"gpt-4\",  # LiteLLM handles provider routing\n        \"api_key\": \"...\",\n        \"temperature\": 0.7,\n        # LiteLLM accepts provider-specific params\n    }\n)\n```\n\nThe client is imported from `rlm.clients.litellm` and instantiated with the provided `backend_kwargs`.\n\n**Sources:** [rlm/clients/__init__.py:39-42]()\n\n### Model Routing\n\nLiteLLM uses provider prefixes in model names to route requests:\n\n| Model String | Provider | Example |\n|--------------|----------|---------|\n| `gpt-4` | OpenAI | Direct OpenAI API call |\n| `claude-3-sonnet-20240229` | Anthropic | Direct Anthropic API call |\n| `command-r-plus` | Cohere | Direct Cohere API call |\n| `replicate/meta/llama-2-70b-chat` | Replicate | Replicate API with model path |\n| `huggingface/bigcode/starcoder` | Hugging Face | Hugging Face Inference API |\n| `vertex_ai/gemini-pro` | Google Vertex | Google Vertex AI |\n| `bedrock/anthropic.claude-v2` | AWS Bedrock | AWS Bedrock |\n\nLiteLLM automatically detects the provider and formats requests appropriately, requiring only the corresponding API keys in environment variables.\n\n### Backend Parameters\n\nThe `LiteLLMClient` accepts parameters for any supported provider:\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| `model` | str | Model identifier with optional provider prefix |\n| `api_key` | str | API key for the model provider |\n| `api_base` | str | Custom API endpoint (for local servers) |\n| `temperature` | float | Sampling temperature |\n| `max_tokens` | int | Maximum response tokens |\n| `top_p` | float | Nucleus sampling |\n| `timeout` | float | Request timeout |\n| Provider-specific | various | Additional parameters passed through to underlying API |\n\n### Usage Patterns\n\nThe LiteLLM backend is particularly useful for:\n\n1. **Multi-provider experimentation**: Test the same prompt across different providers\n2. **Fallback strategies**: Switch providers if one is unavailable\n3. **Cost optimization**: Route to cheaper models when appropriate\n4. **Local model testing**: Connect to locally hosted models via `api_base`\n\n**Sources:** [rlm/clients/__init__.py:39-42]()\n\n## Backend Selection and Comparison\n\n```mermaid\ngraph TB\n    Decision{\"Backend Selection\"}\n    \n    Decision -->|\"Need Claude?\"| Anthropic[\"backend='anthropic'<br/>AnthropicClient\"]\n    Decision -->|\"Multi-provider support?\"| LiteLLM[\"backend='litellm'<br/>LiteLLMClient\"]\n    Decision -->|\"OpenAI/compatible?\"| OpenAI[\"backend='openai'<br/>OpenAIClient\"]\n    Decision -->|\"Gateway/observability?\"| Portkey[\"backend='portkey'<br/>PortkeyClient\"]\n    \n    Anthropic --> Features1[\"+ Direct Claude access<br/>+ Anthropic-specific features<br/>- Single provider\"]\n    LiteLLM --> Features2[\"+ 100+ providers<br/>+ Easy switching<br/>+ Fallback support<br/>- Extra dependency\"]\n    OpenAI --> Features3[\"+ Native OpenAI<br/>+ vLLM support<br/>+ OpenRouter<br/>- Limited to compatible APIs\"]\n    Portkey --> Features4[\"+ Observability<br/>+ Load balancing<br/>+ Caching<br/>- Requires Portkey account\"]\n```\n\n**Diagram: Backend Selection Decision Tree**\n\n**Sources:** [rlm/clients/__init__.py:11-50]()\n\n## Backend Comparison Table\n\n| Feature | Anthropic | LiteLLM | OpenAI | Portkey |\n|---------|-----------|---------|--------|---------|\n| Claude Models | ✓ | ✓ | ✗ | ✓ |\n| GPT Models | ✗ | ✓ | ✓ | ✓ |\n| Multi-provider | ✗ | ✓ | ✗ | ✓ |\n| Local Models | ✗ | ✓ | ✓ (vLLM) | ✗ |\n| Observability | ✗ | ✗ | ✗ | ✓ |\n| Extra Dependencies | anthropic | litellm | openai | portkey-ai |\n| Setup Complexity | Low | Low | Low | Medium |\n\n## Integration with RLM System\n\nBoth backends integrate identically with the RLM system architecture:\n\n```mermaid\nsequenceDiagram\n    participant RLM as \"RLM Core\"\n    participant Handler as \"LMHandler\"\n    participant Client as \"AnthropicClient<br/>or<br/>LiteLLMClient\"\n    participant API as \"Provider API\"\n    \n    RLM->>Handler: \"request completion\"\n    Handler->>Client: \"completion(prompt, model, ...)\"\n    Client->>Client: \"format for provider\"\n    Client->>API: \"HTTP request\"\n    API-->>Client: \"response\"\n    Client->>Client: \"parse & normalize\"\n    Client-->>Handler: \"LMResponse\"\n    Handler-->>RLM: \"RLMChatCompletion\"\n```\n\n**Diagram: Backend Integration Sequence**\n\nBoth clients:\n1. Implement the `BaseLM` interface for uniform access\n2. Accept requests from `LMHandler` via the completion method\n3. Format requests according to their provider's API specification\n4. Parse responses into the standardized `LMResponse` and `RLMChatCompletion` types\n5. Track token usage in `UsageSummary` objects\n6. Handle errors consistently with the RLM error model\n\n**Sources:** [rlm/clients/__init__.py:11-50](), [examples/lm_in_repl.py:36-67]()\n\n## Environment Variable Configuration\n\nBoth backends support API key configuration via environment variables:\n\n```bash\n# Anthropic backend\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"\n\n# LiteLLM backend (provider-specific)\nexport OPENAI_API_KEY=\"sk-...\"\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"\nexport COHERE_API_KEY=\"...\"\n# etc.\n```\n\nThe RLM system loads environment variables via `dotenv` at import time, allowing backends to authenticate without passing keys explicitly in code.\n\n**Sources:** [rlm/clients/__init__.py:1-8](), [examples/lm_in_repl.py:10-16]()\n\n## Error Handling\n\nBoth backends follow the standard RLM error handling pattern:\n\n- API authentication errors propagate as exceptions during client initialization\n- Request failures return `LMResponse` objects with populated `error` fields\n- The `LMHandler` logs errors but allows the RLM system to decide retry/fallback behavior\n- Network timeouts respect the `timeout` parameter in `backend_kwargs`\n\nClients implementing `BaseLM` are responsible for normalizing provider-specific error formats into the common `LMResponse.error` field.\n\n**Sources:** [rlm/clients/__init__.py:11-50]()\n\n---\n\n# Page: Execution Environments\n\n# Execution Environments\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [Makefile](Makefile)\n- [README.md](README.md)\n- [rlm/environments/base_env.py](rlm/environments/base_env.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis document provides a comprehensive technical overview of the execution environment system in RLM, which defines how and where code generated by language models is executed. Execution environments are REPL-like abstractions that provide isolated or non-isolated contexts for running Python code, handling sub-LM calls, and managing state across iterations.\n\nThis page covers the core environment architecture, including the `BaseEnv` interface hierarchy, communication protocols, and persistence mechanisms. For detailed documentation of specific environment implementations, see:\n- [Environment Overview](#5.1) - `BaseEnv`, `IsolatedEnv`, `NonIsolatedEnv` interfaces\n- [LocalREPL](#5.2) - Default local Python execution environment\n- [DockerREPL](#5.3) - Containerized execution environment\n- [Cloud Sandbox Environments](#5.4) - Modal, Prime, and Daytona cloud-based sandboxes\n\nFor information about how environments communicate with the LM Handler, see [LM Handler](#3.2). For details on the communication data types and protocols, see [Communication Protocols and Data Types](#3.5).\n\n---\n\n## Environment Type Hierarchy\n\nThe RLM environment system is built around a three-level abstraction hierarchy that separates the core interface, isolation guarantees, and concrete implementations.\n\n### Class Hierarchy Diagram\n\n```mermaid\ngraph TB\n    BaseEnv[\"BaseEnv<br/>(abstract base class)\"]\n    \n    subgraph \"Isolation Strategy\"\n        NonIsolated[\"NonIsolatedEnv<br/>(same machine)\"]\n        Isolated[\"IsolatedEnv<br/>(cloud sandboxes)\"]\n    end\n    \n    subgraph \"Non-Isolated Implementations\"\n        LocalREPL[\"LocalREPL<br/>(Python exec)\"]\n        DockerREPL[\"DockerREPL<br/>(Docker container)\"]\n    end\n    \n    subgraph \"Isolated Implementations\"\n        ModalREPL[\"ModalREPL<br/>(Modal Sandboxes)\"]\n        PrimeREPL[\"PrimeREPL<br/>(Prime Intellect)\"]\n        DaytonaREPL[\"DaytonaREPL<br/>(Daytona SDK)\"]\n    end\n    \n    BaseEnv --> NonIsolated\n    BaseEnv --> Isolated\n    \n    NonIsolated --> LocalREPL\n    NonIsolated --> DockerREPL\n    \n    Isolated --> ModalREPL\n    Isolated --> PrimeREPL\n    Isolated --> DaytonaREPL\n    \n    Protocol[\"SupportsPersistence<br/>(protocol)\"]\n    Protocol -.->|\"implements\"| LocalREPL\n```\n\n**Sources:** [rlm/environments/base_env.py:1-183]()\n\n### BaseEnv Interface\n\nThe `BaseEnv` abstract base class defines the core contract that all execution environments must implement. It provides three essential operations:\n\n| Method | Purpose | Returns |\n|--------|---------|---------|\n| `setup()` | Initialize the environment, start processes/containers | `None` |\n| `load_context(context_payload)` | Load context data into the environment namespace | `None` |\n| `execute_code(code)` | Execute Python code and return results | `REPLResult` |\n\nAll environments are initialized with two key parameters:\n- `persistent: bool` - Whether the environment maintains state across multiple code executions\n- `depth: int` - The recursion depth for sub-LM calls (passed to LM Handler)\n\n**Sources:** [rlm/environments/base_env.py:7-29]()\n\n### IsolatedEnv vs NonIsolatedEnv\n\nThe `IsolatedEnv` and `NonIsolatedEnv` classes differentiate environments based on their isolation guarantees:\n\n```mermaid\ngraph LR\n    subgraph \"NonIsolatedEnv\"\n        SameMachine[\"Executes on same<br/>machine as RLM\"]\n        DirectSocket[\"Direct TCP socket<br/>to LM Handler\"]\n        SharedResources[\"May share filesystem,<br/>network, processes\"]\n    end\n    \n    subgraph \"IsolatedEnv\"\n        CloudMachine[\"Executes on separate<br/>cloud machine\"]\n        HTTPBroker[\"HTTP broker +<br/>polling pattern\"]\n        CompleteIsolation[\"Complete isolation<br/>from host process\"]\n    end\n    \n    NonIsolatedEnv --> SameMachine\n    NonIsolatedEnv --> DirectSocket\n    NonIsolatedEnv --> SharedResources\n    \n    IsolatedEnv --> CloudMachine\n    IsolatedEnv --> HTTPBroker\n    IsolatedEnv --> CompleteIsolation\n```\n\n**Non-Isolated Environments** ([rlm/environments/base_env.py:53-73]()):\n- Execute on the same physical/virtual machine as the RLM orchestrator\n- Use direct TCP socket connections to communicate with the LM Handler\n- Examples: `LocalREPL` (same process), `DockerREPL` (same host, different container)\n- Trade-off: Better performance, less security isolation\n\n**Isolated Environments** ([rlm/environments/base_env.py:31-50]()):\n- Execute on completely separate cloud-based machines (e.g., Modal, Prime Intellect, Daytona)\n- Use HTTP broker pattern with polling to bridge network isolation\n- Examples: `ModalREPL`, `PrimeREPL`, `DaytonaREPL`\n- Trade-off: Complete isolation, higher latency\n\n**Sources:** [rlm/environments/base_env.py:31-73](), [README.md:79-115]()\n\n---\n\n## Communication Patterns\n\nExecution environments must communicate with the LM Handler to make sub-LM calls (recursive invocations). The communication pattern differs based on isolation level.\n\n### Non-Isolated: Direct Socket Protocol\n\n```mermaid\nsequenceDiagram\n    participant Code as \"Executed Code\"\n    participant Socket as \"TCP Socket\"\n    participant Handler as \"LMHandler<br/>(host:port)\"\n    \n    Code->>Socket: \"llm_query(prompt, model)\"\n    Socket->>Handler: \"LMRequest<br/>(4-byte length + JSON)\"\n    Handler->>Handler: \"Route to client<br/>based on depth\"\n    Handler->>Socket: \"LMResponse<br/>(JSON)\"\n    Socket->>Code: \"RLMChatCompletion\"\n```\n\nNon-isolated environments establish direct TCP socket connections to the LM Handler server. The `llm_query()` function injected into the execution namespace:\n1. Serializes the request as JSON with a 4-byte length prefix\n2. Sends directly to the handler's `(host, port)` address\n3. Receives the response synchronously\n4. Deserializes and returns the `RLMChatCompletion` object\n\n**Sources:** [README.md:79-94]()\n\n### Isolated: HTTP Broker with Polling\n\n```mermaid\nsequenceDiagram\n    participant Code as \"Executed Code<br/>(in sandbox)\"\n    participant Broker as \"HTTP Broker<br/>(Flask in sandbox)\"\n    participant Tunnel as \"Tunnel/Preview<br/>(Modal/Prime/Daytona)\"\n    participant Poller as \"Poller Thread<br/>(on host)\"\n    participant Handler as \"LMHandler<br/>(on host)\"\n    \n    Code->>Broker: \"llm_query(prompt, model)\"\n    Broker->>Broker: \"Store in pending_requests\"\n    Broker-->>Code: \"Return immediately<br/>(non-blocking)\"\n    \n    loop \"Poll every 100ms\"\n        Poller->>Tunnel: \"GET /pending_requests\"\n        Tunnel->>Broker: \"Forward request\"\n        Broker->>Poller: \"[LMRequest, ...]\"\n        Poller->>Handler: \"Process requests\"\n        Handler->>Poller: \"[LMResponse, ...]\"\n        Poller->>Tunnel: \"POST /completed_requests\"\n        Tunnel->>Broker: \"Forward responses\"\n    end\n    \n    Code->>Broker: \"Wait for response<br/>(polling loop)\"\n    Broker->>Code: \"RLMChatCompletion\"\n```\n\nIsolated environments cannot establish direct TCP connections due to network isolation. Instead, they use an HTTP broker pattern:\n\n1. **Broker Server**: Flask HTTP server runs inside the sandbox ([Modal's Sandbox.create()](https://modal.com/docs/guide/sandboxes), Prime's expose, Daytona's preview URLs)\n2. **Pending Queue**: `llm_query()` posts requests to the broker and immediately returns a request ID\n3. **Poller Thread**: Host-side thread polls the broker every 100ms for pending requests\n4. **Handler Processing**: Poller forwards requests to LM Handler, receives responses\n5. **Completion Queue**: Poller posts responses back to broker\n6. **Result Retrieval**: `llm_query()` polls the broker until its response is available\n\n**Sources:** [README.md:95-115]()\n\n---\n\n## SupportsPersistence Protocol\n\nThe `SupportsPersistence` protocol defines the interface for environments that support multi-turn conversations with persistent state. Environments implementing this protocol can maintain context and history across multiple `RLM.completion()` calls.\n\n### Protocol Methods\n\n```mermaid\ngraph TB\n    Protocol[\"SupportsPersistence<br/>(runtime_checkable)\"]\n    \n    subgraph \"Required Methods\"\n        UpdateAddress[\"update_handler_address<br/>(address: tuple)\"]\n        AddContext[\"add_context<br/>(payload, index?) -> int\"]\n        GetContextCount[\"get_context_count<br/>() -> int\"]\n        AddHistory[\"add_history<br/>(messages, index?) -> int\"]\n        GetHistoryCount[\"get_history_count<br/>() -> int\"]\n    end\n    \n    Protocol --> UpdateAddress\n    Protocol --> AddContext\n    Protocol --> GetContextCount\n    Protocol --> AddHistory\n    Protocol --> GetHistoryCount\n    \n    subgraph \"Versioning Scheme\"\n        ContextVersions[\"context_0, context_1, ...<br/>context -> context_0\"]\n        HistoryVersions[\"history_0, history_1, ...<br/>history -> history_0\"]\n    end\n    \n    AddContext -.-> ContextVersions\n    AddHistory -.-> HistoryVersions\n```\n\n**Sources:** [rlm/environments/base_env.py:76-182]()\n\n### Method Specifications\n\n| Method | Purpose | Behavior |\n|--------|---------|----------|\n| `update_handler_address(address)` | Update LM Handler address for sub-calls | Called by RLM when handler address changes between completions |\n| `add_context(payload, index?)` | Store context payload as `context_N` | Auto-increment if `index=None`, otherwise use specified index |\n| `get_context_count()` | Return number of contexts stored | Used by RLM to inform model of available contexts |\n| `add_history(messages, index?)` | Store message history as `history_N` | Must deep copy the list; auto-increment if `index=None` |\n| `get_history_count()` | Return number of histories stored | Used by RLM to inform model of available histories |\n\n**Versioning and Aliasing:**\n- Contexts are versioned: `context_0`, `context_1`, `context_2`, ...\n- Histories are versioned: `history_0`, `history_1`, `history_2`, ...\n- Unversioned aliases always point to index 0:\n  - `context` → `context_0`\n  - `history` → `history_0`\n\n**Sources:** [rlm/environments/base_env.py:108-182]()\n\n### Runtime Checking\n\nEnvironments can be checked for persistence support at runtime:\n\n```python\nfrom rlm.environments.base_env import SupportsPersistence\n\nif isinstance(env, SupportsPersistence):\n    env.add_context(my_context)\n    env.add_history(my_history)\n```\n\nThis pattern allows the RLM orchestrator to conditionally enable persistent mode features without requiring all environments to implement the protocol.\n\n**Sources:** [rlm/environments/base_env.py:76-106]()\n\n---\n\n## Environment Lifecycle\n\nThe typical lifecycle of an environment during an RLM execution follows a four-phase pattern:\n\n```mermaid\nstateDiagram-v2\n    [*] --> Created: \"BaseEnv.__init__()\"\n    \n    Created --> Setup: \"env.setup()\"\n    note right of Setup\n        Initialize resources:\n        - Start containers/sandboxes\n        - Create namespaces\n        - Establish connections\n    end note\n    \n    Setup --> ContextLoaded: \"env.load_context(payload)\"\n    note right of ContextLoaded\n        Load data into namespace:\n        - context variable\n        - llm_query functions\n        - State management\n    end note\n    \n    ContextLoaded --> Executing: \"env.execute_code(code)\"\n    note right of Executing\n        Execute Python code:\n        - Run in namespace\n        - Capture stdout/stderr\n        - Track sub-LM calls\n        - Serialize state\n    end note\n    \n    Executing --> Executing: \"Multiple iterations\"\n    Executing --> [*]: \"Cleanup/teardown\"\n```\n\n### Phase 1: Initialization\n\nThe environment is instantiated with configuration parameters:\n\n```python\nenv = get_environment(\n    environment_type=\"local\",  # or \"docker\", \"modal\", \"prime\", \"daytona\"\n    persistent=True,\n    depth=1,\n    **environment_kwargs\n)\n```\n\n**Sources:** [README.md:82-87]()\n\n### Phase 2: Setup\n\nThe `setup()` method initializes the execution context:\n- **LocalREPL**: Creates sandboxed namespace with `_SAFE_BUILTINS`\n- **DockerREPL**: Starts Docker container, establishes volume mounts\n- **ModalREPL**: Creates Modal Sandbox, sets up encrypted tunnel\n- **PrimeREPL**: Creates Prime sandbox, exposes HTTP broker port\n- **DaytonaREPL**: Launches Daytona workspace, configures preview URL\n\n**Sources:** [rlm/environments/base_env.py:19-20]()\n\n### Phase 3: Context Loading\n\nThe `load_context(context_payload)` method injects context data into the execution namespace:\n\n```python\n# Context becomes available as 'context' variable in executed code\nenv.load_context({\"data\": [1, 2, 3, 4, 5]})\n\n# For persistent environments, can add multiple contexts\nif isinstance(env, SupportsPersistence):\n    env.add_context(context_0)  # Stored as context_0\n    env.add_context(context_1)  # Stored as context_1\n```\n\n**Sources:** [rlm/environments/base_env.py:23-24]()\n\n### Phase 4: Code Execution\n\nThe `execute_code(code)` method runs Python code and returns a `REPLResult`:\n\n```python\nresult = env.execute_code(\"\"\"\nfor item in context:\n    print(item * 2)\n    \nanswer = sum(context)\n\"\"\")\n\n# result.stdout: \"2\\n4\\n6\\n8\\n10\\n\"\n# result.locals: {\"answer\": 15, ...}\n# result.rlm_calls: [RLMChatCompletion, ...] if sub-calls were made\n```\n\nThe `REPLResult` object contains:\n- `stdout: str` - Standard output from code execution\n- `stderr: str` - Standard error from code execution\n- `locals: dict` - Final local namespace variables\n- `execution_time: float` - Time taken to execute (seconds)\n- `rlm_calls: list[RLMChatCompletion]` - Sub-LM calls made during execution\n\n**Sources:** [rlm/environments/base_env.py:27-28]()\n\n---\n\n## Environment Factory and Registration\n\nEnvironments are created through the `get_environment()` factory function, which maps environment type strings to concrete implementations:\n\n```mermaid\ngraph TB\n    Factory[\"get_environment()<br/>(factory function)\"]\n    \n    subgraph \"Environment Types\"\n        Local[\"'local'\"]\n        Docker[\"'docker'\"]\n        Modal[\"'modal'\"]\n        Prime[\"'prime'\"]\n        Daytona[\"'daytona'\"]\n    end\n    \n    subgraph \"Implementations\"\n        LocalREPL[\"LocalREPL<br/>(NonIsolatedEnv)\"]\n        DockerREPL[\"DockerREPL<br/>(NonIsolatedEnv)\"]\n        ModalREPL[\"ModalREPL<br/>(IsolatedEnv)\"]\n        PrimeREPL[\"PrimeREPL<br/>(IsolatedEnv)\"]\n        DaytonaREPL[\"DaytonaREPL<br/>(IsolatedEnv)\"]\n    end\n    \n    Factory --> Local\n    Factory --> Docker\n    Factory --> Modal\n    Factory --> Prime\n    Factory --> Daytona\n    \n    Local --> LocalREPL\n    Docker --> DockerREPL\n    Modal --> ModalREPL\n    Prime --> PrimeREPL\n    Daytona --> DaytonaREPL\n```\n\n### Usage Pattern\n\n```python\nfrom rlm.environments import get_environment\n\n# Create a local environment (default)\nlocal_env = get_environment(\n    environment_type=\"local\",\n    persistent=True\n)\n\n# Create a Docker environment\ndocker_env = get_environment(\n    environment_type=\"docker\",\n    image=\"python:3.11-slim\",\n    volumes={\"/path/on/host\": \"/path/in/container\"}\n)\n\n# Create a Modal environment (requires Modal SDK)\nmodal_env = get_environment(\n    environment_type=\"modal\",\n    image=\"python:3.11\",\n    timeout=300\n)\n```\n\n**Sources:** [README.md:82-87]()\n\n---\n\n## State Management and Persistence\n\nEnvironments that support persistence use state serialization to maintain execution context across multiple code executions. The state is typically stored using `dill` (a more powerful version of `pickle`) in a temporary file.\n\n### State Serialization Pattern\n\n```mermaid\ngraph LR\n    subgraph \"Execution N\"\n        Code1[\"Execute code_1\"]\n        Namespace1[\"namespace_1\"]\n        Serialize1[\"dill.dump()<br/>/tmp/rlm_state.dill\"]\n    end\n    \n    subgraph \"Execution N+1\"\n        Deserialize2[\"dill.load()<br/>/tmp/rlm_state.dill\"]\n        Namespace2[\"namespace_2<br/>(restored)\"]\n        Code2[\"Execute code_2\"]\n    end\n    \n    Code1 --> Namespace1\n    Namespace1 --> Serialize1\n    Serialize1 --> Deserialize2\n    Deserialize2 --> Namespace2\n    Namespace2 --> Code2\n```\n\nThis pattern enables:\n- **Variable persistence**: Variables defined in one execution are available in subsequent executions\n- **Import persistence**: Imported modules remain loaded\n- **Function persistence**: Defined functions can be called across executions\n- **State continuity**: Complex state (data structures, objects) is preserved\n\n**Sources:** Implied from [rlm/environments/base_env.py:76-182]() and architecture diagrams\n\n---\n\n## Environment Selection Guide\n\nThe choice of environment depends on security requirements, performance needs, and deployment constraints:\n\n| Environment | Use Case | Isolation Level | Latency | Setup Complexity |\n|-------------|----------|-----------------|---------|------------------|\n| `LocalREPL` | Development, testing, low-risk tasks | Low (sandboxed builtins) | Lowest | Minimal |\n| `DockerREPL` | Local production, moderate isolation | Medium (containerized) | Low | Requires Docker |\n| `ModalREPL` | Cloud production, high isolation | High (separate machine) | Medium | Requires Modal account |\n| `PrimeREPL` | Distributed compute, high isolation | High (separate machine) | Medium-High | Requires Prime API key |\n| `DaytonaREPL` | Team collaboration, high isolation | High (separate machine) | Medium-High | Requires Daytona SDK |\n\n**Security Considerations:**\n- `LocalREPL` uses `_SAFE_BUILTINS` to restrict dangerous operations but is not production-safe\n- `DockerREPL` provides container-level isolation but shares the host network\n- Cloud sandboxes (`Modal`, `Prime`, `Daytona`) provide complete isolation but require network egress\n\n**Performance Considerations:**\n- Local environments minimize latency for sub-LM calls (direct socket)\n- Isolated environments add 100ms+ per sub-LM call due to polling\n- Persistent mode adds serialization overhead (~10-50ms per execution)\n\n**Sources:** [README.md:79-115]()\n\n---\n\n## Integration with RLM Orchestrator\n\nThe RLM orchestrator integrates environments through a standardized workflow:\n\n```mermaid\nsequenceDiagram\n    participant RLM as \"RLM Orchestrator\"\n    participant Factory as \"get_environment()\"\n    participant Env as \"BaseEnv\"\n    participant Handler as \"LMHandler\"\n    \n    RLM->>Factory: \"environment='local', persistent=True\"\n    Factory->>Env: \"LocalREPL(persistent=True, depth=1)\"\n    Env->>RLM: \"return env\"\n    \n    RLM->>Env: \"env.setup()\"\n    Env->>Env: \"Initialize namespace\"\n    \n    RLM->>Env: \"env.load_context(context_payload)\"\n    Env->>Env: \"Inject context, llm_query\"\n    \n    loop \"For each code block\"\n        RLM->>Env: \"env.execute_code(code)\"\n        Env->>Env: \"exec(code, namespace)\"\n        \n        alt \"Code calls llm_query()\"\n            Env->>Handler: \"LMRequest (via socket/HTTP)\"\n            Handler->>Env: \"LMResponse\"\n        end\n        \n        Env->>RLM: \"REPLResult (stdout, stderr, locals, rlm_calls)\"\n    end\n    \n    alt \"Persistent Mode\"\n        RLM->>Env: \"update_handler_address(new_address)\"\n        RLM->>Env: \"add_context(new_context)\"\n        RLM->>Env: \"add_history(message_history)\"\n    end\n```\n\nThe orchestrator:\n1. Creates the environment via factory\n2. Calls `setup()` to initialize resources\n3. Calls `load_context()` to inject initial data\n4. Executes code blocks via `execute_code()` in each iteration\n5. Aggregates `REPLResult` objects into `RLMIteration` objects\n6. Updates handler address and adds contexts/histories for persistent sessions\n\n**Sources:** [README.md:66-77](), [rlm/environments/base_env.py:1-183]()\n\n---\n\n# Page: Environment Overview\n\n# Environment Overview\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [AGENTS.md](AGENTS.md)\n- [Makefile](Makefile)\n- [README.md](README.md)\n- [examples/lm_in_prime_repl.py](examples/lm_in_prime_repl.py)\n- [examples/prime_repl_example.py](examples/prime_repl_example.py)\n- [rlm/environments/base_env.py](rlm/environments/base_env.py)\n- [rlm/environments/constants.py](rlm/environments/constants.py)\n- [tests/test_imports.py](tests/test_imports.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis document describes the environment abstraction layer in RLM, which provides execution contexts for code generated by language models. It covers the `BaseEnv` interface, the distinction between isolated and non-isolated environments, and the `SupportsPersistence` protocol for multi-turn sessions.\n\nFor specific environment implementations, see [LocalREPL](#5.2), [DockerREPL](#5.3), and [Cloud Sandbox Environments](#5.4). For information about communication protocols between environments and the LM handler, see [Communication Protocols and Data Types](#3.5).\n\n## Environment Architecture Overview\n\nThe environment system is organized as a hierarchy of abstract base classes that define different execution contexts and capabilities.\n\n```mermaid\ngraph TB\n    BaseEnv[\"BaseEnv<br/>(abstract base)\"]\n    NonIsolatedEnv[\"NonIsolatedEnv<br/>(abstract)\"]\n    IsolatedEnv[\"IsolatedEnv<br/>(abstract)\"]\n    \n    LocalREPL[\"LocalREPL\"]\n    DockerREPL[\"DockerREPL\"]\n    ModalREPL[\"ModalREPL\"]\n    PrimeREPL[\"PrimeREPL\"]\n    DaytonaREPL[\"DaytonaREPL\"]\n    \n    SupportsPersistence[\"SupportsPersistence<br/>(Protocol)\"]\n    \n    BaseEnv --> NonIsolatedEnv\n    BaseEnv --> IsolatedEnv\n    \n    NonIsolatedEnv --> LocalREPL\n    NonIsolatedEnv --> DockerREPL\n    \n    IsolatedEnv --> ModalREPL\n    IsolatedEnv --> PrimeREPL\n    IsolatedEnv --> DaytonaREPL\n    \n    LocalREPL -.implements.-> SupportsPersistence\n    \n    style BaseEnv stroke-width:3px\n    style NonIsolatedEnv stroke-width:2px\n    style IsolatedEnv stroke-width:2px\n```\n\n**Environment Type Hierarchy**\n\nSources: [rlm/environments/base_env.py:1-183](), [tests/test_imports.py:140-190]()\n\n## BaseEnv Interface\n\nThe `BaseEnv` class in [rlm/environments/base_env.py:7-29]() is the abstract base class for all REPL-like environments. It defines the core contract that all environments must implement.\n\n### Constructor Parameters\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| `persistent` | `bool` | Whether the environment supports multi-turn sessions (default: `False`) |\n| `depth` | `int` | Recursion depth for sub-LM calls (default: `1`) |\n| `**kwargs` | `Any` | Additional environment-specific configuration |\n\n### Abstract Methods\n\nAll environments must implement three abstract methods:\n\n| Method | Signature | Purpose |\n|--------|-----------|---------|\n| `setup()` | `() -> None` | Initialize the execution environment, globals, and helper functions |\n| `load_context(context_payload)` | `(dict \\| list \\| str) -> None` | Make context data available to executed code |\n| `execute_code(code)` | `(str) -> REPLResult` | Execute code string and return results with stdout/stderr/locals |\n\n```mermaid\ngraph LR\n    setup[\"setup()\"]\n    load_context[\"load_context(payload)\"]\n    execute_code[\"execute_code(code)\"]\n    \n    setup --> load_context\n    load_context --> execute_code\n    execute_code --> execute_code\n    \n    setup -.initializes.-> globals[\"Global namespace<br/>with llm_query()\"]\n    load_context -.creates.-> context_var[\"context variable\"]\n    execute_code -.returns.-> REPLResult[\"REPLResult<br/>(stdout, stderr, locals)\"]\n```\n\n**BaseEnv Method Call Sequence**\n\nSources: [rlm/environments/base_env.py:7-29](), [AGENTS.md:144-176]()\n\n## Environment Categories\n\nRLM divides environments into two fundamental categories based on their execution location and isolation guarantees.\n\n### NonIsolatedEnv\n\nThe `NonIsolatedEnv` class in [rlm/environments/base_env.py:53-73]() represents environments that execute on the same physical or virtual machine as the RLM process. These environments provide varying levels of sandboxing but share the host's computational resources.\n\n**Characteristics:**\n- Execute on the same machine as the LM handler\n- Use direct TCP socket communication (see [Communication Protocols](#3.5))\n- Lower latency for sub-LM calls\n- Share virtual environment dependencies with host\n- Suitable for trusted code execution and development\n\n**Implementations:**\n- `LocalREPL`: In-process execution using Python `exec()` with `_SAFE_BUILTINS` sandboxing\n- `DockerREPL`: Containerized execution using Docker, with socket access via `host.docker.internal`\n\nSources: [rlm/environments/base_env.py:53-73](), [README.md:89-93]()\n\n### IsolatedEnv\n\nThe `IsolatedEnv` class in [rlm/environments/base_env.py:31-50]() represents environments that execute on completely separate machines from the LM process, typically in cloud-based sandboxes.\n\n**Characteristics:**\n- Execute on separate machines (cloud sandboxes)\n- Guarantee complete isolation from LM process\n- Use HTTP broker pattern with polling for communication (see [Communication Protocols](#3.5))\n- Higher latency due to network roundtrips\n- Suitable for untrusted code execution and production deployments\n\n**Implementations:**\n- `ModalREPL`: Execution using Modal Sandboxes with encrypted tunnels\n- `PrimeREPL`: Execution using Prime Intellect Sandboxes with port exposure\n- `DaytonaREPL`: Execution using Daytona SDK with preview URLs\n\nSources: [rlm/environments/base_env.py:31-50](), [README.md:95-115](), [AGENTS.md:212-319]()\n\n## Communication Models\n\nThe communication model differs fundamentally between isolated and non-isolated environments.\n\n```mermaid\ngraph TB\n    subgraph NonIsolated[\"Non-Isolated Communication\"]\n        LocalCode[\"Code Execution<br/>(exec in namespace)\"]\n        llm_query_local[\"llm_query(prompt)\"]\n        send_lm_req[\"send_lm_request()\"]\n        socket_conn[\"Direct TCP Socket\"]\n        LMHandler_local[\"LMHandler<br/>(host:port)\"]\n        \n        LocalCode --> llm_query_local\n        llm_query_local --> send_lm_req\n        send_lm_req --> socket_conn\n        socket_conn --> LMHandler_local\n    end\n    \n    subgraph Isolated[\"Isolated Communication\"]\n        SandboxCode[\"Code Execution<br/>(remote sandbox)\"]\n        llm_query_iso[\"llm_query(prompt)\"]\n        broker_enqueue[\"POST /enqueue<br/>(Flask broker)\"]\n        tunnel[\"Encrypted Tunnel<br/>(encrypted_ports/expose)\"]\n        poller[\"Poller Thread<br/>(GET /pending)\"]\n        LMHandler_iso[\"LMHandler<br/>(via socket)\"]\n        \n        SandboxCode --> llm_query_iso\n        llm_query_iso --> broker_enqueue\n        broker_enqueue --> tunnel\n        tunnel --> poller\n        poller --> LMHandler_iso\n        LMHandler_iso --> poller\n        poller --> tunnel\n        tunnel --> broker_enqueue\n    end\n```\n\n**Communication Architecture: Socket vs HTTP Broker**\n\n### Non-Isolated Socket Protocol\n\nNon-isolated environments use a length-prefixed JSON protocol over TCP sockets:\n\n1. Code calls `llm_query(prompt)` during execution\n2. Helper function calls `send_lm_request(address, LMRequest)` from [rlm/core/comms_utils.py]()\n3. Opens TCP connection to `LMHandler` at stored `(host, port)` address\n4. Sends 4-byte big-endian length prefix + UTF-8 JSON payload\n5. Receives `LMResponse` containing `RLMChatCompletion` or error\n\n### Isolated HTTP Broker Pattern\n\nIsolated environments use an HTTP broker server inside the sandbox with polling from the host:\n\n1. Flask broker server runs inside sandbox at `localhost:8080`\n2. Code calls `llm_query(prompt)`, which POSTs to `http://localhost:8080/enqueue`\n3. Broker queues request and blocks waiting for response\n4. Host poller thread polls `{tunnel_url}/pending` every 100ms\n5. Host forwards requests to `LMHandler` via socket, gets `LMResponse`\n6. Host POSTs response to `{tunnel_url}/respond`\n7. Broker unblocks original `/enqueue` call with response\n\n| Endpoint | Method | Purpose |\n|----------|--------|---------|\n| `/enqueue` | POST | Submit LLM request from sandbox code (blocks until response) |\n| `/pending` | GET | Get list of pending requests (called by host poller) |\n| `/respond` | POST | Submit response for a request ID (called by host poller) |\n| `/health` | GET | Health check endpoint |\n\nSources: [rlm/environments/base_env.py:31-73](), [AGENTS.md:212-319](), [examples/lm_in_prime_repl.py:1-77]()\n\n## SupportsPersistence Protocol\n\nThe `SupportsPersistence` protocol in [rlm/environments/base_env.py:76-182]() defines a runtime-checkable interface for environments that support persistent multi-turn sessions with versioned context and history management.\n\n### Protocol Methods\n\n| Method | Signature | Purpose |\n|--------|-----------|---------|\n| `update_handler_address` | `(tuple[str, int]) -> None` | Update LM handler address between completions |\n| `add_context` | `(dict\\|list\\|str, int\\|None) -> int` | Add context payload, returns index used |\n| `get_context_count` | `() -> int` | Return number of contexts added |\n| `add_history` | `(list[dict[str, Any]], int\\|None) -> int` | Add message history, returns index used |\n| `get_history_count` | `() -> int` | Return number of histories added |\n\n### Versioning and Aliasing\n\nThe protocol implements a versioning system for contexts and histories:\n\n**Versioning Behavior:**\n- Contexts are named `context_0`, `context_1`, `context_2`, ... in order of addition\n- Histories are named `history_0`, `history_1`, `history_2`, ... in order of addition\n- Auto-increment when `context_index=None` or `history_index=None`\n- Explicit indexing when providing specific index values\n\n**Aliasing Behavior:**\n- Unversioned name `context` always points to `context_0` (first context)\n- Unversioned name `history` always points to `history_0` (first history)\n\n```mermaid\ngraph LR\n    add_ctx0[\"add_context(data1)\"]\n    add_ctx1[\"add_context(data2)\"]\n    add_hist0[\"add_history(msgs1)\"]\n    \n    ctx0_var[\"context_0<br/>context (alias)\"]\n    ctx1_var[\"context_1\"]\n    hist0_var[\"history_0<br/>history (alias)\"]\n    \n    add_ctx0 --> ctx0_var\n    add_ctx1 --> ctx1_var\n    add_hist0 --> hist0_var\n    \n    exec_code[\"execute_code()\"] -.accesses.-> ctx0_var\n    exec_code -.accesses.-> ctx1_var\n    exec_code -.accesses.-> hist0_var\n```\n\n**Context and History Versioning Model**\n\n### Checking Persistence Support\n\nEnvironments can be checked for persistence support at runtime using `isinstance()`:\n\n```python\nfrom rlm.environments.base_env import SupportsPersistence\n\nif isinstance(env, SupportsPersistence):\n    env.add_context(context_payload)\n    env.add_history(message_history)\n```\n\n### Reference Implementation\n\nThe `LocalREPL` environment implements `SupportsPersistence`. See [LocalREPL](#5.2) for implementation details and [rlm/environments/local_repl.py]() for the complete reference implementation.\n\nSources: [rlm/environments/base_env.py:76-182]()\n\n## Environment Lifecycle\n\nEnvironments follow a standard lifecycle managed by the `RLM` class:\n\n```mermaid\ngraph TD\n    create[\"Create Environment<br/>get_environment(type, kwargs)\"]\n    setup[\"setup()<br/>(initialize namespace)\"]\n    load[\"load_context(payload)<br/>(make context available)\"]\n    iter_start[\"Iteration Start\"]\n    exec[\"execute_code(code)<br/>(run code block)\"]\n    result[\"Return REPLResult\"]\n    check_done[\"Check final_answer<br/>or max_iterations\"]\n    cleanup[\"cleanup()<br/>(release resources)\"]\n    \n    create --> setup\n    setup --> load\n    load --> iter_start\n    iter_start --> exec\n    exec --> result\n    result --> check_done\n    check_done -->|continue| iter_start\n    check_done -->|done| cleanup\n```\n\n**Environment Execution Lifecycle**\n\n### Lifecycle Phases\n\n1. **Creation**: Environment instance created via `get_environment()` factory function\n2. **Setup**: `setup()` initializes execution namespace with globals (`llm_query`, `FINAL_VAR`, etc.)\n3. **Context Loading**: `load_context()` makes context payload available as `context` variable\n4. **Iteration Loop**: Multiple calls to `execute_code()` for each code block\n5. **Cleanup**: `cleanup()` releases resources (sockets, containers, cloud sandboxes)\n\n### State Management\n\nEnvironments must provide these globals to executed code:\n\n| Global | Type | Purpose |\n|--------|------|---------|\n| `context` | `dict \\| list \\| str` | The loaded context payload (alias to `context_0` if persistent) |\n| `llm_query` | `Callable` | Function for single sub-LM calls |\n| `llm_query_batched` | `Callable` | Function for batched sub-LM calls |\n| `FINAL_VAR` | `Callable` | Function for returning final answers |\n\nFor persistent environments with `SupportsPersistence`:\n- `context_0`, `context_1`, ... (versioned contexts)\n- `history_0`, `history_1`, ... (versioned histories)\n\nState persistence is handled via `dill` serialization to `/tmp/rlm_state.dill` for environments that support it.\n\nSources: [rlm/environments/base_env.py:1-183](), [AGENTS.md:163-201](), [rlm/environments/constants.py:1-33]()\n\n## Available Environment Implementations\n\nThe following table summarizes the available environment implementations:\n\n| Environment | Base Class | Isolation Level | Communication | Persistence Support | Use Case |\n|------------|------------|-----------------|---------------|---------------------|----------|\n| `LocalREPL` | `NonIsolatedEnv` | Process-level (`_SAFE_BUILTINS`) | Direct socket | ✓ | Development, trusted code |\n| `DockerREPL` | `NonIsolatedEnv` | Container-level | Direct socket via `host.docker.internal` | ✗ | Containerized execution |\n| `ModalREPL` | `IsolatedEnv` | Cloud sandbox | HTTP broker + encrypted tunnels | ✗ | Production, untrusted code |\n| `PrimeREPL` | `IsolatedEnv` | Cloud sandbox | HTTP broker + port exposure | ✗ | Production, untrusted code |\n| `DaytonaREPL` | `IsolatedEnv` | Cloud sandbox | HTTP broker + preview URLs | ✗ | Production, untrusted code |\n\n### Factory Function\n\nEnvironments are instantiated via the `get_environment()` factory function from [rlm/environments/__init__.py]():\n\n```python\nfrom rlm.environments import get_environment\n\nenv = get_environment(\n    environment_type=\"local\",  # or \"docker\", \"modal\", \"prime\", \"daytona\"\n    lm_handler_address=(host, port),\n    context_payload=data,\n    **environment_kwargs\n)\n```\n\nThe factory function maps environment type strings to implementation classes and handles instantiation with appropriate parameters.\n\nSources: [README.md:79-115](), [tests/test_imports.py:140-190](), [examples/prime_repl_example.py:1-31]()\n\n---\n\n# Page: LocalREPL\n\n# LocalREPL\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [Makefile](Makefile)\n- [README.md](README.md)\n- [rlm/environments/local_repl.py](rlm/environments/local_repl.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis page documents the `LocalREPL` class, a non-isolated Python execution environment that runs code directly in the host process with sandboxing. `LocalREPL` maintains a persistent Python namespace across multiple code executions, provides helper functions for recursive LLM calls, and captures output for inspection.\n\nFor an overview of all execution environments and when to choose each type, see [Environment Overview](#5.1). For isolated execution in containers, see [DockerREPL](#5.3). For cloud-based sandboxed execution, see [Cloud Environments](#5.4).\n\n**Sources:** [rlm/environments/local_repl.py:1-311]()\n\n---\n\n## Overview\n\n`LocalREPL` is the default execution environment for RLM. It provides a lightweight REPL (Read-Eval-Print Loop) that executes Python code in a sandboxed namespace within the same process as the RLM orchestrator. Unlike isolated environments, `LocalREPL` offers minimal overhead but limited security isolation.\n\n### Key Characteristics\n\n| Characteristic | Description |\n|----------------|-------------|\n| **Isolation Level** | Non-isolated (same process) |\n| **Overhead** | Minimal (no container/network overhead) |\n| **State Persistence** | Maintains `globals` and `locals` across executions |\n| **Sandboxing** | Restricted builtins, blocked dangerous operations |\n| **LLM Communication** | Direct socket connection to `LMHandler` |\n| **Use Case** | Rapid prototyping, trusted code, local development |\n\n**Sources:** [rlm/environments/local_repl.py:114-144]()\n\n---\n\n## Class Structure and Inheritance\n\n```mermaid\nclassDiagram\n    class BaseEnv {\n        <<abstract>>\n        +execute_code(code: str) REPLResult\n        +cleanup()\n    }\n    \n    class NonIsolatedEnv {\n        <<abstract>>\n        +environment_type: EnvironmentType\n    }\n    \n    class LocalREPL {\n        -lm_handler_address: tuple[str, int]\n        -original_cwd: str\n        -temp_dir: str\n        -_lock: threading.Lock\n        -globals: dict[str, Any]\n        -locals: dict[str, Any]\n        -_pending_llm_calls: list[RLMChatCompletion]\n        +__init__(lm_handler_address, context_payload, setup_code, **kwargs)\n        +setup()\n        +load_context(context_payload)\n        +execute_code(code: str) REPLResult\n        +cleanup()\n        -_final_var(variable_name: str) str\n        -_llm_query(prompt: str, model: str) str\n        -_llm_query_batched(prompts: list[str], model: str) list[str]\n        -_capture_output() ContextManager\n        -_temp_cwd() ContextManager\n    }\n    \n    BaseEnv <|-- NonIsolatedEnv\n    NonIsolatedEnv <|-- LocalREPL\n```\n\n**Sources:** [rlm/environments/local_repl.py:114-118](), [rlm/environments/base_env.py]()\n\n---\n\n## Initialization and Configuration\n\n### Constructor Parameters\n\nThe `LocalREPL.__init__` method accepts the following parameters:\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| `lm_handler_address` | `tuple[str, int] \\| None` | Socket address of `LMHandler` server for recursive LLM calls |\n| `context_payload` | `dict \\| list \\| str \\| None` | Initial context data to load into the environment |\n| `setup_code` | `str \\| None` | Python code to execute during initialization |\n| `**kwargs` | | Additional arguments passed to `NonIsolatedEnv` |\n\n### Initialization Flow\n\n```mermaid\nflowchart TD\n    A[\"LocalREPL.__init__\"] --> B[\"super().__init__(**kwargs)\"]\n    B --> C[\"Store lm_handler_address\"]\n    C --> D[\"Get original_cwd\"]\n    D --> E[\"Create temp_dir with UUID\"]\n    E --> F[\"Initialize threading._lock\"]\n    F --> G[\"setup()\"]\n    G --> H{\"context_payload?\"}\n    H -->|Yes| I[\"load_context()\"]\n    H -->|No| J{\"setup_code?\"}\n    I --> J\n    J -->|Yes| K[\"execute_code(setup_code)\"]\n    J -->|No| L[\"Initialization Complete\"]\n    K --> L\n```\n\n**Sources:** [rlm/environments/local_repl.py:120-143]()\n\n---\n\n## Sandboxing Mechanism\n\n`LocalREPL` implements sandboxing through a restricted set of builtin functions defined in `_SAFE_BUILTINS`.\n\n### Blocked Operations\n\nThe following dangerous operations are explicitly blocked (set to `None`):\n\n- `input` - Prevents interactive input\n- `eval` - Prevents arbitrary code evaluation\n- `exec` - Prevents arbitrary code execution\n- `compile` - Prevents code object creation\n- `globals` - Prevents access to global namespace\n- `locals` - Prevents access to local namespace\n\n### Namespace Structure\n\n```mermaid\ngraph TB\n    subgraph \"LocalREPL Namespace\"\n        globals[\"globals dict\"]\n        locals[\"locals dict\"]\n        builtins[\"__builtins__: _SAFE_BUILTINS\"]\n        \n        subgraph \"Safe Builtins (22 lines)\"\n            core[\"Core: print, len, str, int, float, list, dict, set, tuple, bool\"]\n            func[\"Functions: enumerate, zip, map, filter, sorted, range, min, max, sum\"]\n            exc[\"Exceptions: Exception, ValueError, TypeError, KeyError, etc.\"]\n            blocked[\"Blocked: input=None, eval=None, exec=None, compile=None\"]\n        end\n        \n        subgraph \"Helper Functions\"\n            final[\"FINAL_VAR(variable_name)\"]\n            query[\"llm_query(prompt, model)\"]\n            batch[\"llm_query_batched(prompts, model)\"]\n        end\n        \n        globals --> builtins\n        globals --> final\n        globals --> query\n        globals --> batch\n        \n        builtins --> core\n        builtins --> func\n        builtins --> exc\n        builtins --> blocked\n    end\n```\n\n**Sources:** [rlm/environments/local_repl.py:22-111](), [rlm/environments/local_repl.py:145-160]()\n\n---\n\n## State Persistence\n\n`LocalREPL` maintains persistent state across multiple `execute_code` calls using two dictionaries:\n\n### Globals Dictionary\n\nInitialized in `setup()` and contains:\n- `__builtins__`: Reference to `_SAFE_BUILTINS`\n- `__name__`: Set to `\"__main__\"`\n- Helper functions: `FINAL_VAR`, `llm_query`, `llm_query_batched`\n\n### Locals Dictionary\n\nTracks user-defined variables created during code execution. Variables are persisted using this logic:\n\n```\ncombined = {**self.globals, **self.locals}\nexec(code, combined, combined)\n\n# After execution, update locals\nfor key, value in combined.items():\n    if key not in self.globals and not key.startswith(\"_\"):\n        self.locals[key] = value\n```\n\nThis ensures that new variables (except those starting with `_`) are stored in `self.locals` and available in subsequent executions.\n\n**Sources:** [rlm/environments/local_repl.py:145-152](), [rlm/environments/local_repl.py:271-277]()\n\n---\n\n## Built-in Helper Functions\n\n### FINAL_VAR\n\n```python\ndef _final_var(variable_name: str) -> str\n```\n\nReturns the string representation of a variable from the locals namespace. This is used to return final answers from the REPL.\n\n**Example Usage:**\n```python\n# In executed code\nresult = 42\nFINAL_VAR(\"result\")  # Returns \"42\"\n```\n\n**Sources:** [rlm/environments/local_repl.py:162-167]()\n\n### llm_query\n\n```python\ndef _llm_query(prompt: str, model: str | None = None) -> str\n```\n\nMakes a recursive LLM call through the socket connection to `LMHandler`. Returns the LLM's response as a string.\n\n**Communication Flow:**\n\n```mermaid\nsequenceDiagram\n    participant Code as \"Executed Code\"\n    participant LocalREPL\n    participant Socket as \"Socket Connection\"\n    participant LMHandler\n    participant Client as \"LLM Client\"\n    \n    Code->>LocalREPL: \"llm_query(prompt, model)\"\n    LocalREPL->>LocalREPL: \"Create LMRequest(prompt, model)\"\n    LocalREPL->>Socket: \"send_lm_request(address, request)\"\n    Socket->>LMHandler: \"JSON payload\"\n    LMHandler->>Client: \"Forward to appropriate client\"\n    Client-->>LMHandler: \"RLMChatCompletion\"\n    LMHandler-->>Socket: \"LMResponse\"\n    Socket-->>LocalREPL: \"LMResponse\"\n    LocalREPL->>LocalREPL: \"Append to _pending_llm_calls\"\n    LocalREPL-->>Code: \"response string\"\n```\n\n**Sources:** [rlm/environments/local_repl.py:169-193]()\n\n### llm_query_batched\n\n```python\ndef _llm_query_batched(prompts: list[str], model: str | None = None) -> list[str]\n```\n\nMakes multiple concurrent LLM calls and returns a list of responses in the same order as the input prompts. Uses `send_lm_request_batched` to process requests concurrently.\n\n**Sources:** [rlm/environments/local_repl.py:195-222]()\n\n---\n\n## Context Loading\n\nThe `load_context` method provides initial data to the execution environment. It supports three context types:\n\n| Context Type | Storage | Loading Method |\n|--------------|---------|----------------|\n| `str` | `context.txt` | Read as text file, assign to `context` variable |\n| `dict` | `context.json` | Read as JSON, assign to `context` variable |\n| `list` | `context.json` | Read as JSON, assign to `context` variable |\n\n### Implementation\n\n```mermaid\nflowchart TD\n    A[\"load_context(context_payload)\"] --> B{\"Type?\"}\n    B -->|str| C[\"Write to context.txt\"]\n    B -->|dict/list| D[\"Write to context.json\"]\n    C --> E[\"Execute: with open('context.txt') as f: context = f.read()\"]\n    D --> F[\"Execute: import json; with open('context.json') as f: context = json.load(f)\"]\n    E --> G[\"context variable available in namespace\"]\n    F --> G\n```\n\n**Sources:** [rlm/environments/local_repl.py:224-237]()\n\n---\n\n## Code Execution\n\n### execute_code Method\n\n```python\ndef execute_code(self, code: str) -> REPLResult\n```\n\nThe core method that executes Python code and returns a structured result.\n\n### Execution Pipeline\n\n```mermaid\nflowchart TD\n    A[\"execute_code(code)\"] --> B[\"start_time = time.perf_counter()\"]\n    B --> C[\"Clear _pending_llm_calls\"]\n    C --> D[\"_capture_output() context\"]\n    D --> E[\"_temp_cwd() context\"]\n    E --> F[\"combined = {**globals, **locals}\"]\n    F --> G[\"exec(code, combined, combined)\"]\n    G --> H{\"Exception?\"}\n    H -->|No| I[\"Update self.locals with new vars\"]\n    H -->|Yes| J[\"Append exception to stderr\"]\n    I --> K[\"Create REPLResult\"]\n    J --> K\n    K --> L[\"Return REPLResult with stdout, stderr, locals, time, rlm_calls\"]\n```\n\n### REPLResult Structure\n\nThe returned `REPLResult` contains:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `stdout` | `str` | Captured standard output |\n| `stderr` | `str` | Captured standard error (includes exceptions) |\n| `locals` | `dict[str, Any]` | Copy of current locals namespace |\n| `execution_time` | `float` | Time in seconds |\n| `rlm_calls` | `list[RLMChatCompletion]` | Recursive LLM calls made during execution |\n\n**Sources:** [rlm/environments/local_repl.py:261-291]()\n\n---\n\n## Output Capture\n\n### _capture_output Context Manager\n\nThe `_capture_output` method provides thread-safe redirection of `sys.stdout` and `sys.stderr`:\n\n```python\n@contextmanager\ndef _capture_output(self):\n    with self._lock:\n        old_stdout, old_stderr = sys.stdout, sys.stderr\n        stdout_buf, stderr_buf = io.StringIO(), io.StringIO()\n        try:\n            sys.stdout, sys.stderr = stdout_buf, stderr_buf\n            yield stdout_buf, stderr_buf\n        finally:\n            sys.stdout, sys.stderr = old_stdout, old_stderr\n```\n\n**Key Features:**\n- Thread-safe using `threading.Lock`\n- Uses `io.StringIO` for in-memory buffering\n- Guarantees restoration of original streams via `finally` block\n\n**Sources:** [rlm/environments/local_repl.py:239-249]()\n\n---\n\n## Temporary Directory and File Operations\n\n### Temporary Directory Management\n\nEach `LocalREPL` instance creates a unique temporary directory:\n\n```python\nself.temp_dir = tempfile.mkdtemp(prefix=f\"repl_env_{uuid.uuid4()}_\")\n```\n\nThis directory is used for:\n- Storing context files (`context.txt`, `context.json`)\n- Any file operations performed by executed code\n\n### _temp_cwd Context Manager\n\nCode execution temporarily changes the working directory to `temp_dir`:\n\n```python\n@contextmanager\ndef _temp_cwd(self):\n    old_cwd = os.getcwd()\n    try:\n        os.chdir(self.temp_dir)\n        yield\n    finally:\n        os.chdir(old_cwd)\n```\n\nThis ensures that:\n- File operations in executed code are isolated to the temporary directory\n- The original working directory is always restored\n\n**Sources:** [rlm/environments/local_repl.py:131](), [rlm/environments/local_repl.py:251-259]()\n\n---\n\n## Thread Safety\n\n`LocalREPL` uses a `threading.Lock` to ensure thread-safe output capture:\n\n```mermaid\ngraph TB\n    subgraph \"Thread Safety Mechanism\"\n        lock[\"threading.Lock (_lock)\"]\n        capture[\"_capture_output()\"]\n        \n        lock --> capture\n        capture --> redirect[\"Redirect sys.stdout/stderr\"]\n        capture --> restore[\"Restore original streams\"]\n        \n        note1[\"Lock acquired at start of _capture_output\"]\n        note2[\"Lock released when context exits\"]\n        \n        lock -.-> note1\n        capture -.-> note2\n    end\n```\n\n**Critical Section:** The entire duration of output capture (from stream redirection to restoration) is protected by the lock, preventing race conditions when multiple threads attempt to execute code concurrently.\n\n**Sources:** [rlm/environments/local_repl.py:132](), [rlm/environments/local_repl.py:239-249]()\n\n---\n\n## Cleanup and Resource Management\n\n### Cleanup Process\n\n```mermaid\nflowchart TD\n    A[\"cleanup()\"] --> B[\"shutil.rmtree(temp_dir)\"]\n    B --> C{\"Success?\"}\n    C -->|Yes| D[\"Clear globals dict\"]\n    C -->|No| D\n    D --> E[\"Clear locals dict\"]\n    E --> F[\"Cleanup Complete\"]\n```\n\n### Context Manager Protocol\n\n`LocalREPL` implements the context manager protocol for automatic resource cleanup:\n\n```python\ndef __enter__(self):\n    return self\n\ndef __exit__(self, exc_type, exc_val, exc_tb):\n    self.cleanup()\n    return False\n```\n\n**Usage Example:**\n```python\nwith LocalREPL(lm_handler_address=address) as repl:\n    result = repl.execute_code(\"x = 42\")\n# Cleanup automatically called here\n```\n\n### Destructor\n\nThe `__del__` method ensures cleanup even if the context manager is not used:\n\n```python\ndef __del__(self):\n    self.cleanup()\n```\n\n**Note:** Exception handling in `cleanup()` ensures that temporary directory deletion failures do not crash the program.\n\n**Sources:** [rlm/environments/local_repl.py:293-310]()\n\n---\n\n## Usage Patterns\n\n### Basic Execution\n\n```python\nfrom rlm.environments.local_repl import LocalREPL\n\n# Initialize without LM handler (no recursive calls)\nrepl = LocalREPL()\n\n# Execute code\nresult = repl.execute_code(\"\"\"\nx = 10\ny = 20\nprint(f\"Sum: {x + y}\")\n\"\"\")\n\nprint(result.stdout)  # \"Sum: 30\\n\"\nprint(result.locals)  # {\"x\": 10, \"y\": 20}\n```\n\n### With Context Loading\n\n```python\n# Load context as dictionary\nrepl = LocalREPL(context_payload={\"data\": [1, 2, 3, 4, 5]})\n\nresult = repl.execute_code(\"\"\"\ntotal = sum(context['data'])\nprint(f\"Total: {total}\")\n\"\"\")\n```\n\n### With Recursive LLM Calls\n\n```python\n# Initialize with LM handler address\nrepl = LocalREPL(lm_handler_address=(\"localhost\", 8080))\n\nresult = repl.execute_code(\"\"\"\nresponse = llm_query(\"What is 2+2?\")\nprint(f\"LLM says: {response}\")\n\"\"\")\n\n# Check tracked LLM calls\nprint(len(result.rlm_calls))  # 1\n```\n\n**Sources:** [rlm/environments/local_repl.py:114-291]()\n\n---\n\n# Page: DockerREPL\n\n# DockerREPL\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [Makefile](Makefile)\n- [README.md](README.md)\n- [rlm/environments/docker_repl.py](rlm/environments/docker_repl.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThe `DockerREPL` environment provides containerized Python code execution for RLM using Docker. It runs code in an isolated Docker container while maintaining the ability to make LLM queries back to the host process through an HTTP proxy server. This environment offers a middle ground between the fully local `LocalREPL` and cloud-based isolated environments, providing process-level isolation without requiring cloud infrastructure.\n\nFor local host-process execution, see [LocalREPL](#5.2). For fully isolated cloud-based execution, see [Cloud Environments](#5.4). For an overview of the environment architecture, see [Environment Overview](#5.1).\n\n**Sources:** [rlm/environments/docker_repl.py:1-315](), [README.md:82-84]()\n\n---\n\n## Architecture Overview\n\n### Component Structure\n\n```mermaid\ngraph TB\n    subgraph \"Host Process\"\n        RLM[\"RLM Core\"]\n        LMHandler[\"LMHandler\"]\n        ProxyServer[\"LLMProxyHandler<br/>(HTTP Server)\"]\n    end\n    \n    subgraph \"Docker Container\"\n        ExecScript[\"Execution Script<br/>(Python -c)\"]\n        StateFile[\"state.dill<br/>(Persistent State)\"]\n        WorkspaceVol[\"/workspace Volume\"]\n    end\n    \n    RLM -->|\"execute_code()\"| DockerREPL[\"DockerREPL\"]\n    DockerREPL -->|\"docker exec\"| ExecScript\n    \n    ExecScript -->|\"HTTP POST\"| ProxyServer\n    ProxyServer -->|\"send_lm_request()\"| LMHandler\n    LMHandler -->|\"RLMChatCompletion\"| ProxyServer\n    ProxyServer -->|\"JSON response\"| ExecScript\n    \n    ExecScript -->|\"dill.dump()\"| StateFile\n    StateFile -->|\"dill.load()\"| ExecScript\n    StateFile -.->|\"mounted at\"| WorkspaceVol\n    \n    DockerREPL -.->|\"manages\"| Container[\"Docker Container\"]\n    DockerREPL -.->|\"manages\"| ProxyServer\n```\n\n**Sources:** [rlm/environments/docker_repl.py:170-315]()\n\nThe `DockerREPL` class manages three primary components:\n\n| Component | Purpose | Implementation |\n|-----------|---------|----------------|\n| **Docker Container** | Isolated Python execution environment | Spawned via `docker run`, uses `python:3.11-slim` by default |\n| **HTTP Proxy Server** | Routes LLM queries from container to host | `LLMProxyHandler` class running on `127.0.0.1:random_port` |\n| **Temporary Workspace** | Persistent state storage between executions | Mounted as `/workspace` volume in container |\n\n**Sources:** [rlm/environments/docker_repl.py:177-203]()\n\n---\n\n## HTTP Proxy System\n\n### Proxy Handler Implementation\n\nThe `LLMProxyHandler` class extends `BaseHTTPRequestHandler` to provide two endpoints for LLM queries:\n\n```mermaid\nsequenceDiagram\n    participant Container as \"Container<br/>(exec script)\"\n    participant Proxy as \"LLMProxyHandler\"\n    participant Handler as \"LMHandler\"\n    \n    Container->>Proxy: POST /llm_query<br/>{prompt, model}\n    Proxy->>Handler: send_lm_request()\n    Handler-->>Proxy: LMResponse\n    Proxy->>Proxy: store in pending_calls\n    Proxy-->>Container: {response: text}\n    \n    Container->>Proxy: POST /llm_query_batched<br/>{prompts, model}\n    Proxy->>Handler: send_lm_request_batched()\n    Handler-->>Proxy: List[LMResponse]\n    Proxy->>Proxy: store completions\n    Proxy-->>Container: {responses: [text]}\n```\n\n**Sources:** [rlm/environments/docker_repl.py:25-88]()\n\n### Endpoints\n\n| Endpoint | Method | Request Body | Response |\n|----------|--------|--------------|----------|\n| `/llm_query` | POST | `{prompt: str, model: str?}` | `{response: str}` or `{error: str}` |\n| `/llm_query_batched` | POST | `{prompts: List[str], model: str?}` | `{responses: List[str]}` |\n\nThe handler maintains state in class attributes:\n\n- `lm_handler_address`: Socket address of the `LMHandler` ([rlm/environments/docker_repl.py:28]())\n- `pending_calls`: List of `RLMChatCompletion` objects for result tracking ([rlm/environments/docker_repl.py:29]())\n- `lock`: Thread lock for synchronizing access to `pending_calls` ([rlm/environments/docker_repl.py:30]())\n\n**Sources:** [rlm/environments/docker_repl.py:25-88]()\n\n---\n\n## Container Lifecycle\n\n### Initialization Sequence\n\n```mermaid\ngraph LR\n    Init[\"__init__()\"] --> StartProxy[\"Start HTTP<br/>proxy server\"]\n    StartProxy --> GetPort[\"Get random<br/>port number\"]\n    GetPort --> StartContainer[\"docker run -d\"]\n    StartContainer --> InstallDeps[\"docker exec<br/>pip install dill requests\"]\n    InstallDeps --> LoadContext[\"load_context()<br/>(if provided)\"]\n    LoadContext --> RunSetup[\"execute_code()<br/>(if setup_code)\"]\n    RunSetup --> Ready[\"Ready\"]\n```\n\n**Sources:** [rlm/environments/docker_repl.py:204-203]()\n\nThe `setup()` method performs initialization in the following order:\n\n1. **Proxy Server Setup** ([rlm/environments/docker_repl.py:207-219]())\n   - Creates dynamic `Handler` class with shared state\n   - Binds to `127.0.0.1:0` (random port)\n   - Starts server thread in daemon mode\n\n2. **Container Launch** ([rlm/environments/docker_repl.py:222-243]())\n   - Runs container with volume mount: `-v {temp_dir}:/workspace`\n   - Adds host resolution: `--add-host host.docker.internal:host-gateway`\n   - Keeps container alive with `tail -f /dev/null`\n\n3. **Dependency Installation** ([rlm/environments/docker_repl.py:246-249]())\n   - Installs `dill` for state serialization\n   - Installs `requests` for proxy communication\n\n**Sources:** [rlm/environments/docker_repl.py:204-249]()\n\n### Cleanup Process\n\n```mermaid\ngraph LR\n    Cleanup[\"cleanup()\"] --> StopContainer[\"docker stop<br/>container_id\"]\n    StopContainer --> ShutdownProxy[\"proxy_server.<br/>shutdown()\"]\n    ShutdownProxy --> RemoveTemp[\"shutil.rmtree()<br/>temp_dir\"]\n    RemoveTemp --> Done[\"Cleanup Complete\"]\n```\n\n**Sources:** [rlm/environments/docker_repl.py:294-304]()\n\nThe `cleanup()` method is called via:\n- Context manager exit (`__exit__`)\n- Object destruction (`__del__`)\n- Explicit user call\n\n**Sources:** [rlm/environments/docker_repl.py:306-314]()\n\n---\n\n## Code Execution Flow\n\n### Execution Script Generation\n\nThe `_build_exec_script()` function creates a complete Python script that:\n\n```mermaid\ngraph TB\n    Build[\"_build_exec_script()\"] --> Encode[\"Base64 encode<br/>user code\"]\n    Encode --> Inject[\"Inject into<br/>template\"]\n    \n    subgraph \"Generated Script Components\"\n        LoadState[\"load_state()<br/>from state.dill\"]\n        DefineHelpers[\"Define llm_query()<br/>llm_query_batched()<br/>FINAL_VAR()\"]\n        ExecCode[\"exec(decoded_code)\"]\n        SaveState[\"save_state()<br/>to state.dill\"]\n        PrintJSON[\"print(json.dumps(result))\"]\n    end\n    \n    Inject --> LoadState\n    LoadState --> DefineHelpers\n    DefineHelpers --> ExecCode\n    ExecCode --> SaveState\n    SaveState --> PrintJSON\n```\n\n**Sources:** [rlm/environments/docker_repl.py:90-168]()\n\n### Helper Functions Injected into Container\n\n| Function | Purpose | Implementation |\n|----------|---------|----------------|\n| `llm_query(prompt, model=None)` | Single LLM query via proxy | POST to `{PROXY}/llm_query` ([rlm/environments/docker_repl.py:105-111]()) |\n| `llm_query_batched(prompts, model=None)` | Batched LLM queries | POST to `{PROXY}/llm_query_batched` ([rlm/environments/docker_repl.py:113-119]()) |\n| `load_state()` | Load persisted locals from dill | Reads `/workspace/state.dill` ([rlm/environments/docker_repl.py:121-128]()) |\n| `save_state(s)` | Persist locals to dill | Filters and serializes to `/workspace/state.dill` ([rlm/environments/docker_repl.py:130-138]()) |\n| `FINAL_VAR(name)` | Extract variable for final answer | Returns `str(_locals.get(name))` ([rlm/environments/docker_repl.py:142-144]()) |\n\n**Sources:** [rlm/environments/docker_repl.py:90-168]()\n\n### Execution Steps\n\n```mermaid\nsequenceDiagram\n    participant DockerREPL\n    participant Script as \"Exec Script\"\n    participant Dill as \"state.dill\"\n    participant Proxy as \"LLMProxyHandler\"\n    \n    DockerREPL->>DockerREPL: clear pending_calls\n    DockerREPL->>DockerREPL: _build_exec_script(code)\n    DockerREPL->>Script: docker exec python -c\n    \n    Script->>Dill: load_state()\n    Dill-->>Script: _locals dict\n    \n    Script->>Script: exec(user_code)\n    \n    opt llm_query called\n        Script->>Proxy: POST /llm_query\n        Proxy-->>Script: response\n    end\n    \n    Script->>Dill: save_state(_locals)\n    Script->>DockerREPL: print(json.dumps(result))\n    \n    DockerREPL->>DockerREPL: copy pending_calls\n    DockerREPL->>DockerREPL: return REPLResult\n```\n\n**Sources:** [rlm/environments/docker_repl.py:258-292]()\n\nThe `execute_code()` method returns a `REPLResult` with:\n\n- `stdout`: Captured standard output\n- `stderr`: Captured standard error plus container stderr\n- `locals`: Dictionary of variable names to their `repr()` strings\n- `execution_time`: Elapsed time in seconds\n- `rlm_calls`: List of `RLMChatCompletion` objects from nested LLM queries\n\n**Sources:** [rlm/environments/docker_repl.py:278-284]()\n\n---\n\n## State Persistence\n\n### State Serialization Strategy\n\n```mermaid\ngraph TB\n    ExecStart[\"Execution Start\"] --> LoadDill[\"load_state()\"]\n    \n    LoadDill --> CheckFile{\"/workspace/state.dill<br/>exists?\"}\n    CheckFile -->|Yes| Deserialize[\"dill.load()\"]\n    CheckFile -->|No| EmptyDict[\"Return {}\"]\n    \n    Deserialize --> LoadedLocals[\"_locals\"]\n    EmptyDict --> LoadedLocals\n    \n    LoadedLocals --> ExecCode[\"exec(user_code,<br/>globals + locals)\"]\n    ExecCode --> UpdateLocals[\"Update _locals\"]\n    \n    UpdateLocals --> FilterVars[\"Filter: exclude<br/>_ prefixed vars\"]\n    FilterVars --> TestSerialize[\"Test dill.dumps()<br/>for each var\"]\n    TestSerialize --> SaveDill[\"dill.dump(clean)\"]\n    \n    SaveDill --> ExecEnd[\"Execution End\"]\n```\n\n**Sources:** [rlm/environments/docker_repl.py:121-138]()\n\n### State Filtering Rules\n\nThe `save_state()` function applies filtering to prevent serialization errors:\n\n1. **Exclude private variables**: Remove keys starting with `_` ([rlm/environments/docker_repl.py:131]())\n2. **Test serializability**: Attempt `dill.dumps()` on each value ([rlm/environments/docker_repl.py:133-136]())\n3. **Remove non-serializable**: Delete variables that fail serialization ([rlm/environments/docker_repl.py:137]())\n\nThis ensures state persistence across `execute_code()` calls while avoiding common serialization issues with functions, modules, or other complex objects.\n\n**Sources:** [rlm/environments/docker_repl.py:130-138]()\n\n---\n\n## Configuration\n\n### Constructor Parameters\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `image` | `str` | `\"python:3.11-slim\"` | Docker image for container |\n| `lm_handler_address` | `tuple[str, int] \\| None` | `None` | LMHandler socket address for LLM queries |\n| `context_payload` | `dict \\| list \\| str \\| None` | `None` | Initial context data, loaded as `context` variable |\n| `setup_code` | `str \\| None` | `None` | Python code executed during initialization |\n\n**Sources:** [rlm/environments/docker_repl.py:177-203]()\n\n### Docker Image Requirements\n\nThe Docker image must provide:\n- Python 3.11 or higher\n- Ability to install `dill` and `requests` via pip\n\nThe default `python:3.11-slim` image satisfies these requirements. Custom images can be specified:\n\n```python\nrepl = DockerREPL(\n    image=\"my-custom-python:latest\",\n    lm_handler_address=handler.address\n)\n```\n\n**Sources:** [rlm/environments/docker_repl.py:179](), [README.md:82-84]()\n\n### Volume Mounting\n\nThe container automatically mounts a temporary directory at `/workspace`:\n\n```\n-v {temp_dir}:/workspace\n```\n\nThis volume persists:\n- `state.dill`: Serialized locals between executions\n- Any files created by user code in `/workspace`\n\nThe temporary directory is created at initialization and removed during cleanup.\n\n**Sources:** [rlm/environments/docker_repl.py:193](), [rlm/environments/docker_repl.py:229]()\n\n### Network Configuration\n\nThe container uses `host.docker.internal` to communicate with the host proxy:\n\n```\n--add-host host.docker.internal:host-gateway\n```\n\nThis allows the container's `requests` calls to reach `http://host.docker.internal:{proxy_port}`.\n\n**Sources:** [rlm/environments/docker_repl.py:230-231](), [rlm/environments/docker_repl.py:102]()\n\n---\n\n## Usage Examples\n\n### Basic Execution Without LLM Queries\n\n```python\nfrom rlm.environments.docker_repl import DockerREPL\n\nwith DockerREPL() as repl:\n    result = repl.execute_code(\"x = 1 + 2\")\n    print(result.locals)  # {'x': '3'}\n    \n    result = repl.execute_code(\"print(x * 2)\")\n    print(result.stdout)  # '6'\n```\n\n**Sources:** [examples/docker_repl_example.py:39-46]()\n\n### Execution With LLM Handler\n\n```python\nfrom rlm.core.lm_handler import LMHandler\nfrom rlm.clients.openai import OpenAIClient\nfrom rlm.environments.docker_repl import DockerREPL\n\nwith LMHandler(client=OpenAIClient(model_name=\"gpt-4\")) as handler:\n    with DockerREPL(lm_handler_address=handler.address) as repl:\n        result = repl.execute_code('answer = llm_query(\"What is 2+2?\")')\n        result = repl.execute_code(\"print(answer)\")\n        print(result.stdout)  # LLM's response\n```\n\n**Sources:** [examples/docker_repl_example.py:48-63]()\n\n### Integration with RLM\n\n```python\nfrom rlm import RLM\n\nrlm = RLM(\n    backend=\"openai\",\n    backend_kwargs={\"model_name\": \"gpt-4\"},\n    environment=\"docker\",\n    environment_kwargs={\n        \"image\": \"python:3.11-slim\",\n        \"setup_code\": \"import numpy as np\"\n    }\n)\n\nresponse = rlm.completion(\"Calculate the mean of [1, 2, 3, 4, 5]\")\n```\n\n**Sources:** [README.md:72-77]()\n\n---\n\n## Implementation Details\n\n### Context Manager Support\n\nThe `DockerREPL` class implements the context manager protocol:\n\n- `__enter__()`: Returns `self` ([rlm/environments/docker_repl.py:306-307]())\n- `__exit__()`: Calls `cleanup()` and returns `False` ([rlm/environments/docker_repl.py:309-311]())\n\nThis ensures proper cleanup even if exceptions occur during execution.\n\n**Sources:** [rlm/environments/docker_repl.py:306-311]()\n\n### Error Handling\n\nError handling occurs at multiple levels:\n\n| Location | Error Type | Handling Strategy |\n|----------|-----------|-------------------|\n| Container startup | Docker command failure | Raise `RuntimeError` with stderr ([rlm/environments/docker_repl.py:240-241]()) |\n| Code execution | JSON parse error | Return `REPLResult` with stdout/stderr ([rlm/environments/docker_repl.py:286-292]()) |\n| LLM query | HTTP request error | Catch exception, return `f\"Error: {e}\"` ([rlm/environments/docker_repl.py:108-111]()) |\n| State serialization | `dill.dumps()` failure | Remove non-serializable variable ([rlm/environments/docker_repl.py:133-136]()) |\n\n**Sources:** [rlm/environments/docker_repl.py:240-241](), [rlm/environments/docker_repl.py:275-292]()\n\n### Threading Considerations\n\nThe proxy server runs in a daemon thread, requiring thread-safe access to `pending_calls`:\n\n```python\nwith self._calls_lock:\n    self.pending_calls.append(response.chat_completion)\n```\n\nThe lock is acquired when:\n- Storing completions in the proxy handler ([rlm/environments/docker_repl.py:64-65]())\n- Clearing pending calls before execution ([rlm/environments/docker_repl.py:261-262]())\n- Copying pending calls after execution ([rlm/environments/docker_repl.py:271-273]())\n\n**Sources:** [rlm/environments/docker_repl.py:195](), [rlm/environments/docker_repl.py:261-273]()\n\n### Class Hierarchy\n\n```mermaid\ngraph BT\n    DockerREPL[\"DockerREPL\"] --> NonIsolatedEnv[\"NonIsolatedEnv\"]\n    NonIsolatedEnv --> BaseEnv[\"BaseEnv\"]\n    \n    LocalREPL[\"LocalREPL\"] --> NonIsolatedEnv\n    \n    ModalREPL[\"ModalREPL\"] --> IsolatedEnv[\"IsolatedEnv\"]\n    IsolatedEnv --> BaseEnv\n```\n\n`DockerREPL` inherits from `NonIsolatedEnv` because it runs on the same machine as the RLM host process, even though it provides container-level isolation.\n\n**Sources:** [rlm/environments/docker_repl.py:170](), [rlm/environments/base_env.py:1-50]()\n\n---\n\n# Page: Cloud Sandbox Environments\n\n# Cloud Environments\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [Makefile](Makefile)\n- [README.md](README.md)\n- [examples/daytona_repl_example.py](examples/daytona_repl_example.py)\n- [examples/quickstart.py](examples/quickstart.py)\n- [rlm/environments/daytona_repl.py](rlm/environments/daytona_repl.py)\n- [rlm/environments/modal_repl.py](rlm/environments/modal_repl.py)\n- [rlm/environments/prime_repl.py](rlm/environments/prime_repl.py)\n\n</details>\n\n\n\nThis page documents cloud-based execution environments that provide fully isolated code execution for RLM completions. These environments run on separate cloud infrastructure, ensuring complete isolation from the host process where the RLM orchestrator runs.\n\nFor information about the environment type hierarchy and the `BaseEnv` interface, see [Environment Overview](#5.1). For local and Docker-based environments, see [LocalREPL](#5.2) and [DockerREPL](#5.3).\n\n---\n\n## Overview\n\nCloud environments inherit from the `IsolatedEnv` abstract base class and execute Python code in cloud-based sandboxes. Unlike `NonIsolatedEnv` implementations (`LocalREPL`, `DockerREPL`) that run on the same machine as the RLM, cloud environments guarantee complete process isolation by executing code on remote infrastructure.\n\nThe RLM system supports two cloud environment types:\n- **Modal Sandboxes** (`ModalREPL`): Production-ready cloud sandboxes via Modal.com\n- **Prime Intellect Sandboxes** (`PrimeREPL`): Beta cloud sandboxes (implementation incomplete)\n\nBoth environments enable recursive LLM calls through the `llm_query()` and `llm_query_batched()` functions, communicating back to the host's `LMHandler` via network requests.\n\n**Sources:** [README.md:69-106](), [rlm/environments/base_env.py:28-47]()\n\n---\n\n## Environment Type Hierarchy\n\n```mermaid\ngraph TB\n    BaseEnv[\"BaseEnv<br/>(abstract)\"]\n    IsolatedEnv[\"IsolatedEnv<br/>(abstract)\"]\n    NonIsolatedEnv[\"NonIsolatedEnv<br/>(abstract)\"]\n    \n    ModalREPL[\"ModalREPL<br/>Modal Sandboxes\"]\n    PrimeREPL[\"PrimeREPL<br/>Prime Sandboxes<br/>(not implemented)\"]\n    LocalREPL[\"LocalREPL\"]\n    DockerREPL[\"DockerREPL\"]\n    \n    BaseEnv --> IsolatedEnv\n    BaseEnv --> NonIsolatedEnv\n    \n    IsolatedEnv --> ModalREPL\n    IsolatedEnv --> PrimeREPL\n    \n    NonIsolatedEnv --> LocalREPL\n    NonIsolatedEnv --> DockerREPL\n    \n    IsolatedEnv -.->|\"guarantee\"| Isolation[\"Complete process<br/>isolation on<br/>separate machine\"]\n    NonIsolatedEnv -.->|\"run on\"| SameMachine[\"Same machine<br/>as RLM host\"]\n```\n\nThe `IsolatedEnv` class defines the contract for cloud-based environments that run on completely separate machines from the LM, providing security guarantees for untrusted code execution.\n\n**Sources:** [rlm/environments/base_env.py:28-47](), [README.md:85-106]()\n\n---\n\n## IsolatedEnv Interface\n\nThe `IsolatedEnv` abstract base class defines three required methods:\n\n| Method | Return Type | Purpose |\n|--------|-------------|---------|\n| `setup()` | `None` | Initialize the cloud sandbox environment |\n| `load_context(context_payload)` | `None` | Load user context into the sandbox's `context` variable |\n| `execute_code(code)` | `REPLResult` | Execute Python code and return results with stdout, stderr, locals, and nested RLM calls |\n\nAll isolated environments must implement these methods to enable code execution in cloud sandboxes while maintaining compatibility with the RLM execution loop.\n\n**Sources:** [rlm/environments/base_env.py:28-47]()\n\n---\n\n## Modal Sandboxes (ModalREPL)\n\nModal Sandboxes provide production-ready, fully isolated Python execution environments through [Modal.com's sandbox infrastructure](https://modal.com/docs/guide/sandboxes). The `ModalREPL` class implements the `IsolatedEnv` interface to execute code on Modal's cloud infrastructure.\n\n### Setup and Authentication\n\nModal requires installation and authentication before use:\n\n```bash\nuv add modal  # Install Modal SDK\nmodal setup   # Authenticate Modal account\n```\n\nThe `modal setup` command configures API credentials for accessing Modal's cloud infrastructure. These credentials are stored locally and used by `ModalREPL` to spawn sandboxes.\n\n### Configuration\n\nTo use Modal Sandboxes with RLM:\n\n```python\nfrom rlm import RLM\n\nrlm = RLM(\n    backend=\"openai\",\n    backend_kwargs={\"model_name\": \"gpt-4\"},\n    environment=\"modal\",\n    environment_kwargs={\n        # Optional Modal-specific configuration\n    }\n)\n\nresult = rlm.completion(\"Execute code in Modal sandbox\")\n```\n\nThe `environment=\"modal\"` parameter routes to `ModalREPL` through the environment factory system documented in [Environment Overview](#5.1).\n\n### Execution Flow\n\n```mermaid\nsequenceDiagram\n    participant RLM as \"RLM Core\"\n    participant Modal as \"ModalREPL\"\n    participant Cloud as \"Modal Cloud<br/>Sandbox\"\n    participant Handler as \"LMHandler<br/>(host process)\"\n    \n    RLM->>Modal: \"execute_code(code)\"\n    activate Modal\n    \n    Modal->>Cloud: \"Spawn sandbox<br/>+ inject code\"\n    activate Cloud\n    \n    opt \"Code calls llm_query()\"\n        Cloud->>Handler: \"HTTP request to<br/>handler address\"\n        activate Handler\n        Handler->>Handler: \"Route to LLM client\"\n        Handler-->>Cloud: \"LLM response\"\n        deactivate Handler\n    end\n    \n    Cloud->>Cloud: \"Capture stdout,<br/>stderr, locals\"\n    Cloud-->>Modal: \"Execution results\"\n    deactivate Cloud\n    \n    Modal-->>RLM: \"REPLResult(stdout,<br/>stderr, locals,<br/>rlm_calls)\"\n    deactivate Modal\n```\n\nModal Sandboxes execute code in ephemeral containers on Modal's infrastructure. When code invokes `llm_query()`, the sandbox makes HTTP requests back to the host's `LMHandler` to enable recursive LLM calls while maintaining isolation.\n\n**Sources:** [README.md:88-93]()\n\n---\n\n## Prime Intellect Sandboxes (PrimeREPL)\n\nPrime Intellect Sandboxes provide cloud-based execution through Prime Intellect's sandbox infrastructure. **Note:** The `PrimeREPL` implementation is currently incomplete and not available for use.\n\n### Setup Requirements\n\nPrime Intellect requires API key configuration:\n\n```bash\nexport PRIME_API_KEY=<your-api-key>\n```\n\nPrime CLI setup instructions are available in the [Prime Intellect documentation](https://docs.primeintellect.ai/inference/overview). The API key authenticates requests to Prime's sandbox infrastructure.\n\n### Implementation Status\n\n> **WARNING:** Prime Intellect Sandboxes are a beta feature with incomplete implementation. The `PrimeREPL` class is not yet functional in the RLM codebase and should not be used in production.\n\nFuture implementation will follow the `IsolatedEnv` interface pattern established by `ModalREPL`, providing similar isolation guarantees and recursive LLM call capabilities.\n\n**Sources:** [README.md:95-105]()\n\n---\n\n## Communication Architecture\n\nCloud environments maintain bidirectional communication with the host process to enable recursive LLM calls:\n\n```mermaid\ngraph LR\n    subgraph Host[\"Host Process\"]\n        RLM[\"RLM Core\"]\n        Handler[\"LMHandler<br/>(TCP socket)\"]\n        Client[\"LLM Client<br/>(OpenAI, etc.)\"]\n    end\n    \n    subgraph Cloud[\"Cloud Sandbox<br/>(Modal/Prime)\"]\n        Code[\"User Code\"]\n        Query[\"llm_query()<br/>llm_query_batched()\"]\n    end\n    \n    RLM -->|\"1. execute_code()\"| Code\n    Code -->|\"2. Calls\"| Query\n    Query -->|\"3. Network request\"| Handler\n    Handler -->|\"4. Forward\"| Client\n    Client -->|\"5. LLM response\"| Handler\n    Handler -->|\"6. Response\"| Query\n    Query -->|\"7. Returns string\"| Code\n    Code -->|\"8. REPLResult\"| RLM\n```\n\n### Network Protocol\n\nCloud sandboxes communicate with the host `LMHandler` over TCP sockets (Modal) or HTTP (implementation-dependent). The handler address is injected into the sandbox environment during initialization, allowing `llm_query()` functions to locate the handler.\n\nThe communication follows the `LMRequest`/`LMResponse` protocol documented in [Communication and Data Types](#3.5), transmitting JSON-serialized prompts and responses between sandbox and host.\n\n**Sources:** [README.md:86-87]()\n\n---\n\n## Configuration Parameters\n\nCloud environments are configured through the `environment_kwargs` parameter passed to the `RLM` constructor:\n\n| Parameter | Type | Description | Default |\n|-----------|------|-------------|---------|\n| `lm_handler_address` | `tuple[str, int]` | TCP address of LMHandler (auto-set by RLM) | Set by RLM |\n| `context_payload` | `dict \\| list \\| str` | Initial context to load into sandbox | `None` |\n| `setup_code` | `str` | Python code executed during sandbox setup | `None` |\n\nExample with context and setup:\n\n```python\nrlm = RLM(\n    backend=\"openai\",\n    backend_kwargs={\"model_name\": \"gpt-4\"},\n    environment=\"modal\",\n    environment_kwargs={\n        \"context_payload\": {\"data\": [1, 2, 3, 4, 5]},\n        \"setup_code\": \"import numpy as np\\nimport pandas as pd\"\n    }\n)\n```\n\nThe `lm_handler_address` is automatically set by the `RLM` class during initialization and should not be manually configured.\n\n**Sources:** [rlm/environments/base_env.py:12-25]()\n\n---\n\n## Use Cases and Benefits\n\nCloud environments provide key advantages for production RLM deployments:\n\n### Security Isolation\nCloud sandboxes prevent malicious or buggy LLM-generated code from affecting the host system. Code runs in ephemeral containers that are destroyed after execution, with no access to host filesystem or network resources.\n\n### Scalability\nCloud infrastructure enables parallel execution of multiple RLM completions across distributed sandboxes, supporting high-throughput workloads that exceed single-machine capacity.\n\n### Reproducibility\nCloud sandboxes provide consistent execution environments with predictable resource limits (CPU, memory, timeout), improving reproducibility of RLM trajectories across runs.\n\n### Dependency Isolation\nEach sandbox can have custom Python package installations without affecting the host environment, enabling RLM completions with conflicting dependency requirements.\n\n**Sources:** [README.md:69-77](), [rlm/environments/base_env.py:29-32]()\n\n---\n\n## Environment Factory Integration\n\nCloud environments are instantiated through the environment factory system. The `get_environment()` factory function (documented in [Environment Overview](#5.1)) routes the `environment` string parameter to the appropriate class:\n\n```python\n# In RLM.__init__()\nfrom rlm.environments import get_environment\n\nenv = get_environment(\n    environment=\"modal\",  # Routes to ModalREPL\n    environment_kwargs={...}\n)\n```\n\nThe factory pattern enables extensibility—new cloud environment implementations can be added by:\n1. Creating a class that inherits from `IsolatedEnv`\n2. Implementing `setup()`, `load_context()`, and `execute_code()`\n3. Registering the class in the factory function\n\n**Sources:** [rlm/environments/base_env.py:28-47]()\n\n---\n\n## Comparison with Non-Isolated Environments\n\n| Feature | Cloud (Isolated) | LocalREPL | DockerREPL |\n|---------|------------------|-----------|------------|\n| Execution location | Remote cloud | Host process | Host Docker |\n| Process isolation | Complete | Minimal | Container-level |\n| Network isolation | Yes | No | Yes (bridge) |\n| Setup complexity | High (auth required) | Low | Medium (Docker install) |\n| Latency overhead | High (network) | Minimal | Low |\n| Use case | Production, untrusted code | Development, benchmarks | Moderate isolation needs |\n\nCloud environments trade increased latency and setup complexity for stronger security guarantees. For development and trusted workloads, `LocalREPL` or `DockerREPL` offer better performance.\n\n**Sources:** [README.md:69-106](), [rlm/environments/base_env.py:28-70]()\n\n---\n\n# Page: Logging and Visualization\n\n# Logging and Visualization\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [.gitignore](.gitignore)\n- [docs/src/lib/utils.ts](docs/src/lib/utils.ts)\n- [rlm/logger/__init__.py](rlm/logger/__init__.py)\n- [rlm/logger/verbose.py](rlm/logger/verbose.py)\n- [visualizer/src/lib/parse-logs.ts](visualizer/src/lib/parse-logs.ts)\n- [visualizer/src/lib/types.ts](visualizer/src/lib/types.ts)\n- [visualizer/src/lib/utils.ts](visualizer/src/lib/utils.ts)\n\n</details>\n\n\n\nThis page documents the logging and visualization subsystem of RLM, which provides comprehensive recording of execution trajectories and an interactive web-based viewer for analyzing RLM behavior. The system consists of the `RLMLogger` class for recording execution data to JSONL files, a structured log format with rich metadata, parsing utilities for processing log files, and a Next.js-based trajectory visualizer.\n\nFor information about the core RLM execution flow that generates these logs, see [RLM Class](#3.1). For details on the data types used in communication between components, see [Communication and Data Types](#3.5).\n\n---\n\n## System Overview\n\nThe logging and visualization system captures the complete execution trajectory of an RLM completion, including all iterations, code blocks, REPL results, and nested LLM calls. This data enables debugging, analysis, and understanding of how the RLM processes tasks.\n\n```mermaid\ngraph TB\n    RLM[\"RLM Core\"]\n    Logger[\"RLMLogger\"]\n    Files[\"JSONL Log Files<br/>(logs/ directory)\"]\n    Parser[\"parse-logs.ts<br/>parseJSONL()\"]\n    Visualizer[\"Next.js Visualizer<br/>(visualizer/)\"]\n    User[\"Developer/User\"]\n    \n    RLM -->|\"log(RLMIteration)\"| Logger\n    RLM -->|\"log_metadata(config)\"| Logger\n    Logger -->|\"write JSONL\"| Files\n    Files -->|\"read\"| Parser\n    Parser -->|\"ParsedJSONL\"| Visualizer\n    Visualizer -->|\"interactive UI\"| User\n    \n    subgraph \"Logged Data\"\n        Iterations[\"RLMIteration[]\"]\n        Metadata[\"RLMConfigMetadata\"]\n    end\n    \n    Logger --> Iterations\n    Logger --> Metadata\n```\n\n**Sources:** [README.md:129-152](), [.gitignore:5]()\n\n---\n\n## RLMLogger Class\n\nThe `RLMLogger` class is responsible for recording RLM execution trajectories to JSONL files. Each RLM completion creates a timestamped log file containing metadata and per-iteration records.\n\n### Initialization and Configuration\n\n```python\nfrom rlm.logger import RLMLogger\nfrom rlm import RLM\n\nlogger = RLMLogger(log_dir=\"./logs\")\nrlm = RLM(\n    backend=\"openai\",\n    backend_kwargs={\"model_name\": \"gpt-4\"},\n    logger=logger\n)\n```\n\nThe logger is passed to the `RLM` instance during initialization. When enabled, every `completion()` call generates a new log file in the specified `log_dir`.\n\n### Log File Naming and Location\n\nLog files are written to the configured directory (default: `./logs/`) with timestamped filenames. The `logs/` directory is excluded from version control via `.gitignore`.\n\n### Recording Mechanism\n\nThe logger captures:\n- **Metadata entry**: Configuration parameters including `backend`, `environment_type`, `max_iterations`, and model settings\n- **Iteration entries**: One entry per RLM iteration containing prompts, responses, code blocks, and execution results\n\n**Sources:** [README.md:129-141](), [.gitignore:5]()\n\n---\n\n## Log Format and Data Structures\n\nRLM logs use the JSONL (JSON Lines) format, where each line is a valid JSON object. The first line contains metadata, followed by one line per iteration.\n\n### JSONL Structure\n\n```\n{\"type\": \"metadata\", \"root_model\": \"gpt-4\", \"backend\": \"openai\", ...}\n{\"iteration\": 0, \"timestamp\": \"2025-01-15T10:30:00\", \"prompt\": [...], ...}\n{\"iteration\": 1, \"timestamp\": \"2025-01-15T10:30:05\", \"prompt\": [...], ...}\n```\n\n### Data Type Hierarchy\n\n```mermaid\ngraph TB\n    LogFile[\"JSONL Log File\"]\n    Metadata[\"RLMConfigMetadata\"]\n    Iteration[\"RLMIteration\"]\n    CodeBlock[\"CodeBlock\"]\n    REPLResult[\"REPLResult\"]\n    RLMCall[\"RLMChatCompletion\"]\n    \n    LogFile -->|\"first line\"| Metadata\n    LogFile -->|\"subsequent lines\"| Iteration\n    Iteration -->|\"code_blocks[]\"| CodeBlock\n    CodeBlock -->|\"result\"| REPLResult\n    REPLResult -->|\"rlm_calls[]\"| RLMCall\n    \n    Metadata -.->|\"root_model\"| ModelInfo[\"model: string\"]\n    Metadata -.->|\"backend\"| BackendInfo[\"backend: string\"]\n    Metadata -.->|\"max_iterations\"| MaxIter[\"max_iterations: number\"]\n    Metadata -.->|\"environment_type\"| EnvType[\"environment_type: string\"]\n    \n    Iteration -.->|\"iteration\"| IterNum[\"iteration: number\"]\n    Iteration -.->|\"prompt\"| PromptMsg[\"prompt: Message[]\"]\n    Iteration -.->|\"response\"| ResponseStr[\"response: string\"]\n    Iteration -.->|\"final_answer\"| FinalAns[\"final_answer: string | null\"]\n    Iteration -.->|\"iteration_time\"| IterTime[\"iteration_time: number\"]\n    \n    CodeBlock -.->|\"code\"| CodeStr[\"code: string\"]\n    \n    REPLResult -.->|\"stdout\"| Stdout[\"stdout: string\"]\n    REPLResult -.->|\"stderr\"| Stderr[\"stderr: string\"]\n    REPLResult -.->|\"locals\"| Locals[\"locals: Record\"]\n    REPLResult -.->|\"execution_time\"| ExecTime[\"execution_time: number\"]\n    \n    RLMCall -.->|\"prompt\"| CallPrompt[\"prompt: string\"]\n    RLMCall -.->|\"response\"| CallResp[\"response: string\"]\n    RLMCall -.->|\"prompt_tokens\"| PTok[\"prompt_tokens: number\"]\n    RLMCall -.->|\"completion_tokens\"| CTok[\"completion_tokens: number\"]\n```\n\n**Sources:** [visualizer/src/lib/types.ts:1-72]()\n\n### RLMConfigMetadata\n\nConfiguration metadata is logged as the first line with `\"type\": \"metadata\"`:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `root_model` | `string \\| null` | Primary LLM model name |\n| `max_depth` | `number \\| null` | Maximum recursion depth |\n| `max_iterations` | `number \\| null` | Maximum iterations per completion |\n| `backend` | `string \\| null` | Backend client type (e.g., \"openai\", \"portkey\") |\n| `backend_kwargs` | `Record<string, unknown> \\| null` | Backend configuration parameters |\n| `environment_type` | `string \\| null` | REPL environment type (e.g., \"local\", \"docker\") |\n| `environment_kwargs` | `Record<string, unknown> \\| null` | Environment configuration parameters |\n| `other_backends` | `string[] \\| null` | Additional backends configured for sub-calls |\n\n**Sources:** [visualizer/src/lib/types.ts:36-45](), [visualizer/src/lib/parse-logs.ts:18-30]()\n\n### RLMIteration\n\nEach iteration entry contains:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `iteration` | `number` | Iteration index (0-based) |\n| `timestamp` | `string` | ISO 8601 timestamp |\n| `prompt` | `Array<{role, content}>` | Messages sent to LLM (system + user + assistant history) |\n| `response` | `string` | LLM's raw response text |\n| `code_blocks` | `CodeBlock[]` | Extracted and executed code blocks |\n| `final_answer` | `string \\| [string, string] \\| null` | Final answer if found in this iteration |\n| `iteration_time` | `number \\| null` | Total iteration execution time in seconds |\n\n**Sources:** [visualizer/src/lib/types.ts:24-33]()\n\n### CodeBlock\n\nRepresents an executed code block within an iteration:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `code` | `string` | Python code that was executed |\n| `result` | `REPLResult` | Execution result from REPL environment |\n\n**Sources:** [visualizer/src/lib/types.ts:19-22]()\n\n### REPLResult\n\nExecution result from a REPL environment:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `stdout` | `string` | Standard output from code execution |\n| `stderr` | `string` | Standard error (empty if no errors) |\n| `locals` | `Record<string, unknown>` | REPL local variables after execution |\n| `execution_time` | `number` | Code block execution time in seconds |\n| `rlm_calls` | `RLMChatCompletion[]` | Nested LLM calls made during execution |\n\n**Sources:** [visualizer/src/lib/types.ts:11-17]()\n\n### RLMChatCompletion\n\nRecords a single LLM call (root or nested):\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `prompt` | `string \\| Record<string, unknown>` | Prompt sent to LLM |\n| `response` | `string` | LLM response text |\n| `prompt_tokens` | `number` | Token count for prompt |\n| `completion_tokens` | `number` | Token count for response |\n| `execution_time` | `number` | LLM call latency in seconds |\n\n**Sources:** [visualizer/src/lib/types.ts:3-9]()\n\n---\n\n## Parsing and Processing\n\nThe visualizer includes TypeScript utilities for parsing and analyzing JSONL log files.\n\n### parseJSONL Function\n\nThe `parseJSONL()` function in [visualizer/src/lib/parse-logs.ts:37-68]() processes JSONL content:\n\n```mermaid\ngraph LR\n    Input[\"JSONL Content\"]\n    Split[\"split by newline\"]\n    Lines[\"Array of JSON strings\"]\n    Parse[\"JSON.parse each line\"]\n    Check{\"type === 'metadata'?\"}\n    Config[\"RLMConfigMetadata\"]\n    Iters[\"RLMIteration[]\"]\n    Output[\"ParsedJSONL{<br/>iterations,<br/>config}\"]\n    \n    Input --> Split\n    Split --> Lines\n    Lines --> Parse\n    Parse --> Check\n    Check -->|\"yes\"| Config\n    Check -->|\"no\"| Iters\n    Config --> Output\n    Iters --> Output\n```\n\n**Implementation details:**\n- Splits content by newlines and filters empty lines [visualizer/src/lib/parse-logs.ts:38]()\n- Attempts to parse each line as JSON [visualizer/src/lib/parse-logs.ts:44]()\n- Identifies metadata entries by `type === 'metadata'` [visualizer/src/lib/parse-logs.ts:47]()\n- Falls back to default config if no metadata found [visualizer/src/lib/parse-logs.ts:19-30]()\n- Handles parse errors gracefully with console logging [visualizer/src/lib/parse-logs.ts:62-64]()\n\n**Sources:** [visualizer/src/lib/parse-logs.ts:37-68]()\n\n### computeMetadata Function\n\nThe `computeMetadata()` function in [visualizer/src/lib/parse-logs.ts:121-167]() calculates aggregate statistics:\n\n| Statistic | Calculation | Description |\n|-----------|-------------|-------------|\n| `totalIterations` | `iterations.length` | Number of RLM iterations |\n| `totalCodeBlocks` | Sum of `code_blocks.length` | Total code blocks executed |\n| `totalSubLMCalls` | Sum of `rlm_calls.length` | Total nested LLM calls |\n| `totalExecutionTime` | Sum of `iteration_time` or block times | Total execution time |\n| `hasErrors` | Any `stderr` non-empty | Whether errors occurred |\n| `contextQuestion` | Extracted from prompts | User's original question |\n| `finalAnswer` | Last non-null `final_answer` | Final result if available |\n\nThe function prioritizes `iteration_time` when available [visualizer/src/lib/parse-logs.ts:132-140](), falling back to summing individual code block execution times for backward compatibility.\n\n**Sources:** [visualizer/src/lib/parse-logs.ts:121-167]()\n\n### Helper Functions\n\n#### extractContextQuestion\n\nExtracts the user's original query from iteration prompts using multiple strategies [visualizer/src/lib/parse-logs.ts:70-119]():\n\n1. Searches for `original query: \"...\"` pattern in user messages\n2. Takes first substantial user message (50-500 chars)\n3. Falls back to system prompt context information\n4. Checks code block `locals.context` variable\n\n#### extractContextVariable\n\nRetrieves the `context` variable value from REPL locals [visualizer/src/lib/parse-logs.ts:4-16]():\n- Iterates through all code blocks in all iterations\n- Returns first non-null `block.result.locals.context` value\n\n#### extractFinalAnswer\n\nNormalizes final answer format [visualizer/src/lib/types.ts:65-71]():\n- Returns `answer[1]` if array format (e.g., `[\"answer\", \"actual text\"]`)\n- Returns string directly if not array\n- Returns `null` if no answer\n\n**Sources:** [visualizer/src/lib/parse-logs.ts:4-16](), [visualizer/src/lib/parse-logs.ts:70-119](), [visualizer/src/lib/types.ts:65-71]()\n\n---\n\n## Trajectory Visualizer\n\nThe trajectory visualizer is a Next.js web application located in `visualizer/` that provides an interactive UI for exploring RLM execution logs.\n\n### Running the Visualizer\n\n```bash\ncd visualizer/\nnpm run dev        # Runs on localhost:3001 by default\n```\n\nThe visualizer provides a file selector interface for loading `.jsonl` files from the logs directory.\n\n**Sources:** [README.md:142-146]()\n\n### Visualizer Architecture\n\n```mermaid\ngraph TB\n    subgraph \"User Interface (Next.js)\"\n        FileSelect[\"File Selector Component\"]\n        IterView[\"Iteration Viewer\"]\n        CodeView[\"Code Block Display\"]\n        CallTree[\"Sub-Call Tree\"]\n        StatsPanel[\"Statistics Panel\"]\n    end\n    \n    subgraph \"Data Layer (visualizer/src/lib/)\"\n        Types[\"types.ts<br/>RLMLogFile<br/>RLMIteration<br/>LogMetadata\"]\n        ParseLogs[\"parse-logs.ts<br/>parseJSONL()<br/>computeMetadata()<br/>parseLogFile()\"]\n        Utils[\"utils.ts<br/>cn() helper\"]\n    end\n    \n    subgraph \"File System\"\n        LogFiles[\"logs/*.jsonl\"]\n    end\n    \n    FileSelect -->|\"load file\"| LogFiles\n    LogFiles -->|\"content\"| ParseLogs\n    ParseLogs -->|\"ParsedJSONL\"| Types\n    Types -->|\"RLMLogFile\"| IterView\n    IterView --> CodeView\n    IterView --> CallTree\n    IterView --> StatsPanel\n    \n    Utils -.->|\"styling\"| FileSelect\n    Utils -.->|\"styling\"| IterView\n```\n\n**Sources:** [visualizer/src/lib/types.ts:1-72](), [visualizer/src/lib/parse-logs.ts:1-181](), [visualizer/src/lib/utils.ts:1-6]()\n\n### Key Features\n\nThe visualizer displays:\n\n- **Configuration panel**: Shows RLM configuration from metadata including backend, model, and environment settings\n- **Iteration timeline**: Sequential view of all iterations with timestamps\n- **Code execution details**: For each code block, displays code, stdout, stderr, and local variables\n- **Nested call visualization**: Tree view of recursive LLM calls with token counts and timing\n- **Aggregate statistics**: Total iterations, code blocks, execution time, and error indicators\n- **Context and final answer**: Extracted question and final answer if available\n\n**Sources:** [README.md:148-151]()\n\n### Data Flow\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant UI as \"Visualizer UI\"\n    participant Loader as \"File Loader\"\n    participant Parser as \"parseLogFile()\"\n    participant Compute as \"computeMetadata()\"\n    participant Display as \"Display Components\"\n    \n    User->>UI: Select .jsonl file\n    UI->>Loader: Read file content\n    Loader->>Parser: parseLogFile(fileName, content)\n    Parser->>Parser: parseJSONL(content)\n    Parser-->>Parser: Extract iterations + config\n    Parser->>Compute: computeMetadata(iterations)\n    Compute-->>Compute: Calculate statistics\n    Compute-->>Parser: LogMetadata\n    Parser-->>UI: RLMLogFile object\n    UI->>Display: Render iterations\n    Display->>User: Interactive visualization\n```\n\n**Sources:** [visualizer/src/lib/parse-logs.ts:169-180]()\n\n---\n\n## Usage Patterns\n\n### Basic Logging\n\nEnable logging by creating an `RLMLogger` instance and passing it to `RLM`:\n\n```python\nfrom rlm.logger import RLMLogger\nfrom rlm import RLM\n\nlogger = RLMLogger(log_dir=\"./logs\")\nrlm = RLM(\n    backend=\"openai\",\n    backend_kwargs={\"model_name\": \"gpt-4\"},\n    logger=logger,\n    verbose=True\n)\n\nresult = rlm.completion(\"Your query here\")\n# Log file automatically written to ./logs/\n```\n\n### Viewing Logs\n\n1. Start the visualizer: `cd visualizer/ && npm run dev`\n2. Navigate to `localhost:3001`\n3. Select a `.jsonl` file from the logs directory\n4. Explore iterations, code blocks, and nested calls interactively\n\n### Programmatic Log Analysis\n\nFor custom analysis, use the parsing utilities directly:\n\n```python\nfrom visualizer.src.lib.parse_logs import parseLogFile\n\nwith open(\"logs/trace_20250115_103000.jsonl\", \"r\") as f:\n    content = f.read()\n    \nlog_file = parseLogFile(\"trace_20250115_103000.jsonl\", content)\nprint(f\"Total iterations: {log_file.metadata.totalIterations}\")\nprint(f\"Total sub-calls: {log_file.metadata.totalSubLMCalls}\")\nprint(f\"Execution time: {log_file.metadata.totalExecutionTime:.2f}s\")\n```\n\n**Sources:** [README.md:129-151](), [visualizer/src/lib/parse-logs.ts:169-180]()\n\n---\n\n## Integration with RLM Core\n\nThe logger integrates with the RLM execution flow at key points:\n\n1. **Initialization**: Logger is passed to `RLM.__init__()` and stored as instance variable\n2. **Metadata logging**: At start of `completion()`, logger records configuration via `log_metadata()`\n3. **Iteration logging**: After each iteration completes, logger records via `log(RLMIteration)`\n4. **File management**: Logger handles file creation, naming, and JSONL serialization\n\nThe `RLMIteration` objects logged contain complete execution state including prompts, responses, code blocks, and all nested `RLMChatCompletion` records from `rlm_calls`.\n\n**Sources:** [README.md:129-141]()\n\n---\n\n# Page: RLM Logger and Verbose Output\n\n# RLM Logger and Verbose Output\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [rlm/core/rlm.py](rlm/core/rlm.py)\n- [rlm/logger/__init__.py](rlm/logger/__init__.py)\n- [rlm/logger/verbose.py](rlm/logger/verbose.py)\n\n</details>\n\n\n\n## Overview\n\nThe RLM system provides two complementary logging mechanisms for observability and debugging:\n\n1. **`RLMLogger`**: Captures complete execution trajectories in JSONL format for post-hoc analysis and visualization\n2. **`VerbosePrinter`**: Displays real-time, formatted console output during execution using the Tokyo Night color theme\n\nBoth components are optional and can be independently enabled. For details on the JSONL log format and trajectory visualization, see [Log Format and Parsing](#6.2) and [Trajectory Visualizer](#6.3).\n\n**Sources:** [rlm/logger/__init__.py:1-4](), [rlm/core/rlm.py:17](), [rlm/core/rlm.py:52-53](), [rlm/core/rlm.py:94-95]()\n\n---\n\n## Logging Architecture\n\nThe logging system integrates with the `RLM` orchestrator to capture execution metadata, iterations, and usage statistics.\n\n### Component Integration\n\n```mermaid\ngraph TB\n    RLM[\"RLM Class<br/>rlm/core/rlm.py\"]\n    \n    RLMLogger[\"RLMLogger<br/>rlm/logger/rlm_logger.py\"]\n    VerbosePrinter[\"VerbosePrinter<br/>rlm/logger/verbose.py\"]\n    \n    RichConsole[\"Rich Console<br/>Terminal Output\"]\n    JSONLFile[\"JSONL Log File<br/>Disk Storage\"]\n    \n    RLM -->|\"log_metadata()\"| RLMLogger\n    RLM -->|\"log(iteration)\"| RLMLogger\n    RLM -->|\"print_metadata()\"| VerbosePrinter\n    RLM -->|\"print_iteration()\"| VerbosePrinter\n    RLM -->|\"print_final_answer()\"| VerbosePrinter\n    RLM -->|\"print_summary()\"| VerbosePrinter\n    \n    RLMLogger -->|\"writes\"| JSONLFile\n    VerbosePrinter -->|\"displays\"| RichConsole\n```\n\n**Sources:** [rlm/core/rlm.py:52-53](), [rlm/core/rlm.py:94-95](), [rlm/core/rlm.py:121-123](), [rlm/core/rlm.py:244-248](), [rlm/core/rlm.py:253-254](), [rlm/logger/__init__.py:1-4]()\n\n| Component | Purpose | Output Destination | Data Format |\n|-----------|---------|-------------------|-------------|\n| `RLMLogger` | Trajectory capture for analysis | JSONL file on disk | Structured JSON objects |\n| `VerbosePrinter` | Real-time execution visibility | Console (stdout) | Rich-formatted text |\n</thinking>\n\n---\n\n## RLMLogger: Trajectory Capture\n\nThe `RLMLogger` class writes structured execution traces to a JSONL (JSON Lines) file, enabling post-hoc analysis and visualization.\n\n### Logging Workflow\n\n```mermaid\nsequenceDiagram\n    participant User as User Code\n    participant RLM as RLM Class\n    participant Logger as RLMLogger\n    participant File as JSONL File\n    \n    User->>RLM: __init__(logger=RLMLogger(...))\n    RLM->>Logger: log_metadata(RLMMetadata)\n    Logger->>File: Write metadata object\n    \n    User->>RLM: completion(prompt)\n    \n    loop Each Iteration\n        RLM->>RLM: _completion_turn()\n        RLM->>Logger: log(RLMIteration)\n        Logger->>File: Write iteration object\n    end\n    \n    RLM->>User: Return RLMChatCompletion\n```\n\n**Sources:** [rlm/core/rlm.py:106-123](), [rlm/core/rlm.py:244-245]()\n\n### Logged Data Types\n\nThe logger captures three types of structured data:\n\n| Data Type | Description | When Logged |\n|-----------|-------------|-------------|\n| `RLMMetadata` | Configuration parameters (backend, model, max_depth, max_iterations) | Once at RLM initialization |\n| `RLMIteration` | Complete iteration data (prompt, response, code blocks, timing) | After each iteration completes |\n| `RLMChatCompletion` | Final completion result (response, usage summary, execution time) | Implicitly included in iterations |\n\n**Sources:** [rlm/core/rlm.py:7-15](), [rlm/core/rlm.py:106-123](), [rlm/core/rlm.py:244-245]()\n\n### JSONL File Structure\n\nEach line in the log file is a complete JSON object. The file begins with metadata, followed by iteration records:\n\n```\n{\"type\": \"metadata\", \"root_model\": \"gpt-4\", \"max_depth\": 1, ...}\n{\"type\": \"iteration\", \"prompt\": [...], \"response\": \"...\", \"code_blocks\": [...], ...}\n{\"type\": \"iteration\", \"prompt\": [...], \"response\": \"...\", \"code_blocks\": [...], ...}\n...\n```\n\nFor detailed schema documentation, see [Log Format and Parsing](#6.2).\n\n**Sources:** Referenced from page 6.2 purpose\n\n### Usage Example\n\n```python\nfrom rlm import RLM\nfrom rlm.logger import RLMLogger\n\n# Create logger with output file path\nlogger = RLMLogger(log_file=\"execution_trace.jsonl\")\n\n# Pass logger to RLM instance\nrlm = RLM(\n    backend=\"openai\",\n    backend_kwargs={\"model_name\": \"gpt-4\"},\n    logger=logger\n)\n\n# Execution trace is automatically logged\nresult = rlm.completion(\"Analyze this dataset...\")\n```\n\n**Sources:** [rlm/core/rlm.py:52](), [rlm/core/rlm.py:94]()\n\n---\n\n## VerbosePrinter: Rich Console Output\n\nThe `VerbosePrinter` class provides real-time, formatted console output during RLM execution using the Rich library and a Tokyo Night-inspired color theme.\n\n### Class Structure\n\n```mermaid\ngraph TB\n    VerbosePrinter[\"VerbosePrinter<br/>rlm/logger/verbose.py\"]\n    \n    VerbosePrinter --> enabled[\"enabled: bool\"]\n    VerbosePrinter --> console[\"console: Console\"]\n    VerbosePrinter --> iteration_count[\"_iteration_count: int\"]\n    \n    VerbosePrinter --> print_metadata[\"print_metadata(metadata)\"]\n    VerbosePrinter --> print_iteration[\"print_iteration(iteration, num)\"]\n    VerbosePrinter --> print_final_answer[\"print_final_answer(answer)\"]\n    VerbosePrinter --> print_summary[\"print_summary(iterations, time, usage)\"]\n    \n    VerbosePrinter --> print_iteration_start[\"print_iteration_start(iteration)\"]\n    VerbosePrinter --> print_completion[\"print_completion(response, time)\"]\n    VerbosePrinter --> print_code_execution[\"print_code_execution(code_block)\"]\n    VerbosePrinter --> print_subcall[\"print_subcall(model, prompt, response)\"]\n```\n\n**Sources:** [rlm/logger/verbose.py:55-76]()\n\n### Tokyo Night Color Theme\n\nThe printer uses a carefully selected color palette inspired by the Tokyo Night theme [rlm/logger/verbose.py:20-46]():\n\n| Color Name | Hex Code | Usage |\n|------------|----------|-------|\n| `primary` | `#7AA2F7` | Headers, titles, emphasis |\n| `secondary` | `#BB9AF7` | Sub-calls, alternate emphasis |\n| `success` | `#9ECE6A` | Code execution, successful output |\n| `warning` | `#E0AF68` | Warnings, max iterations |\n| `error` | `#F7768E` | Errors, stderr output |\n| `text` | `#A9B1D6` | Regular text content |\n| `muted` | `#565F89` | Less important information |\n| `accent` | `#7DCFFF` | Bright accents, model names |\n\nThese colors are pre-defined as `Style` objects for consistent application throughout the output.\n\n**Sources:** [rlm/logger/verbose.py:23-46]()\n\n### Print Methods\n\nThe `VerbosePrinter` exposes methods that correspond to different stages of RLM execution:\n\n#### Configuration Display\n\n**`print_metadata(metadata: RLMMetadata)`** [rlm/logger/verbose.py:151-166]()\n\nDisplays RLM configuration at startup in a formatted panel:\n\n```\n╭─ ◆ RLM ━ Recursive Language Model ──────────────╮\n│                                                   │\n│ Backend         openai          Environment  local│\n│ Model           gpt-4           Max Iterations  30│\n│ Sub-models      gpt-3.5-turbo   Max Depth        1│\n│                                                   │\n╰───────────────────────────────────────────────────╯\n```\n\n**Sources:** [rlm/logger/verbose.py:151-166]()\n\n#### Iteration Display\n\n**`print_iteration(iteration: RLMIteration, iteration_num: int)`** [rlm/logger/verbose.py:300-325]()\n\nMain entry point for printing a complete iteration. Orchestrates calls to:\n- `print_iteration_start()` - Prints iteration header with separator\n- `print_completion()` - Prints LLM response with word count\n- `print_code_execution()` - Prints each code block execution\n- `print_subcall()` - Prints sub-LM calls made during code execution\n\n**Sources:** [rlm/logger/verbose.py:300-325]()\n\n#### Code Execution Display\n\n**`print_code_execution(code_block: CodeBlock)`** [rlm/logger/verbose.py:211-263]()\n\nDisplays code execution results in a formatted panel:\n\n```\n╭─ ▸ Code Execution (0.123s) ──────────────────────╮\n│ Code:                                            │\n│ numbers = [1, 2, 3, 4, 5]                       │\n│ print(sum(numbers))                             │\n│                                                  │\n│ Output:                                          │\n│ 15                                              │\n│                                                  │\n│ ↳ 2 sub-call(s)                                 │\n╰──────────────────────────────────────────────────╯\n```\n\nThe method handles three output scenarios:\n1. **Normal output** (stdout): Green border with `STYLE_SUCCESS`\n2. **Error output** (stderr): Red text with `STYLE_ERROR`\n3. **Sub-calls**: Summary count with `STYLE_SECONDARY`\n\n**Sources:** [rlm/logger/verbose.py:211-263]()\n\n#### Sub-Call Display\n\n**`print_subcall(model, prompt_preview, response_preview, execution_time)`** [rlm/logger/verbose.py:265-298]()\n\nDisplays nested LLM calls made during code execution:\n\n```\n╭─   ↳ Sub-call: gpt-3.5-turbo (1.45s) ───────────╮\n│ Prompt: Analyze these numbers...               │\n│ Response: The sum is 15, which is...           │\n╰─────────────────────────────────────────────────╯\n```\n\n**Sources:** [rlm/logger/verbose.py:265-298]()\n\n#### Final Answer Display\n\n**`print_final_answer(answer: Any)`** [rlm/logger/verbose.py:327-350]()\n\nDisplays the final answer in a prominent panel with warning-colored border:\n\n```\n╭─ ★ Final Answer ─────────────────────────────────╮\n│                                                   │\n│ The sum of the numbers is 15.                   │\n│                                                   │\n╰───────────────────────────────────────────────────╯\n```\n\n**Sources:** [rlm/logger/verbose.py:327-350]()\n\n#### Summary Display\n\n**`print_summary(total_iterations, total_time, usage_summary)`** [rlm/logger/verbose.py:352-393]()\n\nDisplays execution statistics at completion:\n\n```\n═════════════════════════════════════════════════════\n       Iterations      3\n       Total Time      45.23s\n       Input Tokens    12,345\n       Output Tokens   6,789\n═════════════════════════════════════════════════════\n```\n\n**Sources:** [rlm/logger/verbose.py:352-393]()\n\n### Rich Library Integration\n\nThe `VerbosePrinter` uses multiple Rich components for structured output:\n\n| Rich Component | Usage | Location |\n|----------------|-------|----------|\n| `Console` | Main rendering interface | [rlm/logger/verbose.py:74]() |\n| `Panel` | Bordered containers for content | [rlm/logger/verbose.py:139-145](), [rlm/logger/verbose.py:202-209]() |\n| `Table` | Configuration and summary tables | [rlm/logger/verbose.py:97-136](), [rlm/logger/verbose.py:363-392]() |\n| `Rule` | Horizontal separator lines | [rlm/logger/verbose.py:175-180](), [rlm/logger/verbose.py:389-392]() |\n| `Text` | Styled text composition | [rlm/logger/verbose.py:91-94](), [rlm/logger/verbose.py:189-200]() |\n| `Group` | Grouping multiple renderables | [rlm/logger/verbose.py:203]() |\n| `Style` | Color and formatting definitions | [rlm/logger/verbose.py:38-46]() |\n\n**Sources:** [rlm/logger/verbose.py:11-16]()\n\n### Helper Function\n\n**`_to_str(value: Any) -> str`** [rlm/logger/verbose.py:48-52]()\n\nSafely converts any value to string representation. Used throughout the printer to handle various data types without errors.\n\n**Sources:** [rlm/logger/verbose.py:48-52]()\n\n---\n\n## Integration with RLM\n\nBoth logging components integrate seamlessly with the `RLM` class lifecycle.\n\n### RLM Initialization\n\n```mermaid\ngraph TB\n    User[\"User Code\"]\n    RLM_init[\"RLM.__init__()\"]\n    \n    Logger_param[\"logger: RLMLogger | None\"]\n    Verbose_param[\"verbose: bool\"]\n    \n    RLMLogger_obj[\"self.logger = logger\"]\n    VerbosePrinter_obj[\"self.verbose = VerbosePrinter(enabled=verbose)\"]\n    \n    Metadata[\"RLMMetadata object\"]\n    \n    User -->|\"creates RLM instance\"| RLM_init\n    RLM_init --> Logger_param\n    RLM_init --> Verbose_param\n    \n    Logger_param --> RLMLogger_obj\n    Verbose_param --> VerbosePrinter_obj\n    \n    RLM_init -->|\"creates\"| Metadata\n    \n    Metadata -->|\"logger.log_metadata()\"| RLMLogger_obj\n    Metadata -->|\"verbose.print_metadata()\"| VerbosePrinter_obj\n```\n\n**Sources:** [rlm/core/rlm.py:40-124]()\n\nThe `RLM` class accepts both components as initialization parameters [rlm/core/rlm.py:52-53]():\n\n```python\ndef __init__(\n    self,\n    ...\n    logger: RLMLogger | None = None,\n    verbose: bool = False,\n    ...\n):\n```\n\nIf either is provided/enabled, metadata is logged immediately [rlm/core/rlm.py:106-123]():\n\n**Sources:** [rlm/core/rlm.py:40-124]()\n\n### Completion Loop Integration\n\n```mermaid\nsequenceDiagram\n    participant User as User Code\n    participant RLM as RLM.completion()\n    participant Logger as RLMLogger\n    participant Verbose as VerbosePrinter\n    \n    User->>RLM: completion(prompt)\n    \n    loop For each iteration (i < max_iterations)\n        RLM->>RLM: _completion_turn()\n        Note over RLM: Creates RLMIteration object\n        \n        RLM->>Logger: log(iteration)\n        Logger->>Logger: Write to JSONL file\n        \n        RLM->>Verbose: print_iteration(iteration, i+1)\n        Verbose->>Verbose: Display formatted output\n        \n        alt Final answer found\n            RLM->>Verbose: print_final_answer(answer)\n            RLM->>Verbose: print_summary(iterations, time, usage)\n            RLM->>User: Return RLMChatCompletion\n        end\n    end\n    \n    alt Max iterations reached\n        RLM->>Verbose: print_final_answer(answer)\n        RLM->>Verbose: print_summary(iterations, time, usage)\n        RLM->>User: Return RLMChatCompletion\n    end\n```\n\n**Sources:** [rlm/core/rlm.py:217-295]()\n\n### Logging Call Sites\n\nThe `RLM` class makes logging calls at specific execution points:\n\n| Location | Logger Call | Verbose Call | Logged Data |\n|----------|------------|--------------|-------------|\n| `__init__` [rlm/core/rlm.py:106-123]() | `log_metadata(metadata)` | `print_metadata(metadata)` | Configuration parameters |\n| `completion` loop [rlm/core/rlm.py:244-245]() | `log(iteration)` | - | Iteration object |\n| `completion` loop [rlm/core/rlm.py:248]() | - | `print_iteration(iteration, i+1)` | Iteration display |\n| Answer found [rlm/core/rlm.py:253-254]() | - | `print_final_answer(answer)` | Final answer |\n| Answer found [rlm/core/rlm.py:254]() | - | `print_summary(...)` | Execution statistics |\n| Max iterations [rlm/core/rlm.py:280-281]() | - | `print_final_answer(answer)` | Default answer |\n| Max iterations [rlm/core/rlm.py:281]() | - | `print_summary(...)` | Execution statistics |\n\n**Sources:** [rlm/core/rlm.py:106-295]()\n\n### Conditional Execution\n\nBoth loggers check their enabled state before performing operations:\n\n```python\n# VerbosePrinter methods check self.enabled\ndef print_iteration(self, iteration, iteration_num):\n    if not self.enabled:\n        return\n    # ... print logic\n```\n\nThis allows the `RLM` class to always call logging methods without conditional checks, with zero overhead when disabled.\n\n**Sources:** [rlm/logger/verbose.py:305-306]()\n\n---\n\n## Configuration and Usage\n\n### Initialization Parameters\n\nBoth loggers support configuration at initialization:\n\n#### ColorfulLogger\n\n```python\nlogger = ColorfulLogger(enabled: bool = True)\n```\n\n| Parameter | Type | Default | Purpose |\n|-----------|------|---------|---------|\n| `enabled` | `bool` | `True` | Enable/disable console output |\n\n**Sources:** [rlm/logger/root_logger.py:34-46]()\n\n#### REPLEnvLogger\n\n```python\nlogger = REPLEnvLogger(\n    max_output_length: int = 2000,\n    enabled: bool = True\n)\n```\n\n| Parameter | Type | Default | Purpose |\n|-----------|------|---------|---------|\n| `max_output_length` | `int` | `2000` | Maximum characters before truncation |\n| `enabled` | `bool` | `True` | Enable/disable console output |\n\n**Sources:** [rlm/logger/repl_logger.py:19-24]()\n\n### Disabling Logging\n\nTo run the system silently, set `enabled=False` when creating loggers:\n\n```python\n# Silent operation\nroot_logger = ColorfulLogger(enabled=False)\nrepl_logger = REPLEnvLogger(enabled=False)\n```\n\nAll logging methods check `self.enabled` before producing output, ensuring zero overhead when disabled.\n\n**Sources:** [rlm/logger/root_logger.py:50-52](), [rlm/logger/repl_logger.py:53-54]()\n\n### Adjusting Truncation Limits\n\nFor debugging large outputs, increase the truncation limit:\n\n```python\n# Allow longer outputs before truncation\nrepl_logger = REPLEnvLogger(max_output_length=10000)\n```\n\n**Sources:** [rlm/logger/repl_logger.py:19]()\n\n---\n\n## Example Output Formats\n\n### ColorfulLogger Output Example\n\n```\n================================================================================\nSTARTING NEW QUERY | 14:32:15\n================================================================================\nQUERY: Find the needle in the haystack\n\nINITIAL MESSAGES SETUP:\n  [1] SYSTEM: You are an AI assistant with access to a REPL...\n  [2] USER: Find the number 42 in the context...\n\nMODEL RESPONSE (Step 1):\n  Response: I'll search through the context systematically...\n  No tool calls - final response\n\n================================================================================\nFINAL RESPONSE:\n================================================================================\nThe magic number is 42, found at line 523,456\n================================================================================\n```\n\n**Sources:** [rlm/logger/root_logger.py:60-148]()\n\n### REPLEnvLogger Output Example\n\n```\n╭─ In [1]: ─────────────────────────────────────╮\n│ 1 │ needle = context.split('\\n')[523456]     │\n│ 2 │ print(f\"Found: {needle}\")                │\n╰───────────────────────────────────────────────╯\n\n╭─ Out [1]: ────────────────────────────────────╮\n│ Found: The magic number is 42                │\n╰───────────────────────────────────────────────╯\n\n╭─ Timing [1]: ─────────────────────────────────╮\n│ Execution time: 0.0234s                      │\n╰───────────────────────────────────────────────╯\n```\n\n**Sources:** [rlm/logger/repl_logger.py:69-140]()\n\n---\n\n## Implementation Notes\n\n### Thread Safety\n\nNeither logger implements thread safety mechanisms. They are designed for single-threaded sequential execution within the RLM workflow.\n\n**Sources:** Analysis of logger implementations\n\n### Memory Considerations\n\nThe `REPLEnvLogger` stores all executions in memory via the `executions` list. For long-running sessions with many executions, consider calling `clear()` periodically to free memory.\n\n**Sources:** [rlm/logger/repl_logger.py:22-23](), [rlm/logger/repl_logger.py:142-145]()\n\n### Color Support\n\nThe `ColorfulLogger` uses ANSI escape codes which are supported by most modern terminals. If colors don't display correctly, the terminal may not support ANSI codes.\n\n**Sources:** [rlm/logger/root_logger.py:14-32]()\n\n### Rich Library Compatibility\n\nThe `REPLEnvLogger` requires the `rich` library (specified in `requirements.txt`). The library provides robust terminal detection and graceful degradation for unsupported terminals.\n\n**Sources:** [rlm/logger/repl_logger.py:1-7](), Referenced from requirements.txt\n\n---\n\n# Page: Log Format and Parsing\n\n# Log Format and Parsing\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [.gitignore](.gitignore)\n- [docs/src/lib/utils.ts](docs/src/lib/utils.ts)\n- [rlm/clients/__init__.py](rlm/clients/__init__.py)\n- [rlm/clients/azure_openai.py](rlm/clients/azure_openai.py)\n- [rlm/core/types.py](rlm/core/types.py)\n- [visualizer/src/lib/parse-logs.ts](visualizer/src/lib/parse-logs.ts)\n- [visualizer/src/lib/types.ts](visualizer/src/lib/types.ts)\n- [visualizer/src/lib/utils.ts](visualizer/src/lib/utils.ts)\n\n</details>\n\n\n\nThis document describes the JSONL log format used by RLM to record execution trajectories, the data structures that compose log entries, and the parsing utilities for processing these logs. For information about how logs are created and written, see [RLM Logger](#6.1). For the web-based visualization interface that consumes these parsed logs, see [Trajectory Visualizer](#6.3).\n\n## Overview\n\nRLM execution trajectories are stored as newline-delimited JSON (JSONL) files, where each line represents either a metadata entry or a single iteration of the RLM execution loop. The log format captures:\n\n- Configuration metadata (model, environment, parameters)\n- Iteration-by-iteration execution traces\n- Code blocks executed in each iteration\n- REPL results including stdout, stderr, and local variables\n- Nested LLM calls made during code execution\n- Timing and token usage information\n\nThese logs enable post-execution analysis, debugging, and visualization of the complete RLM reasoning process.\n\n**Sources:** [visualizer/src/lib/types.ts:1-72](), [visualizer/src/lib/parse-logs.ts:1-182]()\n\n## JSONL File Structure\n\nEach RLM log file follows a two-entry-type structure:\n\n1. **Metadata Entry** (first line): Contains configuration information about the RLM instance\n2. **Iteration Entries** (subsequent lines): One entry per iteration of the RLM execution loop\n\n### Metadata Entry Format\n\nThe first line of a log file is a metadata object with `type: \"metadata\"`:\n\n```json\n{\n  \"type\": \"metadata\",\n  \"root_model\": \"gpt-4\",\n  \"max_depth\": 5,\n  \"max_iterations\": 30,\n  \"backend\": \"openai\",\n  \"backend_kwargs\": {\"temperature\": 0.7},\n  \"environment_type\": \"local\",\n  \"environment_kwargs\": {},\n  \"other_backends\": [\"anthropic\"]\n}\n```\n\n### Iteration Entry Format\n\nEach iteration entry contains the complete state of one execution iteration:\n\n```json\n{\n  \"type\": \"iteration\",\n  \"iteration\": 1,\n  \"timestamp\": \"2024-01-15T10:30:45.123Z\",\n  \"prompt\": [\n    {\"role\": \"system\", \"content\": \"...\"},\n    {\"role\": \"user\", \"content\": \"...\"}\n  ],\n  \"response\": \"Let me solve this...\\n```python\\nprint('hello')\\n```\",\n  \"code_blocks\": [...],\n  \"final_answer\": null,\n  \"iteration_time\": 1.234\n}\n```\n\n**Sources:** [visualizer/src/lib/types.ts:35-45](), [visualizer/src/lib/parse-logs.ts:37-68]()\n\n## Data Structure Hierarchy\n\nThe following diagram shows how log data structures are hierarchically composed:\n\n```mermaid\ngraph TD\n    RLMLogFile[\"RLMLogFile\"]\n    RLMConfigMetadata[\"RLMConfigMetadata\"]\n    LogMetadata[\"LogMetadata\"]\n    RLMIteration[\"RLMIteration\"]\n    CodeBlock[\"CodeBlock\"]\n    REPLResult[\"REPLResult\"]\n    RLMChatCompletion[\"RLMChatCompletion\"]\n    \n    RLMLogFile --> RLMConfigMetadata\n    RLMLogFile --> LogMetadata\n    RLMLogFile --> RLMIteration\n    \n    RLMIteration --> CodeBlock\n    CodeBlock --> REPLResult\n    REPLResult --> RLMChatCompletion\n    \n    RLMIteration --> |\"prompt field\"| PromptMessages[\"Array<Message>\"]\n    RLMIteration --> |\"response field\"| ResponseString[\"string\"]\n    RLMIteration --> |\"final_answer field\"| FinalAnswer[\"string | [string,string] | null\"]\n```\n\n**Diagram: Log Data Structure Hierarchy**\n\n**Sources:** [visualizer/src/lib/types.ts:1-72]()\n\n## Core Data Types\n\n### RLMIteration\n\nRepresents a single iteration of the RLM execution loop. Contains the LLM prompt, response, extracted code blocks, and execution results.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `type` | `string` | Optional, set to `\"iteration\"` |\n| `iteration` | `number` | Iteration number (1-indexed) |\n| `timestamp` | `string` | ISO 8601 timestamp |\n| `prompt` | `Array<{role: string, content: string}>` | Chat messages sent to LLM |\n| `response` | `string` | Raw LLM response text |\n| `code_blocks` | `CodeBlock[]` | Extracted code blocks with results |\n| `final_answer` | `string \\| [string, string] \\| null` | Final answer if found |\n| `iteration_time` | `number \\| null` | Total iteration time in seconds |\n\n**Sources:** [visualizer/src/lib/types.ts:24-33]()\n\n### CodeBlock\n\nRepresents a single code block extracted from the LLM response and executed in the REPL environment.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `code` | `string` | Python code extracted from markdown |\n| `result` | `REPLResult` | Execution results from REPL |\n\n**Sources:** [visualizer/src/lib/types.ts:19-22]()\n\n### REPLResult\n\nContains the complete execution results from running a code block in the REPL environment.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `stdout` | `string` | Standard output captured |\n| `stderr` | `string` | Standard error captured |\n| `locals` | `Record<string, unknown>` | Local variables from REPL state |\n| `execution_time` | `number` | Execution time in seconds |\n| `rlm_calls` | `RLMChatCompletion[]` | Nested LLM calls made via `llm_query` |\n\n**Sources:** [visualizer/src/lib/types.ts:11-17]()\n\n### RLMChatCompletion\n\nRecords a single LLM call, either the main iteration call or a nested call from within executed code.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `prompt` | `string \\| Record<string, unknown>` | Prompt sent to LLM |\n| `response` | `string` | LLM response text |\n| `prompt_tokens` | `number` | Tokens in prompt |\n| `completion_tokens` | `number` | Tokens in completion |\n| `execution_time` | `number` | LLM call time in seconds |\n\n**Sources:** [visualizer/src/lib/types.ts:3-9]()\n\n### RLMConfigMetadata\n\nStores configuration information about the RLM instance that generated the log.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `root_model` | `string \\| null` | Primary LLM model identifier |\n| `max_depth` | `number \\| null` | Maximum recursion depth |\n| `max_iterations` | `number \\| null` | Maximum iteration limit |\n| `backend` | `string \\| null` | LLM backend type (e.g., \"openai\") |\n| `backend_kwargs` | `Record<string, unknown> \\| null` | Backend configuration |\n| `environment_type` | `string \\| null` | REPL environment type |\n| `environment_kwargs` | `Record<string, unknown> \\| null` | Environment configuration |\n| `other_backends` | `string[] \\| null` | Additional configured backends |\n\n**Sources:** [visualizer/src/lib/types.ts:36-45]()\n\n### LogMetadata\n\nComputed summary statistics about a log file, generated during parsing.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `totalIterations` | `number` | Number of iterations in log |\n| `totalCodeBlocks` | `number` | Total code blocks executed |\n| `totalSubLMCalls` | `number` | Total nested LLM calls |\n| `contextQuestion` | `string` | Extracted user question/prompt |\n| `finalAnswer` | `string \\| null` | Final answer if present |\n| `totalExecutionTime` | `number` | Total execution time (seconds) |\n| `hasErrors` | `boolean` | Whether any stderr was captured |\n\n**Sources:** [visualizer/src/lib/types.ts:55-63]()\n\n## Parsing Flow\n\nThe following diagram illustrates how raw JSONL content is parsed into structured data:\n\n```mermaid\ngraph LR\n    JSONLFile[\"JSONL File Content\"]\n    parseJSONL[\"parseJSONL()\"]\n    SplitLines[\"Split by newlines\"]\n    ParseLine[\"JSON.parse() each line\"]\n    \n    CheckType{\"line.type === 'metadata'?\"}\n    MetadataObj[\"RLMConfigMetadata\"]\n    IterationObj[\"RLMIteration\"]\n    \n    IterationsArray[\"RLMIteration[]\"]\n    ComputeMeta[\"computeMetadata()\"]\n    LogMetadata[\"LogMetadata\"]\n    \n    ParsedJSONL[\"ParsedJSONL\"]\n    parseLogFile[\"parseLogFile()\"]\n    RLMLogFile[\"RLMLogFile\"]\n    \n    JSONLFile --> parseJSONL\n    parseJSONL --> SplitLines\n    SplitLines --> ParseLine\n    ParseLine --> CheckType\n    \n    CheckType -->|\"yes\"| MetadataObj\n    CheckType -->|\"no\"| IterationObj\n    IterationObj --> IterationsArray\n    \n    IterationsArray --> ParsedJSONL\n    MetadataObj --> ParsedJSONL\n    \n    ParsedJSONL --> parseLogFile\n    IterationsArray --> ComputeMeta\n    ComputeMeta --> LogMetadata\n    LogMetadata --> RLMLogFile\n    parseLogFile --> RLMLogFile\n```\n\n**Diagram: JSONL Parsing Pipeline**\n\n**Sources:** [visualizer/src/lib/parse-logs.ts:37-68](), [visualizer/src/lib/parse-logs.ts:169-180]()\n\n## Parsing Utilities\n\n### parseJSONL()\n\nThe core parsing function that converts raw JSONL string content into structured data.\n\n**Function Signature:**\n```typescript\nfunction parseJSONL(content: string): ParsedJSONL\n```\n\n**Implementation Details:**\n- Splits content by newlines and filters empty lines [visualizer/src/lib/parse-logs.ts:38]()\n- Parses each line as JSON [visualizer/src/lib/parse-logs.ts:44]()\n- Routes lines to metadata or iterations based on `type` field [visualizer/src/lib/parse-logs.ts:47-61]()\n- Returns both iterations array and config metadata [visualizer/src/lib/parse-logs.ts:67]()\n- Handles parsing errors gracefully with console logging [visualizer/src/lib/parse-logs.ts:62-64]()\n\n**Return Type:**\n```typescript\ninterface ParsedJSONL {\n  iterations: RLMIteration[];\n  config: RLMConfigMetadata;\n}\n```\n\n**Sources:** [visualizer/src/lib/parse-logs.ts:32-68]()\n\n### computeMetadata()\n\nAnalyzes an array of iterations to compute summary statistics and extract key information.\n\n**Function Signature:**\n```typescript\nfunction computeMetadata(iterations: RLMIteration[]): LogMetadata\n```\n\n**Computed Metrics:**\n- Counts total code blocks across all iterations [visualizer/src/lib/parse-logs.ts:129]()\n- Sums nested LLM calls from all REPL results [visualizer/src/lib/parse-logs.ts:148]()\n- Aggregates execution time from `iteration_time` or block execution times [visualizer/src/lib/parse-logs.ts:132-140]()\n- Detects errors by checking for non-empty stderr [visualizer/src/lib/parse-logs.ts:144-146]()\n- Extracts final answer from last iteration with answer [visualizer/src/lib/parse-logs.ts:153-155]()\n- Calls `extractContextQuestion()` to derive user prompt [visualizer/src/lib/parse-logs.ts:162]()\n\n**Sources:** [visualizer/src/lib/parse-logs.ts:121-167]()\n\n### extractContextQuestion()\n\nAttempts to extract the original user question or prompt from iteration data using multiple heuristics.\n\n**Extraction Strategy:**\n1. Searches for `original query: \"...\"` pattern in user messages [visualizer/src/lib/parse-logs.ts:80-83]()\n2. Finds first substantial user message (50-500 chars) [visualizer/src/lib/parse-logs.ts:91-94]()\n3. Looks for context information in system prompt [visualizer/src/lib/parse-logs.ts:98-104]()\n4. Checks code block locals for `context` variable [visualizer/src/lib/parse-logs.ts:107-116]()\n5. Falls back to generic message if none found [visualizer/src/lib/parse-logs.ts:118]()\n\n**Sources:** [visualizer/src/lib/parse-logs.ts:70-119]()\n\n### extractContextVariable()\n\nRetrieves the `context` variable value from REPL execution results, if present.\n\n**Function Signature:**\n```typescript\nfunction extractContextVariable(iterations: RLMIteration[]): string | null\n```\n\n**Implementation:**\n- Iterates through all iterations and code blocks [visualizer/src/lib/parse-logs.ts:5-6]()\n- Checks `result.locals.context` for each block [visualizer/src/lib/parse-logs.ts:7-8]()\n- Returns first string context found [visualizer/src/lib/parse-logs.ts:9-11]()\n\n**Sources:** [visualizer/src/lib/parse-logs.ts:4-16]()\n\n### parseLogFile()\n\nTop-level function that orchestrates complete log file parsing from filename and content.\n\n**Function Signature:**\n```typescript\nfunction parseLogFile(fileName: string, content: string): RLMLogFile\n```\n\n**Processing Steps:**\n1. Calls `parseJSONL()` to extract iterations and config [visualizer/src/lib/parse-logs.ts:170]()\n2. Calls `computeMetadata()` to generate summary statistics [visualizer/src/lib/parse-logs.ts:171]()\n3. Assembles complete `RLMLogFile` object with all data [visualizer/src/lib/parse-logs.ts:173-179]()\n\n**Sources:** [visualizer/src/lib/parse-logs.ts:169-180]()\n\n## Type Relationships\n\nThe following diagram shows how parsing functions operate on and transform data types:\n\n```mermaid\ngraph TB\n    StringContent[\"Raw JSONL String\"]\n    parseJSONL_fn[\"parseJSONL()\"]\n    ParsedJSONL[\"ParsedJSONL\"]\n    \n    Iterations[\"RLMIteration[]\"]\n    Config[\"RLMConfigMetadata\"]\n    \n    computeMetadata_fn[\"computeMetadata()\"]\n    LogMetadata[\"LogMetadata\"]\n    \n    extractContextQuestion_fn[\"extractContextQuestion()\"]\n    extractContextVariable_fn[\"extractContextVariable()\"]\n    extractFinalAnswer_fn[\"extractFinalAnswer()\"]\n    \n    parseLogFile_fn[\"parseLogFile()\"]\n    RLMLogFile[\"RLMLogFile\"]\n    \n    StringContent --> parseJSONL_fn\n    parseJSONL_fn --> ParsedJSONL\n    ParsedJSONL --> Iterations\n    ParsedJSONL --> Config\n    \n    Iterations --> computeMetadata_fn\n    Iterations --> extractContextQuestion_fn\n    Iterations --> extractContextVariable_fn\n    \n    computeMetadata_fn --> LogMetadata\n    computeMetadata_fn -.calls.-> extractContextQuestion_fn\n    computeMetadata_fn -.calls.-> extractFinalAnswer_fn\n    \n    StringContent --> parseLogFile_fn\n    parseLogFile_fn -.calls.-> parseJSONL_fn\n    parseLogFile_fn -.calls.-> computeMetadata_fn\n    parseLogFile_fn --> RLMLogFile\n    \n    Config --> RLMLogFile\n    Iterations --> RLMLogFile\n    LogMetadata --> RLMLogFile\n```\n\n**Diagram: Parsing Functions and Type Transformations**\n\n**Sources:** [visualizer/src/lib/parse-logs.ts:1-182](), [visualizer/src/lib/types.ts:1-72]()\n\n## Final Answer Extraction\n\nThe log format supports three representations of final answers:\n\n| Format | Type | Usage |\n|--------|------|-------|\n| Simple string | `string` | Direct answer text |\n| Tuple | `[string, string]` | `[tag, answer_text]` pair |\n| None | `null` | No final answer in iteration |\n\nThe `extractFinalAnswer()` helper function normalizes these formats:\n\n**Function Signature:**\n```typescript\nfunction extractFinalAnswer(answer: string | [string, string] | null): string | null\n```\n\n**Behavior:**\n- Returns `null` for null input [visualizer/src/lib/types.ts:66]()\n- Extracts second element from tuple format [visualizer/src/lib/types.ts:67-69]()\n- Returns string directly [visualizer/src/lib/types.ts:70]()\n\n**Sources:** [visualizer/src/lib/types.ts:65-71]()\n\n## Backwards Compatibility\n\nThe parsing utilities handle logs that predate the metadata entry feature:\n\n- `getDefaultConfig()` provides null values for all config fields [visualizer/src/lib/parse-logs.ts:19-30]()\n- `parseJSONL()` initializes with default config if no metadata entry present [visualizer/src/lib/parse-logs.ts:40]()\n- Iteration parsing continues normally for logs without metadata [visualizer/src/lib/parse-logs.ts:59-61]()\n\nThis ensures older log files remain parseable despite schema evolution.\n\n**Sources:** [visualizer/src/lib/parse-logs.ts:18-30](), [visualizer/src/lib/parse-logs.ts:37-68]()\n\n## Usage Example\n\nThe typical usage pattern for parsing a log file:\n\n```typescript\nimport { parseLogFile } from './lib/parse-logs';\nimport { readFileSync } from 'fs';\n\n// Read JSONL file\nconst content = readFileSync('logs/execution_20240115.jsonl', 'utf-8');\n\n// Parse into structured format\nconst logFile = parseLogFile('execution_20240115.jsonl', content);\n\n// Access parsed data\nconsole.log(`Iterations: ${logFile.metadata.totalIterations}`);\nconsole.log(`Question: ${logFile.metadata.contextQuestion}`);\nconsole.log(`Final Answer: ${logFile.metadata.finalAnswer}`);\n\n// Iterate through execution trace\nfor (const iteration of logFile.iterations) {\n  console.log(`Iteration ${iteration.iteration}:`);\n  for (const block of iteration.code_blocks) {\n    console.log(`  Code: ${block.code.substring(0, 50)}...`);\n    console.log(`  Nested calls: ${block.result.rlm_calls.length}`);\n  }\n}\n```\n\nThe parsed `RLMLogFile` object is then typically passed to visualization components for rendering. See [Trajectory Visualizer](#6.3) for details on how these parsed structures are consumed by the UI.\n\n**Sources:** [visualizer/src/lib/parse-logs.ts:169-180]()\n\n---\n\n# Page: Trajectory Visualizer\n\n# Trajectory Visualizer\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [.github/workflows/docs.yml](.github/workflows/docs.yml)\n- [.gitignore](.gitignore)\n- [docs/next.config.js](docs/next.config.js)\n- [docs/src/components/Sidebar.tsx](docs/src/components/Sidebar.tsx)\n- [docs/src/lib/utils.ts](docs/src/lib/utils.ts)\n- [visualizer/src/lib/parse-logs.ts](visualizer/src/lib/parse-logs.ts)\n- [visualizer/src/lib/types.ts](visualizer/src/lib/types.ts)\n- [visualizer/src/lib/utils.ts](visualizer/src/lib/utils.ts)\n\n</details>\n\n\n\nThe Trajectory Visualizer is a web-based interactive tool for exploring and debugging RLM execution trajectories. It provides a graphical interface for inspecting JSONL log files generated by the `RLMLogger`, displaying iterations, code blocks, LLM calls, and execution results in a structured, navigable format.\n\nFor information about generating log files, see [RLM Logger](#6.1). For details about the JSONL log format and data structures, see [Log Format and Parsing](#6.2).\n\n---\n\n## Overview\n\nThe visualizer is a Next.js application located in the `visualizer/` directory that reads JSONL log files and presents them through an interactive UI. It parses each log file to extract iterations, configuration metadata, and computed statistics, then renders them in a hierarchical view that allows developers to trace through the RLM's reasoning process step-by-step.\n\n**Sources:** [README.md:129-151]()\n\n---\n\n## Architecture\n\nThe visualizer follows a three-layer architecture: the **Parsing Layer** processes raw JSONL files, the **Data Layer** structures the parsed content into TypeScript types, and the **Presentation Layer** renders the interactive UI.\n\n### System Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Input\"\n        JSONL[\"JSONL Log Files<br/>(from RLMLogger)\"]\n    end\n    \n    subgraph \"Parsing Layer\"\n        ParseLogFile[\"parseLogFile()<br/>Main entry point\"]\n        ParseJSONL[\"parseJSONL()<br/>Line-by-line parser\"]\n        ComputeMetadata[\"computeMetadata()<br/>Statistics computation\"]\n        ExtractContext[\"extractContextQuestion()<br/>Context extraction\"]\n    end\n    \n    subgraph \"Data Layer\"\n        RLMLogFile[\"RLMLogFile<br/>Complete log structure\"]\n        RLMIteration[\"RLMIteration[]<br/>Iteration sequence\"]\n        RLMConfigMetadata[\"RLMConfigMetadata<br/>RLM configuration\"]\n        LogMetadata[\"LogMetadata<br/>Computed statistics\"]\n    end\n    \n    subgraph \"Presentation Layer\"\n        FileSelector[\"File Selector<br/>Upload interface\"]\n        IterationView[\"Iteration View<br/>Timeline display\"]\n        CodeBlockView[\"Code Block View<br/>Execution details\"]\n        SubLMView[\"Sub-LM Call View<br/>Nested queries\"]\n    end\n    \n    JSONL --> ParseLogFile\n    ParseLogFile --> ParseJSONL\n    ParseLogFile --> ComputeMetadata\n    ComputeMetadata --> ExtractContext\n    \n    ParseJSONL --> RLMIteration\n    ParseJSONL --> RLMConfigMetadata\n    ComputeMetadata --> LogMetadata\n    \n    RLMIteration --> RLMLogFile\n    RLMConfigMetadata --> RLMLogFile\n    LogMetadata --> RLMLogFile\n    \n    RLMLogFile --> FileSelector\n    FileSelector --> IterationView\n    IterationView --> CodeBlockView\n    CodeBlockView --> SubLMView\n```\n\n**Sources:** [visualizer/src/lib/parse-logs.ts:1-181](), [visualizer/src/lib/types.ts:1-72]()\n\n---\n\n## Setup and Usage\n\n### Installation\n\nThe visualizer requires Node.js and uses npm for dependency management:\n\n```bash\ncd visualizer/\nnpm install\nnpm run dev  # Starts development server on localhost:3001\n```\n\nThe application runs as a local web server and does not require any external API keys or backend services.\n\n**Sources:** [README.md:142-146]()\n\n### Generating Log Files\n\nTo generate log files that can be visualized, initialize an `RLMLogger` and pass it to the `RLM` instance:\n\n```python\nfrom rlm.logger import RLMLogger\nfrom rlm import RLM\n\nlogger = RLMLogger(log_dir=\"./logs\")\nrlm = RLM(\n    backend=\"openai\",\n    backend_kwargs={\"model_name\": \"gpt-4\"},\n    logger=logger\n)\n\nresult = rlm.completion(\"Your query here\")\n# Log file is automatically saved to ./logs/<timestamp>.jsonl\n```\n\nEach completion call generates a separate JSONL file containing all iterations, code blocks, and sub-LM calls for that execution.\n\n**Sources:** [README.md:131-140]()\n\n---\n\n## Parsing Layer\n\n### Data Flow Pipeline\n\n```mermaid\ngraph LR\n    subgraph \"File Input\"\n        RawContent[\"Raw JSONL<br/>content string\"]\n    end\n    \n    subgraph \"parseJSONL()\"\n        SplitLines[\"Split by newlines\"]\n        ParseLine[\"JSON.parse() each line\"]\n        TypeCheck[\"Check line.type\"]\n        \n        SplitLines --> ParseLine\n        ParseLine --> TypeCheck\n    end\n    \n    subgraph \"Output Branches\"\n        MetadataLine[\"type: 'metadata'\"]\n        IterationLine[\"RLMIteration object\"]\n        \n        TypeCheck --> MetadataLine\n        TypeCheck --> IterationLine\n    end\n    \n    subgraph \"computeMetadata()\"\n        CountBlocks[\"Count code blocks\"]\n        CountCalls[\"Count rlm_calls\"]\n        SumTime[\"Sum execution_time\"]\n        ExtractAnswer[\"Extract final_answer\"]\n        ExtractQuestion[\"extractContextQuestion()\"]\n        \n        CountBlocks --> MetadataObj[\"LogMetadata\"]\n        CountCalls --> MetadataObj\n        SumTime --> MetadataObj\n        ExtractAnswer --> MetadataObj\n        ExtractQuestion --> MetadataObj\n    end\n    \n    RawContent --> SplitLines\n    MetadataLine --> ConfigOut[\"RLMConfigMetadata\"]\n    IterationLine --> IterationsOut[\"RLMIteration[]\"]\n    \n    IterationsOut --> computeMetadata\n    ConfigOut --> FinalOutput[\"RLMLogFile\"]\n    MetadataObj --> FinalOutput\n    IterationsOut --> FinalOutput\n```\n\n**Sources:** [visualizer/src/lib/parse-logs.ts:37-68](), [visualizer/src/lib/parse-logs.ts:121-167]()\n\n### parseLogFile Function\n\nThe `parseLogFile()` function is the main entry point for parsing a log file. It orchestrates the parsing pipeline:\n\n| Step | Function | Input | Output |\n|------|----------|-------|--------|\n| 1 | `parseJSONL()` | Raw JSONL string | `{ iterations, config }` |\n| 2 | `computeMetadata()` | `RLMIteration[]` | `LogMetadata` |\n| 3 | Assembly | All components | `RLMLogFile` |\n\n**Implementation:**\n[visualizer/src/lib/parse-logs.ts:169-180]()\n\n**Sources:** [visualizer/src/lib/parse-logs.ts:169-180]()\n\n### parseJSONL Function\n\nThe `parseJSONL()` function processes the raw JSONL content line-by-line:\n\n1. **Line Splitting:** Splits input by newlines, filtering empty lines [visualizer/src/lib/parse-logs.ts:38]()\n2. **JSON Parsing:** Attempts to parse each line as JSON [visualizer/src/lib/parse-logs.ts:44]()\n3. **Type Discrimination:** Checks if `parsed.type === 'metadata'` to separate metadata from iterations [visualizer/src/lib/parse-logs.ts:47]()\n4. **Metadata Extraction:** Populates `RLMConfigMetadata` with null-coalescing for backwards compatibility [visualizer/src/lib/parse-logs.ts:48-57]()\n5. **Iteration Collection:** Accumulates non-metadata entries as `RLMIteration` objects [visualizer/src/lib/parse-logs.ts:60]()\n\n**Default Handling:** When no metadata line exists, `getDefaultConfig()` provides a fallback configuration with all fields set to `null` [visualizer/src/lib/parse-logs.ts:19-30]().\n\n**Sources:** [visualizer/src/lib/parse-logs.ts:37-68](), [visualizer/src/lib/parse-logs.ts:19-30]()\n\n### computeMetadata Function\n\nThe `computeMetadata()` function derives summary statistics from the iteration sequence:\n\n```mermaid\ngraph TD\n    Input[\"RLMIteration[]\"]\n    \n    subgraph \"Iteration Loop\"\n        CountBlocks[\"totalCodeBlocks += code_blocks.length\"]\n        CheckIterTime[\"iteration_time != null?\"]\n        UseIterTime[\"totalExecutionTime += iteration_time\"]\n        SumBlockTime[\"Sum block.result.execution_time\"]\n        \n        CheckIterTime -->|yes| UseIterTime\n        CheckIterTime -->|no| SumBlockTime\n    end\n    \n    subgraph \"Code Block Loop\"\n        CheckStderr[\"block.result.stderr?\"]\n        SetError[\"hasErrors = true\"]\n        CountRLMCalls[\"totalSubLMCalls += rlm_calls.length\"]\n        \n        CheckStderr -->|yes| SetError\n    end\n    \n    subgraph \"Final Answer Extraction\"\n        CheckFinalAnswer[\"iter.final_answer?\"]\n        ExtractFinalAnswerFunc[\"extractFinalAnswer()\"]\n        \n        CheckFinalAnswer -->|yes| ExtractFinalAnswerFunc\n    end\n    \n    Input --> CountBlocks\n    Input --> CheckIterTime\n    CountBlocks --> CheckStderr\n    CheckStderr --> CountRLMCalls\n    CountRLMCalls --> CheckFinalAnswer\n    \n    ExtractFinalAnswerFunc --> Output[\"LogMetadata\"]\n    CountBlocks --> Output\n    CountRLMCalls --> Output\n    UseIterTime --> Output\n    SumBlockTime --> Output\n    SetError --> Output\n```\n\nThe function tracks:\n- **totalIterations:** Length of iteration array\n- **totalCodeBlocks:** Sum of code blocks across all iterations\n- **totalSubLMCalls:** Sum of `rlm_calls` arrays from all `REPLResult` objects\n- **totalExecutionTime:** Prefers `iteration_time` when available, otherwise sums `execution_time` from code blocks\n- **hasErrors:** Set to `true` if any `stderr` is non-empty\n- **finalAnswer:** Extracted from the last iteration's `final_answer` field\n- **contextQuestion:** Extracted by `extractContextQuestion()`\n\n**Sources:** [visualizer/src/lib/parse-logs.ts:121-167]()\n\n### extractContextQuestion Function\n\nThe `extractContextQuestion()` function attempts to identify the user's original query through multiple heuristics:\n\n1. **Quoted Query Match:** Searches for `original query: \"...\"` pattern in user messages [visualizer/src/lib/parse-logs.ts:80-83]()\n2. **User Message Heuristic:** Selects first substantial user message (50-500 chars) [visualizer/src/lib/parse-logs.ts:91-94]()\n3. **System Prompt Fallback:** Searches for context description in system message [visualizer/src/lib/parse-logs.ts:98-104]()\n4. **Context Variable Inspection:** Checks `context` variable in code block locals [visualizer/src/lib/parse-logs.ts:107-116]()\n5. **Default Message:** Returns generic message if no context found\n\n**Sources:** [visualizer/src/lib/parse-logs.ts:70-119]()\n\n---\n\n## Type System\n\n### Type Hierarchy\n\n```mermaid\ngraph TB\n    subgraph \"Top-Level Types\"\n        RLMLogFile[\"RLMLogFile<br/>fileName, filePath, iterations,<br/>metadata, config\"]\n    end\n    \n    subgraph \"Iteration Data\"\n        RLMIteration[\"RLMIteration<br/>iteration, timestamp, prompt,<br/>response, code_blocks,<br/>final_answer, iteration_time\"]\n        CodeBlock[\"CodeBlock<br/>code, result\"]\n        REPLResult[\"REPLResult<br/>stdout, stderr, locals,<br/>execution_time, rlm_calls\"]\n        RLMChatCompletion[\"RLMChatCompletion<br/>prompt, response,<br/>prompt_tokens, completion_tokens,<br/>execution_time\"]\n    end\n    \n    subgraph \"Metadata Types\"\n        LogMetadata[\"LogMetadata<br/>totalIterations, totalCodeBlocks,<br/>totalSubLMCalls, contextQuestion,<br/>finalAnswer, totalExecutionTime,<br/>hasErrors\"]\n        RLMConfigMetadata[\"RLMConfigMetadata<br/>root_model, max_depth,<br/>max_iterations, backend,<br/>backend_kwargs, environment_type,<br/>environment_kwargs, other_backends\"]\n    end\n    \n    RLMLogFile --> RLMIteration\n    RLMLogFile --> LogMetadata\n    RLMLogFile --> RLMConfigMetadata\n    \n    RLMIteration --> CodeBlock\n    CodeBlock --> REPLResult\n    REPLResult --> RLMChatCompletion\n```\n\n**Sources:** [visualizer/src/lib/types.ts:1-72]()\n\n### Key Type Definitions\n\n#### RLMLogFile Interface\n\nThe top-level interface representing a complete parsed log file:\n\n```typescript\ninterface RLMLogFile {\n  fileName: string;           // Display name\n  filePath: string;           // File path or identifier\n  iterations: RLMIteration[]; // Execution sequence\n  metadata: LogMetadata;      // Computed statistics\n  config: RLMConfigMetadata;  // RLM configuration\n}\n```\n\n**Sources:** [visualizer/src/lib/types.ts:47-53]()\n\n#### RLMIteration Interface\n\nRepresents a single iteration in the RLM execution loop:\n\n```typescript\ninterface RLMIteration {\n  type?: string;              // Optional type discriminator\n  iteration: number;          // 0-indexed iteration number\n  timestamp: string;          // ISO 8601 timestamp\n  prompt: Array<{             // Chat prompt (role/content pairs)\n    role: string;\n    content: string;\n  }>;\n  response: string;           // LLM response text\n  code_blocks: CodeBlock[];   // Extracted code blocks\n  final_answer: string | [string, string] | null;  // If present, RLM terminated\n  iteration_time: number | null;  // Total iteration time in seconds\n}\n```\n\n**Sources:** [visualizer/src/lib/types.ts:24-33]()\n\n#### CodeBlock and REPLResult Interfaces\n\n`CodeBlock` pairs code with its execution result:\n\n```typescript\ninterface CodeBlock {\n  code: string;        // Python code\n  result: REPLResult;  // Execution outcome\n}\n\ninterface REPLResult {\n  stdout: string;                   // Standard output\n  stderr: string;                   // Standard error\n  locals: Record<string, unknown>;  // REPL variables\n  execution_time: number;           // Execution time in seconds\n  rlm_calls: RLMChatCompletion[];   // Nested LLM calls made during execution\n}\n```\n\n**Sources:** [visualizer/src/lib/types.ts:11-22]()\n\n#### RLMConfigMetadata Interface\n\nStores RLM configuration captured at the start of logging:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `root_model` | `string \\| null` | Primary model name |\n| `max_depth` | `number \\| null` | Maximum recursion depth |\n| `max_iterations` | `number \\| null` | Maximum iteration count |\n| `backend` | `string \\| null` | Backend type (e.g., \"openai\") |\n| `backend_kwargs` | `Record<string, unknown> \\| null` | Backend configuration |\n| `environment_type` | `string \\| null` | Environment type (e.g., \"local\") |\n| `environment_kwargs` | `Record<string, unknown> \\| null` | Environment configuration |\n| `other_backends` | `string[] \\| null` | Additional configured backends |\n\n**Sources:** [visualizer/src/lib/types.ts:36-45]()\n\n#### LogMetadata Interface\n\nContains computed statistics about the execution:\n\n```typescript\ninterface LogMetadata {\n  totalIterations: number;     // Number of iterations\n  totalCodeBlocks: number;     // Total code blocks executed\n  totalSubLMCalls: number;     // Total nested LLM calls\n  contextQuestion: string;     // Extracted user query\n  finalAnswer: string | null;  // Final answer if present\n  totalExecutionTime: number;  // Total execution time in seconds\n  hasErrors: boolean;          // Whether any stderr was produced\n}\n```\n\n**Sources:** [visualizer/src/lib/types.ts:55-63]()\n\n---\n\n## Utility Functions\n\n### extractFinalAnswer Function\n\nHelper function to normalize the `final_answer` field, which can be stored in multiple formats:\n\n```typescript\nfunction extractFinalAnswer(answer: string | [string, string] | null): string | null {\n  if (!answer) return null;\n  if (Array.isArray(answer)) {\n    return answer[1];  // Take second element (answer text)\n  }\n  return answer;\n}\n```\n\nThe array format `[thinking, answer]` is used when the LLM provides both reasoning and a final answer. This function extracts just the answer component.\n\n**Sources:** [visualizer/src/lib/types.ts:65-71]()\n\n### cn Utility Function\n\nThe `cn()` function is a utility for merging Tailwind CSS class names, implemented in both the visualizer and docs directories:\n\n```typescript\nfunction cn(...inputs: ClassValue[]) {\n  return twMerge(clsx(inputs));\n}\n```\n\nThis is used throughout the UI components for conditional styling.\n\n**Sources:** [visualizer/src/lib/utils.ts:1-6](), [docs/src/lib/utils.ts:1-6]()\n\n---\n\n## UI Components and Features\n\n### Interactive Elements\n\nThe visualizer provides several interactive views:\n\n1. **File Selector:** Upload/select JSONL files from the file system\n2. **Metadata Dashboard:** Displays summary statistics (iterations, code blocks, sub-LM calls, execution time, errors)\n3. **Configuration Panel:** Shows RLM configuration from `RLMConfigMetadata`\n4. **Iteration Timeline:** Chronological list of all iterations with expand/collapse functionality\n5. **Code Block Inspector:** Displays code, stdout, stderr, and local variables for each execution\n6. **Sub-LM Call Tree:** Hierarchical view of nested `llm_query()` and `llm_query_batched()` calls\n\n**Sources:** [README.md:148-151]()\n\n### Screenshot Reference\n\nThe visualizer UI is shown in the README screenshot:\n\n![RLM Visualizer Example](media/visualizer.png)\n\n**Sources:** [README.md:149-151]()\n\n---\n\n## Integration with Logging System\n\n### Data Flow from RLM to Visualizer\n\n```mermaid\ngraph LR\n    subgraph \"RLM Execution\"\n        RLMCore[\"RLM.completion()\"]\n        RLMLogger[\"RLMLogger\"]\n        WriteJSONL[\"Write .jsonl file\"]\n    end\n    \n    subgraph \"File System\"\n        LogFile[\"./logs/<timestamp>.jsonl\"]\n    end\n    \n    subgraph \"Visualizer\"\n        LoadFile[\"File upload/selection\"]\n        ParseLogFile[\"parseLogFile()\"]\n        RenderUI[\"Render UI components\"]\n    end\n    \n    RLMCore --> RLMLogger\n    RLMLogger --> WriteJSONL\n    WriteJSONL --> LogFile\n    LogFile --> LoadFile\n    LoadFile --> ParseLogFile\n    ParseLogFile --> RenderUI\n```\n\nThe visualizer operates entirely on the client side, reading log files generated by `RLMLogger` without requiring any backend services or API connections.\n\n**Sources:** [README.md:129-151](), [visualizer/src/lib/parse-logs.ts:169-180]()\n\n---\n\n## Development and Extension\n\n### Technology Stack\n\n| Component | Technology |\n|-----------|-----------|\n| Framework | Next.js |\n| Language | TypeScript |\n| UI Library | shadcn/ui |\n| Styling | Tailwind CSS |\n| Parsing | Native JSON parsing |\n\n**Sources:** [README.md:142](), [visualizer/src/lib/utils.ts:1-6]()\n\n### Adding Custom Visualizations\n\nTo extend the visualizer with custom views:\n\n1. **Parse Layer:** Extend `computeMetadata()` to extract additional statistics [visualizer/src/lib/parse-logs.ts:121-167]()\n2. **Type Layer:** Add new fields to `LogMetadata` interface [visualizer/src/lib/types.ts:55-63]()\n3. **UI Layer:** Create new React components that consume the extended metadata\n\nThe parsing layer is stateless and deterministic, making it easy to test new extraction logic independently of the UI.\n\n**Sources:** [visualizer/src/lib/parse-logs.ts:121-167](), [visualizer/src/lib/types.ts:55-63]()\n\n---\n\n## Debugging and Troubleshooting\n\n### Common Issues\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Parse errors | Malformed JSONL | Check that each line is valid JSON |\n| Missing metadata | Old log format | Uses default config (all fields `null`) |\n| Incorrect statistics | Incomplete execution | Check that RLM completed successfully |\n| Empty context | No context loaded | Verify context variable was set in REPL |\n\n**Sources:** [visualizer/src/lib/parse-logs.ts:19-30](), [visualizer/src/lib/parse-logs.ts:62-64]()\n\n### Backwards Compatibility\n\nThe parser handles log files generated by older versions of the RLM system:\n\n- **Missing metadata line:** Uses `getDefaultConfig()` to provide null defaults [visualizer/src/lib/parse-logs.ts:19-30]()\n- **Missing iteration_time:** Falls back to summing code block execution times [visualizer/src/lib/parse-logs.ts:132-140]()\n- **Different final_answer formats:** Normalizes via `extractFinalAnswer()` [visualizer/src/lib/types.ts:65-71]()\n\n**Sources:** [visualizer/src/lib/parse-logs.ts:19-30](), [visualizer/src/lib/parse-logs.ts:132-140](), [visualizer/src/lib/types.ts:65-71]()\n\n---\n\n# Page: Development Guide\n\n# Development Guide\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [Makefile](Makefile)\n- [README.md](README.md)\n- [pyproject.toml](pyproject.toml)\n\n</details>\n\n\n\nThis guide covers the development workflow for contributing to the RLM project, including environment setup, code quality standards, testing procedures, and the CI/CD pipeline. For architectural details, see page 3. For extending functionality, see page 8.\n\n---\n\n## Development Environment Setup\n\n### Prerequisites\n\nThe RLM project requires:\n- **Python 3.11+** (tested on 3.11 and 3.12)\n- **uv** package manager for dependency management\n- **Git** for version control\n- **Node.js 18+** (for documentation and visualizer development)\n\n### Initial Setup\n\n**Step 1: Clone the Repository**\n```bash\ngit clone https://github.com/alexzhang13/rlm.git\ncd rlm\n```\n\n**Step 2: Install uv Package Manager**\n\nThe project uses `uv` instead of `pip` for faster, more reliable dependency management:\n\n```bash\n# Install uv via official installer\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n**Sources**: [README.md:49-54]()\n\n---\n\n**Step 3: Set Up Python Environment**\n\n```bash\n# Create virtual environment with Python 3.12 (or 3.11)\nuv venv --python 3.12\n\n# Activate the virtual environment\n# On Linux/macOS:\nsource .venv/bin/activate\n# On Windows:\n.venv\\Scripts\\activate\n```\n\n**Sources**: [README.md:52]()\n\n---\n\n**Step 4: Install Dependencies**\n\nThe project provides a `Makefile` with convenient installation targets:\n\n```bash\n# Install base dependencies only\nmake install\n\n# Install development dependencies (includes ruff, pytest, pre-commit)\nmake install-dev\n\n# Install optional Modal dependencies\nmake install-modal\n```\n\n**Manual Installation** (if not using Makefile):\n```bash\n# Base installation\nuv pip install -e .\n\n# Development dependencies\nuv sync --group dev --group test\n\n# Optional: Modal Sandboxes\nuv pip install -e \".[modal]\"\n\n# Optional: Prime Sandboxes\nuv pip install -e \".[prime]\"\n\n# Optional: Daytona Sandboxes\nuv pip install -e \".[daytona]\"\n```\n\n**Sources**: [README.md:49-54](), [Makefile:26-33](), [pyproject.toml:10-24]()\n\n---\n\n**Step 5: Configure API Keys**\n\nSet environment variables for LLM providers:\n\n```bash\n# Required for basic usage\nexport OPENAI_API_KEY=sk-...\n\n# Optional: Additional providers\nexport ANTHROPIC_API_KEY=...\nexport PORTKEY_API_KEY=...\nexport GEMINI_API_KEY=...\n\n# Optional: Cloud sandbox providers\nexport MODAL_TOKEN_ID=...\nexport MODAL_TOKEN_SECRET=...\nexport PRIME_API_KEY=...\n```\n\nAlternatively, use a `.env` file (supported via `python-dotenv`).\n\n**Sources**: [README.md:61-64](), [pyproject.toml:16]()\n\n---\n\n**Step 6: Verify Installation**\n\nRun the quickstart example to verify everything is working:\n\n```bash\nmake quickstart\n```\n\nThis runs [examples/quickstart.py]() with the OpenAI client and default `LocalREPL` environment.\n\n**Sources**: [README.md:61-64](), [Makefile:37-38]()\n\n---\n\n### Installing Optional Dependencies\n\nThe project supports multiple execution environments that require additional dependencies.\n\n#### Modal Sandboxes\n\n```bash\n# Install Modal SDK\nuv pip install -e \".[modal]\"\n\n# Authenticate with Modal\nmodal setup\n```\n\n**Sources**: [README.md:99-103](), [pyproject.toml:22]()\n\n---\n\n#### Prime Intellect Sandboxes\n\n```bash\n# Install Prime Sandboxes SDK\nuv pip install -e \".[prime]\"\n\n# Set API key\nexport PRIME_API_KEY=...\n```\n\n**Sources**: [README.md:110-114](), [pyproject.toml:24]()\n\n---\n\n#### Daytona Workspaces\n\n```bash\n# Install Daytona SDK\nuv pip install -e \".[daytona]\"\n\n# Configure Daytona credentials (follow SDK documentation)\n```\n\n**Sources**: [pyproject.toml:23]()\n\n---\n\n## Running Examples\n\nThe project includes several examples demonstrating different features and environments.\n\n### Available Examples\n\n```bash\n# Run all examples (requires all dependencies and API keys)\nmake run-all\n\n# Individual examples:\nmake quickstart      # Basic RLM with LocalREPL\nmake docker-repl     # Docker-based execution\nmake lm-repl         # LM calls within REPL (uses Portkey)\nmake modal-repl      # Modal Sandboxes (requires Modal setup)\n```\n\n**Example Details**:\n\n| Target | Script | Environment | Requirements |\n|--------|--------|-------------|--------------|\n| `quickstart` | `examples/quickstart.py` | `LocalREPL` | `OPENAI_API_KEY` |\n| `docker-repl` | `examples/docker_repl_example.py` | `DockerREPL` | Docker daemon |\n| `lm-repl` | `examples/lm_in_repl.py` | `LocalREPL` | `PORTKEY_API_KEY` |\n| `modal-repl` | `examples/modal_repl_example.py` | `ModalREPL` | Modal authentication |\n\n**Sources**: [Makefile:35-47](), [README.md:61-64]()\n\n---\n\n## Code Quality and Testing\n\nThe project uses automated tools to enforce code quality and consistency.\n\n### Code Quality Tools\n\n**Diagram: Code Quality Toolchain**\n\n```mermaid\ngraph TB\n    subgraph \"Quality Tools\"\n        Ruff[\"ruff<br/>Linter + Formatter\"]\n        Pytest[\"pytest<br/>Test Runner\"]\n    end\n    \n    subgraph \"Configuration Files\"\n        PyProject[\"pyproject.toml<br/>[tool.ruff]<br/>[tool.pytest.ini_options]\"]\n    end\n    \n    subgraph \"Execution Contexts\"\n        MakeCheck[\"make check\"]\n        GitHubActions[\"GitHub Actions<br/>style.yml, test.yml\"]\n    end\n    \n    PyProject -->|\"line-length: 100<br/>target-version: py311\"| Ruff\n    PyProject -->|\"testpaths: tests/\"| Pytest\n    \n    Ruff --> MakeCheck\n    Pytest --> MakeCheck\n    \n    Ruff --> GitHubActions\n    Pytest --> GitHubActions\n```\n\n**Sources**: [pyproject.toml:45-66](), [Makefile:49-58]()\n\n---\n\n### Ruff: Linter and Formatter\n\n**Ruff** is a fast Python linter and formatter that replaces multiple tools (flake8, isort, black).\n\n**Configuration** (from [pyproject.toml:45-64]()):\n```toml\n[tool.ruff]\nline-length = 100\ntarget-version = \"py311\"\n\n[tool.ruff.lint]\nselect = [\n    \"E\",   # pycodestyle errors\n    \"W\",   # pycodestyle warnings\n    \"F\",   # pyflakes\n    \"I\",   # isort\n    \"B\",   # flake8-bugbear\n    \"UP\",  # pyupgrade\n]\nignore = [\n    \"E501\",  # line too long (handled by formatter)\n]\n\n[tool.ruff.format]\nquote-style = \"double\"\n```\n\n**Running Locally**:\n```bash\n# Run linter\nmake lint\n# or\nuv run ruff check .\n\n# Run formatter\nmake format\n# or\nuv run ruff format .\n```\n\n**Sources**: [pyproject.toml:45-64](), [Makefile:49-53]()\n\n---\n\n### Pytest: Test Suite\n\n**Test Structure**:\n\nThe test suite is located in the `tests/` directory with the following organization:\n\n```\ntests/\n├── test_imports.py          # Basic import tests\n├── core/                    # Core RLM functionality tests\n├── clients/                 # LLM client backend tests\n├── environments/            # Execution environment tests\n└── logger/                  # Logging system tests\n```\n\n**Running Tests**:\n```bash\n# Run all tests\nmake test\n# or\nuv run pytest\n\n# Run with coverage\nuv run pytest --cov=rlm --cov-report=html\n\n# Run specific test file\nuv run pytest tests/test_imports.py -v\n```\n\n**Pytest Configuration** (from [pyproject.toml:65-66]()):\n```toml\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\n```\n\n**Sources**: [pyproject.toml:65-66](), [Makefile:55-56]()\n\n---\n\n### Combined Quality Check\n\nThe `make check` target runs all quality checks in sequence:\n\n```bash\nmake check\n```\n\nThis runs:\n1. `ruff check .` (linting)\n2. `ruff format .` (formatting)\n3. `pytest` (tests)\n\nUse this before committing or opening a pull request.\n\n**Sources**: [Makefile:58]()\n\n---\n\n## CI/CD Pipeline\n\nThe project uses GitHub Actions for continuous integration and deployment.\n\n### CI Workflow Overview\n\n**Diagram: GitHub Actions Pipeline**\n\n```mermaid\ngraph TB\n    subgraph \"Trigger Events\"\n        Push[\"Push to main\"]\n        PR[\"Pull Request\"]\n    end\n    \n    subgraph \"Style Workflow (.github/workflows/style.yml)\"\n        StyleSetup[\"Setup Python 3.11<br/>Install uv and deps\"]\n        StyleLint[\"ruff check .\"]\n        StyleFormat[\"ruff format --check .\"]\n    end\n    \n    subgraph \"Test Workflow (.github/workflows/test.yml)\"\n        TestMatrix[\"Matrix: Python 3.11, 3.12\"]\n        TestSetup[\"Setup Python<br/>Install uv and deps\"]\n        TestRun[\"pytest tests/<br/>--cov=rlm\"]\n    end\n    \n    subgraph \"Docs Workflow (.github/workflows/docs.yml)\"\n        DocsBuild[\"Build Next.js site<br/>npm run build\"]\n        DocsDeploy[\"Deploy to GitHub Pages\"]\n    end\n    \n    Push --> StyleSetup\n    Push --> TestMatrix\n    Push --> DocsBuild\n    \n    PR --> StyleSetup\n    PR --> TestMatrix\n    \n    StyleSetup --> StyleLint\n    StyleLint --> StyleFormat\n    \n    TestMatrix --> TestSetup\n    TestSetup --> TestRun\n    \n    DocsBuild --> DocsDeploy\n```\n\n**Sources**: [README.md:16-21]()\n\n---\n\n### Style Workflow\n\n**File**: `.github/workflows/style.yml`\n\n**Purpose**: Enforce code style consistency using `ruff`.\n\n**Triggers**:\n- Push to `main` branch\n- Pull requests\n\n**Steps**:\n1. Checkout repository\n2. Set up Python 3.11\n3. Install `uv` package manager\n4. Install project dependencies\n5. Run `ruff check .`\n6. Run `ruff format --check .`\n\n**Local Simulation**:\n```bash\n# Run the same checks locally\nmake lint\nmake format\n```\n\n**Sources**: [README.md:16-18]()\n\n---\n\n### Test Workflow\n\n**File**: `.github/workflows/test.yml`\n\n**Purpose**: Run test suite across multiple Python versions.\n\n**Triggers**:\n- Push to `main` branch\n- Pull requests\n\n**Matrix Strategy**:\n- Tests run on Python 3.11 and 3.12\n- Ensures compatibility across versions\n\n**Steps**:\n1. Checkout repository\n2. Set up Python (matrix version)\n3. Install `uv` package manager\n4. Install project dependencies\n5. Run `pytest tests/ --cov=rlm --cov-report=term`\n\n**Local Simulation**:\n```bash\n# Run the same checks locally\nmake test\n```\n\n**Sources**: [README.md:19-21]()\n\n---\n\n## Documentation System\n\nThe RLM project includes two separate documentation systems: the main documentation site and the trajectory visualizer.\n\n### Documentation Site Architecture\n\n**Diagram: Documentation Build and Deployment**\n\n```mermaid\ngraph TB\n    subgraph \"Source Files\"\n        DocsSrc[\"docs/src/<br/>Next.js app<br/>TypeScript + React\"]\n        DocsComponents[\"components/<br/>Sidebar, PageLayout\"]\n        DocsPages[\"app/page.tsx<br/>Static content\"]\n    end\n    \n    subgraph \"Build Configuration\"\n        NextConfig[\"next.config.js<br/>output: export<br/>basePath: /rlm\"]\n        PackageJSON[\"package.json<br/>Dependencies\"]\n    end\n    \n    subgraph \"Build Process\"\n        NPMInstall[\"npm ci<br/>Install dependencies\"]\n        NPMBuild[\"npm run build<br/>Generate static HTML\"]\n        OutDir[\"docs/out/<br/>Static site output\"]\n    end\n    \n    subgraph \"Deployment\"\n        GitHubAction[\".github/workflows/docs.yml<br/>On push to main\"]\n        GitHubPages[\"GitHub Pages<br/>alexzhang13.github.io/rlm\"]\n    end\n    \n    DocsSrc --> NPMBuild\n    DocsComponents --> NPMBuild\n    DocsPages --> NPMBuild\n    NextConfig --> NPMBuild\n    PackageJSON --> NPMInstall\n    \n    NPMInstall --> NPMBuild\n    NPMBuild --> OutDir\n    \n    OutDir --> GitHubAction\n    GitHubAction --> GitHubPages\n```\n\n**Sources**: [README.md:9-10]()\n\n---\n\n### Documentation Site Structure\n\nThe documentation is a standalone Next.js application:\n\n```\ndocs/\n├── src/\n│   ├── app/\n│   │   ├── page.tsx              # Homepage\n│   │   ├── layout.tsx            # Root layout\n│   │   └── [section]/            # Dynamic routes\n│   ├── components/\n│   │   ├── Sidebar.tsx           # Navigation sidebar\n│   │   └── PageLayout.tsx        # Page wrapper\n│   └── lib/\n│       └── utils.ts              # Utility functions\n├── public/\n│   └── assets/                   # Static assets\n├── next.config.js                # Next.js configuration\n├── package.json                  # Dependencies\n└── out/                          # Build output (gitignored)\n```\n\n**Key Configuration** (`next.config.js`):\n- `output: \"export\"` - Generates static HTML\n- `basePath: \"/rlm\"` - Sets base path for GitHub Pages\n\n**Sources**: [README.md:9]()\n\n---\n\n### Building Documentation Locally\n\n**Prerequisites**:\n```bash\n# Install Node.js 18+ if not already installed\nnode --version  # Should be 18.0.0 or higher\n```\n\n**Build Steps**:\n```bash\n# Navigate to docs directory\ncd docs/\n\n# Install dependencies\nnpm ci\n\n# Development mode (with hot reload)\nnpm run dev\n\n# Production build\nnpm run build\n\n# Preview production build\nnpm run start\n```\n\nThe development server runs on `http://localhost:3000/rlm` (note the `/rlm` base path).\n\n**Sources**: [README.md:9]()\n\n---\n\n### Visualizer System\n\nThe visualizer is a separate Next.js application for viewing RLM execution trajectories.\n\n**Diagram: Visualizer Architecture**\n\n```mermaid\ngraph TB\n    subgraph \"Log Generation\"\n        RLMLogger[\"RLMLogger<br/>rlm/logger/rlm_logger.py\"]\n        LogFiles[\"*.jsonl files<br/>Execution traces\"]\n        RLMLogger -->|\"writes\"| LogFiles\n    end\n    \n    subgraph \"Visualizer Source\"\n        VisualizerSrc[\"visualizer/src/<br/>TypeScript + React\"]\n        ParseLogs[\"parse-logs.ts<br/>JSONL parser\"]\n        Types[\"types.ts<br/>RLMLogFile<br/>RLMIteration<br/>CodeBlock\"]\n        Components[\"components/<br/>shadcn/ui\"]\n    end\n    \n    subgraph \"Build and Run\"\n        NPMDev[\"npm run dev<br/>localhost:3001\"]\n        NPMBuild[\"npm run build<br/>Static export\"]\n    end\n    \n    LogFiles -->|\"loaded by\"| ParseLogs\n    ParseLogs --> Types\n    Types --> Components\n    \n    VisualizerSrc --> NPMDev\n    VisualizerSrc --> NPMBuild\n```\n\n**Sources**: [README.md:138-160]()\n\n---\n\n### Visualizer Structure\n\n```\nvisualizer/\n├── src/\n│   ├── app/\n│   │   └── page.tsx              # Main visualizer page\n│   ├── components/\n│   │   ├── ui/                   # shadcn/ui components\n│   │   ├── LogViewer.tsx         # Log file viewer\n│   │   └── IterationTree.tsx     # Iteration hierarchy\n│   ├── lib/\n│   │   ├── parse-logs.ts         # JSONL parsing logic\n│   │   └── types.ts              # TypeScript type definitions\n│   └── styles/\n│       └── globals.css           # Global styles\n├── public/\n│   └── logs/                     # Example log files\n├── next.config.js                # Next.js configuration\n├── package.json                  # Dependencies (includes shadcn/ui)\n└── out/                          # Build output (gitignored)\n```\n\n---\n\n### Running the Visualizer\n\n**Step 1: Generate Log Files**\n\nUse `RLMLogger` in your Python code:\n```python\nfrom rlm.logger import RLMLogger\nfrom rlm import RLM\n\nlogger = RLMLogger(log_dir=\"./logs\")\nrlm = RLM(\n    backend=\"openai\",\n    backend_kwargs={\"model_name\": \"gpt-4\"},\n    logger=logger,\n    verbose=True\n)\n\nresult = rlm.completion(\"Your query here\")\n# Creates ./logs/<timestamp>.jsonl\n```\n\n**Step 2: Run Visualizer**\n```bash\ncd visualizer/\n\n# Install dependencies (first time only)\nnpm install\n\n# Run development server\nnpm run dev\n```\n\nThe visualizer opens at `http://localhost:3001` and allows you to:\n- Select and load `.jsonl` log files\n- View iteration-by-iteration execution\n- Inspect code blocks and sub-LM calls\n- See stdout/stderr output\n- Track token usage and execution times\n\n**Sources**: [README.md:138-160]()\n\n---\n\n### Visualizer Log Format\n\nThe visualizer parses `.jsonl` files with the following structure (one JSON object per line):\n\n```typescript\n// RLMLogFile structure\n{\n  metadata: {\n    root_model: string;\n    max_depth: number;\n    max_iterations: number;\n    backend: string;\n    environment_type: string;\n  };\n  iterations: [\n    {\n      prompt: string;\n      response: string;\n      code_blocks: [\n        {\n          code: string;\n          result: {\n            stdout: string;\n            stderr: string;\n            execution_time: number;\n            rlm_calls: [...];  // Nested RLMChatCompletion objects\n          }\n        }\n      ];\n      final_answer: string | null;\n      iteration_time: number;\n    }\n  ];\n  final_answer: string;\n  total_time: number;\n}\n```\n\nThe parser is implemented in `visualizer/src/lib/parse-logs.ts`.\n\n**Sources**: [README.md:138-160]()\n\n---\n\n## Development Best Practices\n\n### Code Style Guidelines\n\nThe project follows consistent naming conventions:\n\n| Element | Convention | Examples |\n|---------|-----------|---------|\n| Classes | PascalCase | `RLM`, `LMHandler`, `LocalREPL`, `BaseLM`, `BaseEnv` |\n| Functions | snake_case | `completion()`, `execute_code()`, `get_client()`, `get_environment()` |\n| Variables | snake_case | `max_iterations`, `model_name`, `repl_env` |\n| Constants | UPPER_SNAKE_CASE | `DEFAULT_TIMEOUT`, `MAX_RETRIES`, `_SAFE_BUILTINS` |\n| Private methods | Leading underscore | `_send_request()`, `_parse_response()`, `_execute()` |\n| Protected attributes | Leading underscore | `_depth`, `_iteration_count` |\n\n**Sources**: [pyproject.toml:45-64]()\n\n---\n\n### Import Organization\n\nFollow PEP 8 import ordering enforced by `ruff`:\n\n1. Standard library imports\n2. Third-party imports\n3. Local application imports\n\nExample:\n```python\n# Standard library\nimport asyncio\nimport os\nfrom typing import Optional, Any\n\n# Third-party\nfrom anthropic import Anthropic\nfrom openai import OpenAI\nfrom rich.console import Console\n\n# Local\nfrom rlm.core.rlm import RLM\nfrom rlm.core.types import RLMChatCompletion, ClientBackend\nfrom rlm.clients.base import BaseLM\nfrom rlm.environments.base import BaseEnv\n```\n\nThe `ruff` linter will automatically organize imports when you run `make format`.\n\n**Sources**: [pyproject.toml:50-57]()\n\n---\n\n### Project Structure\n\n**Diagram: Source Code Organization**\n\n```mermaid\ngraph TB\n    subgraph \"rlm/ Package Root\"\n        Init[\"__init__.py<br/>Exports: RLM, ClientBackend, EnvironmentType\"]\n    end\n    \n    subgraph \"rlm/core/\"\n        RLMClass[\"rlm.py<br/>RLM orchestrator class\"]\n        Types[\"types.py<br/>Core type definitions\"]\n        Handler[\"lm_handler.py<br/>LMHandler TCP server\"]\n        CommsUtils[\"comms_utils.py<br/>Socket/HTTP protocols\"]\n    end\n    \n    subgraph \"rlm/clients/\"\n        BaseClient[\"base.py<br/>BaseLM interface\"]\n        GetClient[\"__init__.py<br/>get_client() factory\"]\n        OpenAIClient[\"openai.py<br/>OpenAIClient\"]\n        AnthropicClient[\"anthropic.py<br/>AnthropicClient\"]\n        OtherClients[\"portkey.py<br/>gemini.py<br/>litellm.py<br/>azure.py\"]\n    end\n    \n    subgraph \"rlm/environments/\"\n        BaseEnv[\"base.py<br/>BaseEnv, IsolatedEnv, NonIsolatedEnv\"]\n        GetEnv[\"__init__.py<br/>get_environment() factory\"]\n        LocalREPL[\"local_repl.py<br/>LocalREPL\"]\n        DockerREPL[\"docker_repl.py<br/>DockerREPL\"]\n        CloudREPLs[\"modal_repl.py<br/>prime_repl.py<br/>daytona_repl.py\"]\n    end\n    \n    subgraph \"rlm/logger/\"\n        RLMLogger[\"rlm_logger.py<br/>RLMLogger\"]\n        VerbosePrinter[\"verbose_printer.py<br/>VerbosePrinter\"]\n    end\n    \n    RLMClass --> Handler\n    RLMClass --> GetClient\n    RLMClass --> GetEnv\n    RLMClass --> Types\n    \n    Handler --> CommsUtils\n    Handler --> BaseClient\n    \n    BaseEnv --> Handler\n    BaseEnv --> CommsUtils\n    \n    GetClient --> BaseClient\n    GetEnv --> BaseEnv\n```\n\n**Sources**: [README.md:30-77]()\n\n---\n\n### Making Changes\n\n**Before Starting**:\n1. Create a feature branch: `git checkout -b feature/your-feature`\n2. Install development dependencies: `make install-dev`\n3. Review relevant documentation:\n   - Architecture overview (page 3)\n   - Specific component documentation (pages 4-5)\n   - API reference (page 9)\n\n**During Development**:\n1. Write tests for new functionality\n2. Run quality checks frequently: `make check`\n3. Commit with descriptive messages\n4. Keep changes focused and atomic\n\n**Before Submitting PR**:\n1. Run full test suite: `make test`\n2. Verify code style: `make lint` and `make format`\n3. Test with examples: `make run-all` (if applicable)\n4. Update documentation if adding public APIs\n5. Ensure all CI checks will pass\n\n**Sources**: [Makefile:1-59](), [pyproject.toml:45-66]()\n\n---\n\n## Troubleshooting\n\n### Installation Issues\n\n**Problem**: `uv` command not found\n```bash\n# Reinstall uv\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Add to PATH (may require shell restart)\nsource $HOME/.cargo/env\n```\n\n**Problem**: Python version mismatch\n```bash\n# Specify Python version explicitly\nuv venv --python 3.12  # or 3.11\n\n# Verify Python version\npython --version\n```\n\n**Problem**: Dependencies fail to install\n```bash\n# Clear uv cache\nuv cache clean\n\n# Reinstall from scratch\nrm -rf .venv/\nuv venv --python 3.12\nuv pip install -e .\n```\n\n**Sources**: [README.md:49-54]()\n\n---\n\n### Example Execution Issues\n\n**Problem**: `make quickstart` fails with API error\n```bash\n# Verify API key is set\necho $OPENAI_API_KEY\n\n# Test API key manually\ncurl https://api.openai.com/v1/models \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\"\n```\n\n**Problem**: Docker example fails\n```bash\n# Ensure Docker daemon is running\ndocker ps\n\n# Pull required image\ndocker pull python:3.11-slim\n\n# Check Docker permissions (Linux)\nsudo usermod -aG docker $USER\nnewgrp docker\n```\n\n**Problem**: Modal example fails\n```bash\n# Verify Modal authentication\nmodal token show\n\n# Re-authenticate if needed\nmodal setup\n```\n\n**Sources**: [README.md:61-103](), [Makefile:37-47]()\n\n---\n\n### Code Quality Issues\n\n**Problem**: `ruff check` reports errors\n```bash\n# Auto-fix most issues\nruff check --fix .\n\n# Review remaining issues\nruff check .\n```\n\n**Problem**: Formatting conflicts\n```bash\n# Format code to project standard\nruff format .\n\n# Verify formatting\nruff format --check .\n```\n\n**Problem**: Tests fail locally\n```bash\n# Reinstall in editable mode\nuv pip install -e .\n\n# Run tests with verbose output\nuv run pytest -v\n\n# Run specific failing test\nuv run pytest tests/test_imports.py::test_specific -v\n```\n\n**Sources**: [Makefile:49-58](), [pyproject.toml:45-66]()\n\n---\n\n### CI/CD Issues\n\n**Problem**: CI passes locally but fails on GitHub\n- Verify Python version (3.11 or 3.12)\n- Check all dependencies are in `pyproject.toml`\n- Review GitHub Actions logs for specific errors\n- Test with same Python version: `uv venv --python 3.11`\n\n**Problem**: Documentation build fails\n```bash\n# Test docs build locally\ncd docs/\nnpm ci\nnpm run build\n\n# Check for TypeScript errors\nnpm run type-check\n```\n\n**Problem**: Style workflow fails\n```bash\n# Run exact CI commands\nruff check .\nruff format --check .\n\n# Fix issues\nruff check --fix .\nruff format .\n```\n\n**Sources**: [README.md:16-21]()\n\n---\n\n### Visualizer Issues\n\n**Problem**: Visualizer won't start\n```bash\n# Verify Node.js version\nnode --version  # Should be 18+\n\n# Reinstall dependencies\ncd visualizer/\nrm -rf node_modules/\nnpm install\nnpm run dev\n```\n\n**Problem**: Log files not loading\n- Verify `.jsonl` file format is valid JSON\n- Check file is in accessible location\n- Confirm file was generated by `RLMLogger`\n\n**Problem**: Build fails\n```bash\n# Check for TypeScript errors\nnpm run type-check\n\n# Clean build cache\nrm -rf .next/\nnpm run build\n```\n\n**Sources**: [README.md:151-156]()\n\n---\n\n## Project Structure\n\n### Directory Organization\n\nThe codebase follows a flat, modular structure with clear separation between core logic, utilities, and examples.\n\n```mermaid\ngraph TB\n    subgraph \"Root Directory\"\n        Main[\"main.py<br/>(Needle-in-Haystack Example)\"]\n        Req[\"requirements.txt<br/>(Dependencies)\"]\n        Env[\".env<br/>(API Configuration)\"]\n        License[\"LICENSE<br/>(MIT License)\"]\n    end\n    \n    subgraph \"rlm/ Package\"\n        Init[\"__init__.py<br/>(Package Exports)\"]\n        RLMBase[\"rlm.py<br/>(RLM Abstract Base)\"]\n        RLMImpl[\"rlm_repl.py<br/>(RLM_REPL Class)\"]\n        REPLMod[\"repl.py<br/>(REPLEnv, Sub_RLM)\"]\n    end\n    \n    subgraph \"rlm/utils/ Utilities\"\n        LLMUtil[\"llm.py<br/>(OpenAIClient)\"]\n        PromptUtil[\"prompts.py<br/>(Prompt Templates)\"]\n        UtilsUtil[\"utils.py<br/>(Code Parsing)\"]\n    end\n    \n    subgraph \"rlm/logger/ Logging\"\n        RootLog[\"root_logger.py<br/>(ColorfulLogger)\"]\n        REPLLog[\"repl_logger.py<br/>(REPLEnvLogger)\"]\n    end\n    \n    Main --> RLMImpl\n    RLMImpl --> REPLMod\n    RLMImpl --> LLMUtil\n    RLMImpl --> PromptUtil\n    RLMImpl --> UtilsUtil\n    RLMImpl --> RootLog\n    \n    REPLMod --> REPLLog\n    \n    Init --> RLMBase\n    Init --> RLMImpl\n    Init --> REPLMod\n```\n\n**Directory Structure with File Purposes**\n\n```\nrlm/\n├── __init__.py              # Package initialization, exports RLM, RLM_REPL, REPLEnv\n├── rlm.py                   # Abstract base class defining RLM interface\n├── rlm_repl.py              # Primary RLM implementation with REPL environment\n├── repl.py                  # REPL execution environment and Sub_RLM for depth=1\n├── utils/\n│   ├── __init__.py          # (Empty, package marker)\n│   ├── llm.py               # OpenAIClient wrapper for API calls\n│   ├── prompts.py           # REPL_SYSTEM_PROMPT and next_action_prompt templates\n│   └── utils.py             # Code block parsing, result formatting, response processing\n└── logger/\n    ├── __init__.py          # (Empty, package marker)\n    ├── root_logger.py       # ColorfulLogger for root-level query/response tracking\n    └── repl_logger.py       # REPLEnvLogger for detailed REPL execution history\n```\n\n**Sources**: [README.md:13-32](), [.gitignore:1-211]()\n\n---\n\n## Core Class Hierarchy\n\n### Class Inheritance and Responsibilities\n\nThe system uses a three-tier class hierarchy with a shared abstract base class.\n\n```mermaid\nclassDiagram\n    class RLM {\n        <<abstract>>\n        +completion(messages) str*\n        +cost_summary() str*\n        +reset()*\n    }\n    \n    class RLM_REPL {\n        -model: str\n        -recursive_model: str\n        -max_iterations: int\n        -repl_env: REPLEnv\n        -logger: ColorfulLogger\n        +__init__(model, recursive_model, max_iterations)\n        +setup_context(context, query)\n        +completion(messages) str\n        +cost_summary() str\n        +reset()\n    }\n    \n    class Sub_RLM {\n        -model: str\n        +__init__(model)\n        +completion(messages) str\n        +cost_summary() str\n        +reset()\n    }\n    \n    class REPLEnv {\n        -context: str\n        -sub_rlm: Sub_RLM\n        -logger: REPLEnvLogger\n        -namespace: dict\n        +__init__(context, recursive_model)\n        +code_execution(code) tuple\n        -llm_query(query) str\n    }\n    \n    RLM <|-- RLM_REPL : \"inherits\"\n    RLM <|-- Sub_RLM : \"inherits\"\n    RLM_REPL *-- REPLEnv : \"contains\"\n    REPLEnv *-- Sub_RLM : \"contains\"\n```\n\n### Class Responsibility Table\n\n| Class | File | Primary Responsibility | Key Methods |\n|-------|------|----------------------|-------------|\n| `RLM` | [rlm/rlm.py]() | Define abstract interface for all RLM implementations | `completion()`, `cost_summary()`, `reset()` |\n| `RLM_REPL` | [rlm/rlm_repl.py]() | Root-level RLM orchestration with iterative processing loop | `setup_context()`, `completion()` with iteration logic |\n| `Sub_RLM` | [rlm/repl.py]() | Depth=1 recursive calls from REPL environment | Simple `completion()` wrapper around OpenAI API |\n| `REPLEnv` | [rlm/repl.py]() | Sandboxed Python execution environment with `llm_query()` | `code_execution()`, provides `llm_query()` function |\n| `OpenAIClient` | [rlm/utils/llm.py]() | Wrapper for OpenAI API calls | `completion()` with error handling |\n\n**Sources**: [rlm/rlm.py](), [rlm/rlm_repl.py](), [rlm/repl.py]()\n\n---\n\n## Code Organization Patterns\n\n### Module Dependency Graph\n\nUnderstanding how modules depend on each other is essential for making modifications without breaking the system.\n\n```mermaid\ngraph LR\n    subgraph \"Entry Layer\"\n        MainPy[\"main.py\"]\n    end\n    \n    subgraph \"Core Layer\"\n        RLMBase[\"rlm/rlm.py<br/>RLM base class\"]\n        RLMREPLMod[\"rlm/rlm_repl.py<br/>RLM_REPL class\"]\n        REPLMod[\"rlm/repl.py<br/>REPLEnv, Sub_RLM\"]\n    end\n    \n    subgraph \"Utilities Layer\"\n        LLMClient[\"rlm/utils/llm.py<br/>OpenAIClient\"]\n        Prompts[\"rlm/utils/prompts.py<br/>Templates\"]\n        Utils[\"rlm/utils/utils.py<br/>Parsing & Formatting\"]\n    end\n    \n    subgraph \"Logging Layer\"\n        RootLogger[\"rlm/logger/root_logger.py<br/>ColorfulLogger\"]\n        REPLLogger[\"rlm/logger/repl_logger.py<br/>REPLEnvLogger\"]\n    end\n    \n    subgraph \"External\"\n        OpenAI[\"OpenAI API\"]\n        DotEnv[\".env file\"]\n    end\n    \n    MainPy --> RLMREPLMod\n    \n    RLMREPLMod -.->|inherits| RLMBase\n    REPLMod -.->|Sub_RLM inherits| RLMBase\n    \n    RLMREPLMod --> REPLMod\n    RLMREPLMod --> LLMClient\n    RLMREPLMod --> Prompts\n    RLMREPLMod --> Utils\n    RLMREPLMod --> RootLogger\n    \n    REPLMod --> LLMClient\n    REPLMod --> REPLLogger\n    \n    Utils --> REPLMod\n    Utils --> RootLogger\n    Utils --> REPLLogger\n    \n    LLMClient --> OpenAI\n    LLMClient -.->|reads| DotEnv\n```\n\n### Layered Architecture\n\nThe system follows a clear layered architecture:\n\n1. **Entry Layer**: Example scripts demonstrating usage ([main.py]())\n2. **Core Layer**: RLM implementations and REPL environment\n3. **Utilities Layer**: Reusable components for API calls, prompts, and parsing\n4. **Logging Layer**: Optional observability components\n5. **External Layer**: OpenAI API and environment configuration\n\n**Dependency Rules**:\n- Upper layers depend on lower layers (entry → core → utilities → external)\n- Utilities may call back to core for execution (circular: `utils.py` → `REPLEnv.code_execution()`)\n- Logging is injected into core components but remains optional\n\n**Sources**: [rlm/__init__.py](), [rlm/rlm_repl.py](), [rlm/repl.py]()\n\n---\n\n## Key Design Patterns\n\n### Pattern 1: Abstract Base Class Interface\n\nThe `RLM` abstract base class defines the contract all implementations must follow.\n\n**Location**: [rlm/rlm.py]()\n\n**Pattern Benefits**:\n- Enforces consistent interface across implementations\n- Enables polymorphic usage (replace `Sub_RLM` with `RLM_REPL`)\n- Simplifies testing with mock implementations\n\n**Contract Methods**:\n```python\n# All RLM implementations must provide:\ndef completion(self, messages: list) -> str\ndef cost_summary(self) -> str  \ndef reset(self) -> None\n```\n\n### Pattern 2: Dependency Injection\n\nThe `RLM_REPL` class accepts model names and configuration as constructor parameters, allowing flexible model selection.\n\n**Location**: [rlm/rlm_repl.py]()\n\n**Key Parameters**:\n| Parameter | Type | Purpose | Default |\n|-----------|------|---------|---------|\n| `model` | `str` | Root LLM model name | Required |\n| `recursive_model` | `str` | Model for sub-queries | Required |\n| `max_iterations` | `int` | Iteration limit | 20 |\n\n**Benefits**:\n- Easy to test with different model combinations\n- No hardcoded model dependencies\n- Simple configuration management\n\n### Pattern 3: Namespace Isolation\n\nThe `REPLEnv` uses a controlled namespace for code execution.\n\n**Location**: [rlm/repl.py]()\n\n**Implementation Strategy**:\n- Create isolated `namespace` dictionary for each REPL instance\n- Inject only necessary globals: `context`, `llm_query()`\n- Execute code with `exec(code, namespace, namespace)`\n- Extract results from `namespace` after execution\n\n**Security Note**: This provides minimal isolation. For production use, consider proper sandboxing (containers, separate processes).\n\n### Pattern 4: Iterative Refinement with Forced Termination\n\nThe main processing loop in `RLM_REPL.completion()` combines iteration with failsafe termination.\n\n**Location**: [rlm/rlm_repl.py]()\n\n**Control Flow**:\n1. Loop up to `max_iterations` times\n2. Each iteration: query LLM → parse response → execute code → check for final answer\n3. If final answer found: return immediately\n4. If `max_iterations` reached: force final answer with special prompt\n\n**Benefits**:\n- Prevents infinite loops\n- Guarantees response even if LLM doesn't naturally conclude\n- Adaptive prompting based on iteration state\n\n**Sources**: [rlm/rlm.py](), [rlm/rlm_repl.py](), [rlm/repl.py]()\n\n---\n\n## Development Workflow\n\n### Setting Up Development Environment\n\n**Prerequisites**:\n- Python 3.8+\n- OpenAI API key\n\n**Steps**:\n\n1. **Clone Repository** (if applicable)\n2. **Install Dependencies**:\n   ```bash\n   pip install -r requirements.txt\n   ```\n   Core dependencies: `openai`, `python-dotenv`, `rich` (optional)\n\n3. **Configure Environment**:\n   Create `.env` file with:\n   ```\n   OPENAI_API_KEY=your_api_key_here\n   ```\n\n4. **Verify Installation**:\n   ```bash\n   python main.py\n   ```\n\n**Sources**: [README.md:13-32](), [requirements.txt]()\n\n---\n\n### Making Modifications\n\nThe codebase is designed for easy modification. Common extension points:\n\n#### Extension Point 1: Custom Prompts\n\n**File**: [rlm/utils/prompts.py]()\n\nModify `REPL_SYSTEM_PROMPT` or `next_action_prompt()` to change LLM behavior:\n- Add new capabilities to system prompt\n- Change iteration-specific instructions\n- Modify expected response format\n\n#### Extension Point 2: Different LLM Providers\n\n**File**: [rlm/utils/llm.py]()\n\nReplace `OpenAIClient` with custom client:\n- Implement same interface: `completion(messages) -> str`\n- Update imports in [rlm/rlm_repl.py]() and [rlm/repl.py]()\n- Maintain message format or add translation layer\n\n#### Extension Point 3: Enhanced REPL Environment\n\n**File**: [rlm/repl.py]()\n\nExtend `REPLEnv` with additional capabilities:\n- Add more built-in functions to namespace\n- Implement state persistence across iterations\n- Add execution timeout mechanisms\n- Integrate external tools or APIs\n\n#### Extension Point 4: Deeper Recursion\n\n**File**: [rlm/repl.py]()\n\nReplace `Sub_RLM` with `RLM_REPL` for arbitrary recursion depth:\n```python\n# Current (depth=1):\nself.sub_rlm = Sub_RLM(recursive_model)\n\n# Extended (arbitrary depth):\nself.sub_rlm = RLM_REPL(\n    model=recursive_model,\n    recursive_model=recursive_model,\n    max_iterations=max_iterations\n)\n```\n\n**Note**: This requires careful handling of nested REPL environments. See [Extending to Deeper Recursion](#5.1).\n\n#### Extension Point 5: Custom RLM Implementations\n\n**File**: [rlm/rlm.py]()\n\nCreate new RLM implementation by extending `RLM` base class:\n- Implement required abstract methods\n- Add custom processing logic\n- Register in [rlm/__init__.py]() for easy import\n\n**Sources**: [rlm/utils/prompts.py](), [rlm/utils/llm.py](), [rlm/repl.py]()\n\n---\n\n## Testing Considerations\n\nThe minimal implementation does not include a formal test suite, but provides clear testing strategies:\n\n### Unit Testing Strategy\n\n| Component | Test Focus | Key Files |\n|-----------|-----------|-----------|\n| `OpenAIClient` | API response handling, error cases | [rlm/utils/llm.py]() |\n| `REPLEnv` | Code execution, namespace isolation | [rlm/repl.py]() |\n| Code parsing | Block extraction, format detection | [rlm/utils/utils.py]() |\n| Prompt generation | Template rendering, iteration logic | [rlm/utils/prompts.py]() |\n\n### Integration Testing Strategy\n\nTest complete query flows:\n1. Simple query without recursion\n2. Query requiring `llm_query()` sub-calls\n3. Multi-iteration reasoning tasks\n4. Edge cases (max iterations, empty responses)\n\n### Example Test Pattern\n\n```python\n# Pseudo-code for testing RLM_REPL\ndef test_simple_query():\n    rlm = RLM_REPL(\n        model=\"gpt-5-nano\",\n        recursive_model=\"gpt-5\",\n        max_iterations=5\n    )\n    rlm.setup_context(context=\"The answer is 42\", query=\"What is the answer?\")\n    \n    result = rlm.completion([])\n    assert \"42\" in result\n```\n\n### Manual Testing with main.py\n\nThe [main.py]() example serves as an integration test:\n- Tests context handling (1M lines)\n- Tests recursive sub-queries\n- Tests iteration logic\n- Provides visual confirmation via logging\n\n**Sources**: [main.py](), [README.md:13-32]()\n\n---\n\n## Dependency Management\n\n### Core Dependencies\n\n```mermaid\ngraph TB\n    subgraph \"Required Dependencies\"\n        OpenAI[\"openai<br/>(LLM API client)\"]\n        DotEnv[\"python-dotenv<br/>(.env loading)\"]\n    end\n    \n    subgraph \"Optional Dependencies\"\n        Rich[\"rich<br/>(Enhanced logging)\"]\n    end\n    \n    subgraph \"Standard Library\"\n        RE[\"re<br/>(Regex parsing)\"]\n        ABC[\"abc<br/>(Abstract classes)\"]\n        OS[\"os<br/>(Environment vars)\"]\n    end\n    \n    RLMCode[\"RLM Codebase\"]\n    \n    RLMCode --> OpenAI\n    RLMCode --> DotEnv\n    RLMCode -.->|optional| Rich\n    RLMCode --> RE\n    RLMCode --> ABC\n    RLMCode --> OS\n```\n\n### Dependency Table\n\n| Package | Version Constraint | Usage | Can Remove? |\n|---------|-------------------|-------|-------------|\n| `openai` | Latest | API calls to GPT models | No (or replace with alternative) |\n| `python-dotenv` | Latest | Load `.env` configuration | Yes (hardcode API key instead) |\n| `rich` | Latest | Colorful console logging | Yes (basic `print()` fallback) |\n\n### Minimal Configuration\n\nTo run with absolute minimal dependencies:\n1. Remove `rich` import from [rlm/logger/root_logger.py]() and [rlm/logger/repl_logger.py]()\n2. Replace logger calls with `print()` statements\n3. Remove `python-dotenv` and set `OPENAI_API_KEY` environment variable directly\n\n**Sources**: [requirements.txt](), [README.md:14-28]()\n\n---\n\n## Logging Architecture\n\nThe logging system is **optional** but provides valuable observability during development.\n\n### Dual Logger System\n\n```mermaid\ngraph TB\n    subgraph \"Root Level\"\n        RLM_REPL[\"RLM_REPL\"]\n        ColorfulLogger[\"ColorfulLogger<br/>(root_logger.py)\"]\n        RLM_REPL --> ColorfulLogger\n    end\n    \n    subgraph \"REPL Level\"\n        REPLEnv[\"REPLEnv\"]\n        REPLEnvLogger[\"REPLEnvLogger<br/>(repl_logger.py)\"]\n        REPLEnv --> REPLEnvLogger\n    end\n    \n    ColorfulLogger -->|\"Logs queries<br/>and responses\"| Console1[\"Console Output<br/>(Rich Panel)\"]\n    REPLEnvLogger -->|\"Logs code<br/>execution\"| Console2[\"Console Output<br/>(Rich Syntax)\"]\n```\n\n### Logger Capabilities\n\n| Logger | Location | Purpose | Key Methods |\n|--------|----------|---------|-------------|\n| `ColorfulLogger` | [rlm/logger/root_logger.py]() | Track root-level LLM queries | `log_query()`, `log_response()` |\n| `REPLEnvLogger` | [rlm/logger/repl_logger.py]() | Track REPL execution details | `log_code()`, `log_result()` |\n\n### Disabling Logging\n\nTo disable or modify logging:\n1. Pass `None` for logger parameters in constructors\n2. Add conditional checks: `if self.logger is not None: self.logger.log_query(...)`\n3. Remove `rich` imports if not needed\n\n**Sources**: [rlm/logger/root_logger.py](), [rlm/logger/repl_logger.py](), [README.md:25-32]()\n\n---\n\n## Configuration Management\n\n### Environment Variables\n\nThe system uses `.env` file for configuration (via `python-dotenv`).\n\n**Required Variables**:\n```\nOPENAI_API_KEY=sk-...\n```\n\n**Optional Variables** (examples for extension):\n```\n# Model selection\nROOT_MODEL=gpt-5-nano\nRECURSIVE_MODEL=gpt-5\n\n# Execution limits\nMAX_ITERATIONS=20\nEXECUTION_TIMEOUT=30\n\n# Logging\nENABLE_LOGGING=true\nLOG_LEVEL=INFO\n```\n\n### Configuration Access Pattern\n\n**Location**: [rlm/utils/llm.py]()\n\n```python\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\napi_key = os.environ.get(\"OPENAI_API_KEY\")\n```\n\n**Best Practices**:\n- Never commit `.env` to version control (already in [.gitignore:1]())\n- Provide `.env-example` with placeholder values\n- Validate required variables at startup\n- Use environment variables for all deployment-specific settings\n\n**Sources**: [.env-example](), [rlm/utils/llm.py](), [.gitignore:1]()\n\n---\n\n## Code Style and Conventions\n\n### Naming Conventions\n\n| Element | Convention | Examples |\n|---------|-----------|----------|\n| Classes | PascalCase | `RLM`, `RLM_REPL`, `REPLEnv`, `OpenAIClient` |\n| Functions | snake_case | `setup_context()`, `code_execution()`, `llm_query()` |\n| Variables | snake_case | `max_iterations`, `repl_env`, `namespace` |\n| Constants | UPPER_SNAKE_CASE | `REPL_SYSTEM_PROMPT`, `USER_PROMPT` |\n| Private members | Leading underscore | `_llm_query()`, `_parse_response()` |\n\n### Module Organization\n\nEach module should focus on a single responsibility:\n- **rlm.py**: Abstract interfaces only\n- **rlm_repl.py**: Complete RLM implementation\n- **repl.py**: REPL environment and execution\n- **utils/llm.py**: LLM client abstraction\n- **utils/prompts.py**: Prompt templates only\n- **utils/utils.py**: Parsing and formatting utilities\n\n### Import Organization\n\nStandard import order (following PEP 8):\n1. Standard library imports\n2. Third-party imports\n3. Local application imports\n\n**Sources**: [rlm/rlm.py](), [rlm/rlm_repl.py](), [rlm/repl.py]()\n\n---\n\n## Version Control and Collaboration\n\n### Git Workflow\n\nThe `.gitignore` is configured to exclude:\n- Environment files: `.env`, `.venv/`, `venv/`\n- Python artifacts: `__pycache__/`, `*.pyc`, `*.pyo`\n- IDE files: `.vscode/`, `.idea/`, `.DS_Store`\n- Build artifacts: `dist/`, `build/`, `*.egg-info/`\n\n### Recommended Branching Strategy\n\nFor contributions:\n1. Fork the repository\n2. Create feature branch: `feature/your-feature-name`\n3. Make changes with clear commit messages\n4. Test thoroughly (especially [main.py]() example)\n5. Submit pull request with description of changes\n\n### File Change Impact Analysis\n\nWhen modifying files, consider downstream impact:\n\n| File Modified | Direct Impact | Indirect Impact |\n|---------------|--------------|-----------------|\n| [rlm/rlm.py]() | All RLM implementations | Entire system (interface change) |\n| [rlm/rlm_repl.py]() | Main RLM usage | Examples, tests |\n| [rlm/repl.py]() | Code execution behavior | RLM_REPL functionality |\n| [rlm/utils/prompts.py]() | LLM behavior | All query results |\n| [rlm/utils/llm.py]() | API interaction | All LLM calls |\n| [rlm/utils/utils.py]() | Response parsing | Execution flow |\n\n**Sources**: [.gitignore:1-211]()\n\n---\n\n## License and Attribution\n\nThe codebase is licensed under the **MIT License**, providing maximum flexibility for modification and distribution.\n\n**License Location**: [LICENSE:1-22]()\n\n**Key Permissions**:\n- Commercial use allowed\n- Modification allowed\n- Distribution allowed\n- Private use allowed\n\n**Requirements**:\n- Include original license and copyright notice\n- No warranty provided\n\n**Attribution**: When building on this codebase, consider linking back to the original repository and blog post: https://alexzhang13.github.io/blog/2025/rlm/\n\n**Sources**: [LICENSE:1-22](), [README.md:3]()\n\n---\n\n# Page: Development Setup\n\n# Development Setup\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [Makefile](Makefile)\n- [README.md](README.md)\n- [pyproject.toml](pyproject.toml)\n- [uv.lock](uv.lock)\n\n</details>\n\n\n\nThis document provides instructions for setting up a local development environment for contributing to the RLM codebase. It covers Python version requirements, dependency installation using `uv`, pre-commit hook configuration, and environment setup. For information about running tests and code quality checks, see [Code Quality and Testing](#8.2). For CI/CD pipeline details, see [CI/CD Pipeline](#8.3).\n\n## Prerequisites\n\n### Python Version Requirements\n\nThe RLM project requires **Python 3.11 or higher**. The specific version is pinned in [.python-version:1]():\n\n```\n3.11\n```\n\nThe project configuration in [pyproject.toml:9]() enforces this minimum version:\n\n```toml\nrequires-python = \">=3.11\"\n```\n\nThe tooling configuration also targets Python 3.11 as the baseline for compatibility checks [pyproject.toml:42]().\n\n### Package Manager: uv\n\nRLM uses `uv` as its package manager for fast, reliable dependency resolution. Install `uv` following the instructions at https://github.com/astral-sh/uv.\n\n**Sources:** [.python-version:1](), [pyproject.toml:9](), [pyproject.toml:42]()\n\n## Dependency Installation\n\n### Development Environment Setup Workflow\n\n```mermaid\ngraph TB\n    Start[\"Developer starts setup\"]\n    \n    CheckPython[\"Check Python version<br/>.python-version\"]\n    InstallUV[\"Install uv package manager\"]\n    \n    CloneRepo[\"Clone repository<br/>git clone\"]\n    \n    SyncDeps[\"uv sync<br/>Install dependencies\"]\n    \n    subgraph \"Dependency Groups\"\n        CoreDeps[\"Core dependencies<br/>pyproject.toml [dependencies]\"]\n        DevDeps[\"Dev dependencies<br/>pyproject.toml [dependency-groups.dev]\"]\n        TestDeps[\"Test dependencies<br/>pyproject.toml [dependency-groups.test]\"]\n        ModalDeps[\"Optional: Modal<br/>pyproject.toml [project.optional-dependencies.modal]\"]\n    end\n    \n    SetupPreCommit[\"pre-commit install<br/>Install hooks from .pre-commit-config.yaml\"]\n    \n    SetupEnv[\"Create .env file<br/>Configure API keys\"]\n    \n    Verify[\"Verify setup<br/>python -c 'import rlm'<br/>pre-commit run --all-files\"]\n    \n    Ready[\"Development environment ready\"]\n    \n    Start --> CheckPython\n    CheckPython --> InstallUV\n    InstallUV --> CloneRepo\n    CloneRepo --> SyncDeps\n    \n    SyncDeps --> CoreDeps\n    SyncDeps --> DevDeps\n    SyncDeps --> TestDeps\n    SyncDeps -.->|optional| ModalDeps\n    \n    CoreDeps --> SetupPreCommit\n    DevDeps --> SetupPreCommit\n    TestDeps --> SetupPreCommit\n    \n    SetupPreCommit --> SetupEnv\n    SetupEnv --> Verify\n    Verify --> Ready\n```\n\n**Sources:** [pyproject.toml:10-38](), [.pre-commit-config.yaml:1-26](), [.python-version:1]()\n\n### Core Dependencies\n\nInstall all dependencies including development tools:\n\n```bash\nuv sync\n```\n\nThis command reads the dependency specifications from [pyproject.toml:10-16]() and installs:\n\n| Dependency | Version Constraint | Purpose |\n|------------|-------------------|---------|\n| `dotenv` | >=0.9.9 | Load environment variables from `.env` files |\n| `openai` | >=2.14.0 | OpenAI API client |\n| `portkey-ai` | >=2.1.0 | Portkey LLM gateway client |\n| `requests` | >=2.32.5 | HTTP library for API communication |\n| `rich` | >=13.0.0 | Terminal output formatting |\n\n### Development Dependencies\n\nThe `dev` dependency group [pyproject.toml:29-33]() includes tools for code quality:\n\n| Tool | Version | Purpose |\n|------|---------|---------|\n| `pre-commit` | >=4.5.1 | Git hook framework |\n| `ruff` | >=0.14.10 | Linter and formatter |\n| `ty` | >=0.0.7 | Type checker |\n\n### Test Dependencies\n\nThe `test` dependency group [pyproject.toml:34-38]() includes testing frameworks:\n\n| Tool | Version | Purpose |\n|------|---------|---------|\n| `pytest` | >=8.0.0 | Test framework |\n| `pytest-asyncio` | >=0.24.0 | Async test support |\n| `pytest-cov` | >=6.0.0 | Coverage reporting |\n\n### Optional Dependencies\n\nFor cloud execution environments (Modal), install the optional `modal` group [pyproject.toml:19]():\n\n```bash\nuv sync --extra modal\n```\n\nThis installs:\n- `modal` >=0.73.0 - Modal cloud sandbox client\n- `dill` >=0.3.7 - Enhanced pickling for remote execution\n\n**Sources:** [pyproject.toml:10-38]()\n\n## Pre-commit Hook Configuration\n\n### Installing Pre-commit Hooks\n\nAfter installing dependencies, activate the pre-commit hooks:\n\n```bash\npre-commit install\n```\n\nThis configures Git to run quality checks before each commit based on [.pre-commit-config.yaml:1-26]().\n\n### Hook Configuration Details\n\n```mermaid\ngraph LR\n    subgraph \"Pre-commit Framework\"\n        PreCommit[\"pre-commit install\"]\n    end\n    \n    subgraph \"Ruff Hooks\"\n        RuffLint[\"ruff hook<br/>id: ruff<br/>args: --fix, --config=pyproject.toml\"]\n        RuffFormat[\"ruff-format hook<br/>id: ruff-format<br/>args: --config=pyproject.toml\"]\n    end\n    \n    subgraph \"Type Checking\"\n        Ty[\"ty hook<br/>id: ty<br/>entry: ty check<br/>args: --exit-zero, --output-format=concise\"]\n    end\n    \n    subgraph \"Configuration Sources\"\n        PreCommitYAML[\".pre-commit-config.yaml\"]\n        PyProject[\"pyproject.toml<br/>[tool.ruff]<br/>[tool.ruff.lint]<br/>[tool.ruff.format]\"]\n    end\n    \n    PreCommit --> RuffLint\n    PreCommit --> RuffFormat\n    PreCommit --> Ty\n    \n    PreCommitYAML -.->|configures hooks| RuffLint\n    PreCommitYAML -.->|configures hooks| RuffFormat\n    PreCommitYAML -.->|configures hooks| Ty\n    \n    PyProject -.->|ruff settings| RuffLint\n    PyProject -.->|ruff settings| RuffFormat\n```\n\n**Sources:** [.pre-commit-config.yaml:1-26](), [pyproject.toml:40-59]()\n\n### Ruff Linter and Formatter\n\nThe `ruff` tool runs twice during pre-commit [.pre-commit-config.yaml:6-11]():\n\n1. **Linter** (`ruff`): Checks code style and common errors with auto-fix enabled\n   - Arguments: `--fix`, `--config=pyproject.toml`\n   - Fixes issues automatically when possible\n\n2. **Formatter** (`ruff-format`): Formats code according to style guidelines\n   - Arguments: `--config=pyproject.toml`\n   - Enforces consistent formatting\n\n#### Ruff Configuration\n\nThe linter configuration [pyproject.toml:40-56]() specifies:\n\n```toml\nline-length = 100\ntarget-version = \"py311\"\n\n[tool.ruff.lint]\nselect = [\n    \"E\",   # pycodestyle errors\n    \"W\",   # pycodestyle warnings\n    \"F\",   # pyflakes\n    \"I\",   # isort\n    \"B\",   # flake8-bugbear\n    \"UP\",  # pyupgrade\n]\nignore = [\n    \"E501\",  # line too long (handled by formatter)\n]\n```\n\nThe formatter configuration [pyproject.toml:57-58]() uses:\n- `quote-style = \"double\"` - Enforces double quotes for strings\n\n### Type Checker (ty)\n\nThe `ty` type checker [.pre-commit-config.yaml:15-25]() runs as a local hook:\n\n- **Entry point**: `ty check`\n- **Arguments**: `--exit-zero`, `--output-format=concise`\n- **Behavior**: Reports type issues but does not block commits (`--exit-zero`)\n- **File types**: Python (`.py`) and stub files (`.pyi`)\n\n**Sources:** [.pre-commit-config.yaml:1-26](), [pyproject.toml:40-58]()\n\n## Environment Configuration\n\n### API Key Setup\n\nRLM requires API keys for LLM backends. Create a `.env` file in the project root:\n\n```bash\n# OpenAI API (for OpenAIClient)\nOPENAI_API_KEY=sk-...\n\n# Portkey.ai (for PortkeyClient)\nPORTKEY_API_KEY=...\n\n# OpenRouter (for OpenAIClient with openrouter backend)\nOPENROUTER_API_KEY=...\n\n# Anthropic (for AnthropicClient)\nANTHROPIC_API_KEY=...\n```\n\nThe RLM codebase uses the `dotenv` package [pyproject.toml:11]() to load these variables automatically.\n\n### Optional: Modal Configuration\n\nIf using Modal cloud sandboxes, authenticate with Modal:\n\n```bash\nmodal token new\n```\n\nThis stores Modal credentials for [ModalREPL](#5.4) environment usage.\n\n**Sources:** [pyproject.toml:11]()\n\n## Verification\n\n### Verify Package Installation\n\nTest that the RLM package is installed correctly:\n\n```bash\npython -c \"import rlm; print(rlm.RLM)\"\n```\n\nExpected output: `<class 'rlm.core.rlm.RLM'>`\n\nThe package entry point is defined in [rlm/__init__.py:1-3]():\n\n```python\nfrom rlm.core.rlm import RLM\n\n__all__ = [\"RLM\"]\n```\n\n### Verify Pre-commit Hooks\n\nRun all pre-commit hooks on all files:\n\n```bash\npre-commit run --all-files\n```\n\nThis executes:\n1. `ruff` linter with auto-fix\n2. `ruff-format` formatter\n3. `ty` type checker (non-blocking)\n\nA successful run indicates the development environment is properly configured.\n\n### Test Suite Verification\n\nRun the test suite to ensure all dependencies are working:\n\n```bash\npytest\n```\n\nThe test configuration is defined in [pyproject.toml:60-61]():\n\n```toml\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\n```\n\n**Sources:** [rlm/__init__.py:1-3](), [pyproject.toml:60-61](), [.pre-commit-config.yaml:1-26]()\n\n## Development Workflow Summary\n\n### Typical Development Cycle\n\n```mermaid\nsequenceDiagram\n    participant Dev as \"Developer\"\n    participant Git as \"Git\"\n    participant PreCommit as \"Pre-commit Hooks\"\n    participant Tools as \"ruff + ty\"\n    \n    Dev->>Dev: \"Edit code files\"\n    Dev->>Git: \"git add <files>\"\n    Dev->>Git: \"git commit -m 'message'\"\n    \n    Git->>PreCommit: \"Trigger hooks\"\n    \n    PreCommit->>Tools: \"Run ruff (linter)\"\n    Tools->>Tools: \"Auto-fix issues\"\n    Tools-->>PreCommit: \"Pass/Fail\"\n    \n    PreCommit->>Tools: \"Run ruff-format\"\n    Tools->>Tools: \"Format code\"\n    Tools-->>PreCommit: \"Pass/Fail\"\n    \n    PreCommit->>Tools: \"Run ty (type check)\"\n    Tools-->>PreCommit: \"Report (non-blocking)\"\n    \n    alt \"Hooks passed\"\n        PreCommit-->>Git: \"Commit allowed\"\n        Git-->>Dev: \"Commit successful\"\n    else \"Hooks failed\"\n        PreCommit-->>Git: \"Block commit\"\n        Git-->>Dev: \"Fix issues and re-commit\"\n        Dev->>Dev: \"Review auto-fixes\"\n        Dev->>Git: \"git add <fixed files>\"\n        Dev->>Git: \"git commit\"\n    end\n```\n\n**Sources:** [.pre-commit-config.yaml:1-26]()\n\n### Tool Configuration Reference\n\n| Configuration File | Purpose | Key Settings |\n|-------------------|---------|--------------|\n| [pyproject.toml:1-62]() | Project metadata, dependencies, tool configs | Python version, dependency lists, ruff rules, pytest paths |\n| [.pre-commit-config.yaml:1-26]() | Pre-commit hook definitions | Hook IDs, arguments, tool versions |\n| [.python-version:1]() | Python version pinning | 3.11 |\n| `uv.lock` (generated) | Locked dependency versions | Reproducible builds |\n\n**Sources:** [pyproject.toml:1-62](), [.pre-commit-config.yaml:1-26](), [.python-version:1]()\n\n## Next Steps\n\nAfter completing the development setup:\n\n1. **Run Tests Locally**: See [Code Quality and Testing](#8.2) for testing workflows\n2. **Understand CI Pipeline**: See [CI/CD Pipeline](#8.3) for automated checks\n3. **Read Core Architecture**: See [Core Architecture](#3) to understand the codebase structure\n4. **Try Quick Start**: See [Quick Start Example](#2.2) to run your first RLM completion\n\n---\n\n# Page: Code Quality and Testing\n\n# Code Quality and Testing\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [AGENTS.md](AGENTS.md)\n- [examples/lm_in_prime_repl.py](examples/lm_in_prime_repl.py)\n- [examples/prime_repl_example.py](examples/prime_repl_example.py)\n- [pyproject.toml](pyproject.toml)\n- [rlm/environments/constants.py](rlm/environments/constants.py)\n- [tests/test_imports.py](tests/test_imports.py)\n\n</details>\n\n\n\nThis document describes the code quality tools and testing infrastructure used in the RLM codebase. It covers the `ruff` linter/formatter configuration, the `pytest` test suite structure, pre-commit hooks, and quality check workflows. For information about the overall development setup and dependency management, see [Development Setup](#7.1). For details on the CI/CD pipeline, see [CI/CD Pipeline](#7.3).\n\n---\n\n## Ruff Linting and Formatting\n\nThe RLM project uses `ruff` as its primary linting and formatting tool. Ruff provides fast, comprehensive code quality checks and automatic formatting with minimal configuration.\n\n### Configuration\n\nThe ruff configuration is defined in [pyproject.toml:45-63]():\n\n| Setting | Value | Purpose |\n|---------|-------|---------|\n| `line-length` | 100 | Maximum line length for code and docstrings |\n| `target-version` | `\"py311\"` | Target Python 3.11+ syntax features |\n| `quote-style` | `\"double\"` | Enforce double quotes for strings |\n\n### Enabled Rules\n\nThe linter enables the following rule categories [pyproject.toml:50-57]():\n\n| Rule Code | Category | Description |\n|-----------|----------|-------------|\n| `E` | pycodestyle errors | Core style violations (PEP 8) |\n| `W` | pycodestyle warnings | Style warnings |\n| `F` | pyflakes | Logical errors (unused imports, undefined names) |\n| `I` | isort | Import sorting and organization |\n| `B` | flake8-bugbear | Common bug patterns |\n| `UP` | pyupgrade | Python version upgrade suggestions |\n\nThe configuration explicitly ignores `E501` (line too long) because the formatter handles line length automatically [pyproject.toml:58-60]().\n\n### Running Ruff\n\nRuff is invoked through the development dependency group [pyproject.toml:34-38]():\n\n```bash\n# Check for issues\nuv run ruff check .\n\n# Auto-fix issues\nuv run ruff check --fix .\n\n# Format code\nuv run ruff format .\n```\n\n### Ruff Configuration Flow\n\n```mermaid\ngraph TB\n    pyproject[\"pyproject.toml\"]\n    \n    subgraph \"Ruff Configuration\"\n        lineLength[\"line-length: 100\"]\n        targetVer[\"target-version: py311\"]\n        quoteStyle[\"quote-style: double\"]\n    end\n    \n    subgraph \"Lint Rules\"\n        E[\"E - pycodestyle errors\"]\n        W[\"W - pycodestyle warnings\"]\n        F[\"F - pyflakes\"]\n        I[\"I - isort\"]\n        B[\"B - flake8-bugbear\"]\n        UP[\"UP - pyupgrade\"]\n    end\n    \n    subgraph \"Ignored Rules\"\n        E501[\"E501 - line too long<br/>(formatter handles)\"]\n    end\n    \n    subgraph \"Execution\"\n        check[\"ruff check\"]\n        checkFix[\"ruff check --fix\"]\n        format[\"ruff format\"]\n    end\n    \n    pyproject --> lineLength\n    pyproject --> targetVer\n    pyproject --> quoteStyle\n    pyproject --> E\n    pyproject --> W\n    pyproject --> F\n    pyproject --> I\n    pyproject --> B\n    pyproject --> UP\n    pyproject --> E501\n    \n    lineLength --> format\n    quoteStyle --> format\n    E --> check\n    W --> check\n    F --> check\n    I --> check\n    B --> check\n    UP --> check\n    \n    check --> checkFix\n    checkFix --> format\n```\n\n**Sources:** [pyproject.toml:45-63]()\n\n---\n\n## Pytest Test Suite\n\nThe project uses `pytest` with `pytest-asyncio` for testing. The test suite is organized into comprehensive test classes that verify imports, detect conflicts, and ensure API completeness.\n\n### Test Configuration\n\nPytest configuration is minimal, specifying only the test discovery path [pyproject.toml:65-66]():\n\n```toml\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\n```\n\nTest dependencies are managed through the `test` dependency group [pyproject.toml:39-43]():\n\n| Package | Version | Purpose |\n|---------|---------|---------|\n| `pytest` | >=8.0.0 | Test framework |\n| `pytest-asyncio` | >=0.24.0 | Async test support |\n| `pytest-cov` | >=6.0.0 | Coverage reporting |\n\n### Test Structure\n\nThe test suite in [tests/test_imports.py:1-504]() is organized into five test classes:\n\n#### 1. TestTopLevelImports\n\nTests that the main `rlm` package exports are accessible [tests/test_imports.py:10-31]():\n\n- `test_rlm_import()` - Verifies `rlm.RLM` is available\n- `test_rlm_rlm_import()` - Tests `from rlm import RLM`\n- `test_rlm_core_rlm_import()` - Tests `from rlm.core.rlm import RLM`\n\n#### 2. TestClientImports\n\nVerifies all LM client implementations can be imported [tests/test_imports.py:33-82]():\n\n- Tests `BaseLM`, `OpenAIClient`, `AnthropicClient`, `PortkeyClient`, `LiteLLMClient`\n- Uses `pytest.importorskip()` for optional dependencies [tests/test_imports.py:51]()\n- Validates `get_client` factory function\n\n#### 3. TestCoreImports\n\nValidates core type and utility imports [tests/test_imports.py:84-138]():\n\n- Tests all core types: `RLMChatCompletion`, `REPLResult`, `CodeBlock`, etc.\n- Tests communication utilities: `LMRequest`, `LMResponse`, socket functions\n- Tests `RLM`, `LMHandler` class imports\n\n#### 4. TestEnvironmentImports\n\nVerifies environment implementations [tests/test_imports.py:140-190]():\n\n- Tests base classes: `BaseEnv`, `IsolatedEnv`, `NonIsolatedEnv`\n- Tests implementations: `LocalREPL`, `DockerREPL`, `ModalREPL`, `PrimeREPL`\n- Uses `pytest.importorskip()` for cloud-specific dependencies [tests/test_imports.py:167,180]()\n\n#### 5. TestLoggerImports\n\nValidates logging infrastructure [tests/test_imports.py:192-215]():\n\n- Tests `RLMLogger` and `VerbosePrinter` imports\n- Verifies `__all__` exports\n\n### Test Categories for Quality Assurance\n\nThe test suite includes specialized test classes for detecting code quality issues:\n\n#### TestImportConflicts\n\nDetects naming conflicts and circular imports [tests/test_imports.py:255-429]():\n\n| Test Method | Purpose |\n|-------------|---------|\n| `test_no_duplicate_names_in_rlm_all()` | Checks for duplicate exports in `__all__` |\n| `test_no_duplicate_names_in_logger_all()` | Checks logger module `__all__` |\n| `test_all_declarations_match_exports()` | Ensures `__all__` items are actually exported |\n| `test_no_circular_imports()` | Attempts to import all modules fresh [tests/test_imports.py:295-354]() |\n| `test_no_naming_conflicts_across_modules()` | Detects cross-module naming collisions [tests/test_imports.py:356-428]() |\n\nThe circular import test maintains lists of core and optional modules [tests/test_imports.py:298-328]():\n\n```python\ncore_modules = [\n    \"rlm\",\n    \"rlm.clients\",\n    \"rlm.clients.base_lm\",\n    \"rlm.core\",\n    # ...\n]\n\noptional_modules = [\n    (\"rlm.clients.openai\", \"openai\"),\n    (\"rlm.environments.modal_repl\", \"modal\"),\n    # ...\n]\n```\n\n#### TestImportCompleteness\n\nEnsures all expected classes are importable [tests/test_imports.py:431-504]():\n\n- `test_all_client_classes_importable()` - Verifies all client classes exist and are proper types [tests/test_imports.py:434-472]()\n- `test_all_environment_classes_importable()` - Verifies all environment classes [tests/test_imports.py:474-503]()\n\n### Test Suite Structure Diagram\n\n```mermaid\ngraph TB\n    testDir[\"tests/\"]\n    testImports[\"test_imports.py\"]\n    \n    subgraph \"Test Classes\"\n        topLevel[\"TestTopLevelImports\"]\n        clients[\"TestClientImports\"]\n        core[\"TestCoreImports\"]\n        envs[\"TestEnvironmentImports\"]\n        logger[\"TestLoggerImports\"]\n        conflicts[\"TestImportConflicts\"]\n        complete[\"TestImportCompleteness\"]\n    end\n    \n    subgraph \"TestTopLevelImports Tests\"\n        tl1[\"test_rlm_import\"]\n        tl2[\"test_rlm_rlm_import\"]\n        tl3[\"test_rlm_core_rlm_import\"]\n    end\n    \n    subgraph \"TestClientImports Tests\"\n        c1[\"test_clients_module_import\"]\n        c2[\"test_base_lm_import\"]\n        c3[\"test_openai_client_import\"]\n        c4[\"test_anthropic_client_import\"]\n        c5[\"test_get_client_function\"]\n    end\n    \n    subgraph \"TestImportConflicts Tests\"\n        ic1[\"test_no_duplicate_names_in_rlm_all\"]\n        ic2[\"test_all_declarations_match_exports\"]\n        ic3[\"test_no_circular_imports\"]\n        ic4[\"test_no_naming_conflicts_across_modules\"]\n    end\n    \n    subgraph \"TestImportCompleteness Tests\"\n        comp1[\"test_all_client_classes_importable\"]\n        comp2[\"test_all_environment_classes_importable\"]\n    end\n    \n    testDir --> testImports\n    testImports --> topLevel\n    testImports --> clients\n    testImports --> core\n    testImports --> envs\n    testImports --> logger\n    testImports --> conflicts\n    testImports --> complete\n    \n    topLevel --> tl1\n    topLevel --> tl2\n    topLevel --> tl3\n    \n    clients --> c1\n    clients --> c2\n    clients --> c3\n    clients --> c4\n    clients --> c5\n    \n    conflicts --> ic1\n    conflicts --> ic2\n    conflicts --> ic3\n    conflicts --> ic4\n    \n    complete --> comp1\n    complete --> comp2\n```\n\n**Sources:** [tests/test_imports.py:1-504](), [pyproject.toml:39-43](), [pyproject.toml:65-66]()\n\n---\n\n## Pre-commit Hooks\n\nPre-commit hooks provide automated quality checks before each commit. They are configured through the `dev` dependency group [pyproject.toml:34-38]().\n\n### Installation\n\nPre-commit hooks are installed during development setup:\n\n```bash\n# Install pre-commit package\nuv sync --group dev\n\n# Install git hooks\nuv run pre-commit install\n```\n\n### Hook Execution\n\nThe hooks run automatically on `git commit`, but can also be invoked manually:\n\n```bash\n# Run on staged files\nuv run pre-commit run\n\n# Run on all files\nuv run pre-commit run --all-files\n```\n\nAccording to the development guidelines in [AGENTS.md:89-101](), the pre-commit checklist before submitting a PR includes:\n\n```bash\n# Run style + lint checks:\nuv run ruff check --fix .\nuv run ruff format .\nuv run pre-commit run --all-files\n\n# Run tests:\nuv run pytest\n```\n\n### Quality Gates\n\nPre-commit hooks enforce several quality gates:\n\n1. **Ruff linting** - Checks for style violations\n2. **Ruff formatting** - Ensures consistent code style\n3. **Type checking** - Validates type hints (when configured)\n4. **Test execution** - Runs pytest suite\n\nThe development philosophy from [AGENTS.md:43-46]() emphasizes:\n\n- **Fail fast, fail loud** - No defensive programming or silent fallbacks\n- **Minimize branching** - Prefer single code paths\n- **Example**: Missing API key → immediate `ValueError`, not graceful fallback\n\n**Sources:** [pyproject.toml:34-38](), [AGENTS.md:89-101](), [AGENTS.md:43-46]()\n\n---\n\n## Quality Check Workflow\n\nThe project provides convenient Make targets for running quality checks, as referenced in [AGENTS.md:89-99]().\n\n### Make Targets for Quality\n\n| Target | Purpose | Commands |\n|--------|---------|----------|\n| `make check` | Run all quality checks | `ruff check`, `ruff format`, `pytest` |\n| `make install-dev` | Install development dependencies | `uv sync --group dev --group test` |\n\n### Development Workflow\n\n```mermaid\ngraph LR\n    subgraph \"Development Cycle\"\n        edit[\"Edit Code\"]\n        check[\"make check\"]\n        fix[\"make fix\"]\n        commit[\"git commit\"]\n        precommit[\"pre-commit hooks\"]\n        push[\"git push\"]\n    end\n    \n    subgraph \"Quality Checks\"\n        ruffCheck[\"ruff check\"]\n        ruffFormat[\"ruff format\"]\n        pytest[\"pytest\"]\n    end\n    \n    subgraph \"Pre-commit Gates\"\n        hookRuff[\"ruff hooks\"]\n        hookTest[\"test hooks\"]\n    end\n    \n    edit --> check\n    check --> ruffCheck\n    check --> ruffFormat\n    check --> pytest\n    \n    ruffCheck --> fix\n    ruffFormat --> fix\n    pytest --> fix\n    \n    fix --> edit\n    \n    pytest --> commit\n    commit --> precommit\n    precommit --> hookRuff\n    precommit --> hookTest\n    \n    hookRuff --> push\n    hookTest --> push\n```\n\n### Comprehensive Quality Checklist\n\nBefore submitting a PR, developers must ensure [AGENTS.md:88-101]():\n\n1. ✓ All code follows `ruff` style guidelines\n2. ✓ No linting errors remain after `ruff check --fix`\n3. ✓ Code is formatted with `ruff format`\n4. ✓ All tests pass with `pytest`\n5. ✓ Pre-commit hooks succeed on all files\n6. ✓ Documentation is updated if behavior changed\n7. ✓ Dead code is deleted (not guarded with conditionals)\n\n### Testing Guidelines\n\nThe development guidelines in [AGENTS.md:70-75]() specify:\n\n- `uv run pytest` with discovery under `tests/`\n- Write simple, deterministic unit tests\n- Update tests when changing functionality\n- For isolated environments, mock external services\n\nThe test suite validates:\n- **Import correctness** - All modules import without errors\n- **API completeness** - All expected exports are available\n- **No conflicts** - No naming collisions or circular imports\n- **Optional dependencies** - Graceful handling with `pytest.importorskip()`\n\n**Sources:** [AGENTS.md:70-75](), [AGENTS.md:88-101](), [tests/test_imports.py:1-504]()\n\n---\n\n## Code Style Guidelines\n\nThe development guide in [AGENTS.md:28-40]() establishes strict style conventions:\n\n### Naming Conventions\n\n| Element | Convention | Examples |\n|---------|------------|----------|\n| Methods | `snake_case` | `execute_code()`, `load_context()` |\n| Classes | `PascalCase` | `LocalREPL`, `PortkeyClient`, `RLMHandler` |\n| Variables | `snake_case` | `lm_handler_address`, `context_payload` |\n| Constants | `UPPER_CASE` | `_SAFE_BUILTINS`, `RLM_SYSTEM_PROMPT` |\n\n### Typing Policy\n\nFrom [AGENTS.md:30-33]():\n\n- **Preferred**: Explicit types\n- **OK**: `cast(...)`, `assert ...` for type narrowing\n- **Sometimes OK**: Untyped args for simple cases\n- **NOT OK**: `# type: ignore` without strong justification\n\n### Error Handling Philosophy\n\nFrom [AGENTS.md:43-46]():\n\n- Fail fast, fail loud\n- Minimize branching\n- No defensive programming or silent fallbacks\n- Immediate errors for configuration issues (e.g., missing API keys)\n\n**Sources:** [AGENTS.md:28-40](), [AGENTS.md:43-46]()\n\n---\n\n## Integration with Development Tools\n\n### UV Package Manager Integration\n\nAll quality tools are installed and invoked through `uv` [pyproject.toml:1-67]():\n\n```bash\n# Install all development dependencies\nuv sync --group dev --group test\n\n# Run quality checks\nuv run ruff check --fix .\nuv run ruff format .\nuv run pytest\n\n# Run pre-commit\nuv run pre-commit run --all-files\n```\n\n### Dependency Groups\n\nQuality tools are organized into dependency groups [pyproject.toml:33-43]():\n\n```toml\n[dependency-groups]\ndev = [\n    \"pre-commit>=4.5.1\",\n    \"ruff>=0.14.10\",\n    \"ty>=0.0.7\",\n]\ntest = [\n    \"pytest>=8.0.0\",\n    \"pytest-asyncio>=0.24.0\",\n    \"pytest-cov>=6.0.0\",\n]\n```\n\n### Tool Integration Diagram\n\n```mermaid\ngraph TB\n    pyproject[\"pyproject.toml\"]\n    uv[\"uv package manager\"]\n    \n    subgraph \"Dependency Groups\"\n        dev[\"dev group:<br/>pre-commit, ruff, ty\"]\n        test[\"test group:<br/>pytest, pytest-asyncio, pytest-cov\"]\n    end\n    \n    subgraph \"Quality Tools\"\n        ruff[\"ruff\"]\n        pytest[\"pytest\"]\n        precommit[\"pre-commit\"]\n    end\n    \n    subgraph \"Tool Configuration\"\n        ruffConfig[\"[tool.ruff]<br/>line-length: 100<br/>target-version: py311\"]\n        pytestConfig[\"[tool.pytest.ini_options]<br/>testpaths: tests\"]\n        ruffLint[\"[tool.ruff.lint]<br/>select: E, W, F, I, B, UP\"]\n        ruffFormat[\"[tool.ruff.format]<br/>quote-style: double\"]\n    end\n    \n    subgraph \"Execution Commands\"\n        uvSync[\"uv sync --group dev --group test\"]\n        uvRuff[\"uv run ruff check --fix .\"]\n        uvFormat[\"uv run ruff format .\"]\n        uvPytest[\"uv run pytest\"]\n        uvPrecommit[\"uv run pre-commit run --all-files\"]\n    end\n    \n    pyproject --> uv\n    uv --> dev\n    uv --> test\n    \n    dev --> ruff\n    dev --> precommit\n    test --> pytest\n    \n    pyproject --> ruffConfig\n    pyproject --> pytestConfig\n    pyproject --> ruffLint\n    pyproject --> ruffFormat\n    \n    ruffConfig --> ruff\n    pytestConfig --> pytest\n    ruffLint --> ruff\n    ruffFormat --> ruff\n    \n    ruff --> uvRuff\n    ruff --> uvFormat\n    pytest --> uvPytest\n    precommit --> uvPrecommit\n    \n    dev --> uvSync\n    test --> uvSync\n```\n\n**Sources:** [pyproject.toml:1-67](), [pyproject.toml:33-43]()\n\n---\n\n## Optional Dependency Testing\n\nThe test suite handles optional dependencies gracefully using `pytest.importorskip()` [tests/test_imports.py:51,67,72,167,180]():\n\n### Optional Client Tests\n\n```python\ndef test_openai_client_import(self):\n    \"\"\"Test OpenAIClient import.\"\"\"\n    pytest.importorskip(\"openai\")\n    from rlm.clients.openai import OpenAIClient\n    assert OpenAIClient is not None\n```\n\n### Optional Environment Tests\n\n```python\ndef test_modal_repl_import(self):\n    \"\"\"Test ModalREPL import.\"\"\"\n    pytest.importorskip(\"modal\")\n    from rlm.environments.modal_repl import ModalREPL\n    assert ModalREPL is not None\n```\n\n### Circular Import Detection\n\nThe circular import test differentiates between core and optional modules [tests/test_imports.py:320-354]():\n\n```python\noptional_modules = [\n    (\"rlm.clients.openai\", \"openai\"),\n    (\"rlm.clients.anthropic\", \"anthropic\"),\n    (\"rlm.clients.portkey\", \"portkey_ai\"),\n    (\"rlm.clients.litellm\", \"litellm\"),\n    (\"rlm.environments.modal_repl\", \"modal\"),\n    (\"rlm.environments.prime_repl\", \"prime_sandboxes\"),\n]\n```\n\nEach optional module is only tested if its dependency is available, ensuring tests pass in minimal installation environments.\n\n**Sources:** [tests/test_imports.py:51](), [tests/test_imports.py:67](), [tests/test_imports.py:167](), [tests/test_imports.py:320-354]()\n\n---\n\n# Page: CI/CD Pipeline\n\n# CI/CD Pipeline\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [.github/workflows/docs.yml](.github/workflows/docs.yml)\n- [docs/next.config.js](docs/next.config.js)\n- [docs/src/components/Sidebar.tsx](docs/src/components/Sidebar.tsx)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis page documents the GitHub Actions workflow for building and deploying the RLM documentation site to GitHub Pages. The workflow automates the build process for the Next.js-based documentation and publishes it to `https://alexzhang13.github.io/rlm/`. For information about the documentation site structure and local development, see [Documentation System](#7.4). For code quality tools and testing, see [Code Quality and Testing](#7.2).\n\n## Overview\n\nThe RLM project uses a GitHub Actions workflow defined in [.github/workflows/docs.yml:1-62]() to automatically build and deploy documentation whenever changes are pushed to the `main` branch. The workflow consists of two jobs:\n\n1. **Build Job** - Compiles the Next.js documentation site into static HTML/CSS/JS\n2. **Deploy Job** - Publishes the built artifacts to GitHub Pages\n\nThe workflow can be triggered automatically on push to `main` or manually via the `workflow_dispatch` event.\n\n```mermaid\ngraph TB\n    subgraph \"Trigger Events\"\n        PushMain[\"Push to main branch\"]\n        ManualTrigger[\"workflow_dispatch<br/>(Manual trigger)\"]\n    end\n    \n    subgraph \"Build Job\"\n        Checkout[\"actions/checkout@v4\"]\n        SetupNode[\"actions/setup-node@v4<br/>Node.js 20\"]\n        NpmCI[\"npm ci<br/>(docs/)\"]\n        NpmBuild[\"npm run build<br/>(docs/)\"]\n        SetupPages[\"actions/configure-pages@v4\"]\n        Upload[\"actions/upload-pages-artifact@v3<br/>(docs/out)\"]\n    end\n    \n    subgraph \"Deploy Job\"\n        DeployPages[\"actions/deploy-pages@v4\"]\n    end\n    \n    subgraph \"GitHub Pages\"\n        GHPages[\"https://...github.io/rlm/\"]\n    end\n    \n    PushMain --> Checkout\n    ManualTrigger --> Checkout\n    \n    Checkout --> SetupNode\n    SetupNode --> NpmCI\n    NpmCI --> NpmBuild\n    NpmBuild --> SetupPages\n    SetupPages --> Upload\n    \n    Upload --> DeployPages\n    DeployPages --> GHPages\n```\n\n**Diagram: Documentation Deployment Workflow**\n\nSources: [.github/workflows/docs.yml:1-62]()\n\n## Workflow Configuration\n\nThe workflow is defined in [.github/workflows/docs.yml:1-62]() and uses the following configuration:\n\n| Configuration | Value | Purpose |\n|--------------|-------|---------|\n| **Triggers** | `push` (main branch), `workflow_dispatch` | Automatic on main push, manual trigger available |\n| **Permissions** | `contents: read`, `pages: write`, `id-token: write` | Read repo, write to Pages, authenticate with OIDC |\n| **Concurrency Group** | `\"pages\"` | Prevents concurrent deployments |\n| **Node.js Version** | `20` | Specified in setup-node@v4 action |\n| **Cache** | npm cache (docs/package.json) | Speeds up dependency installation |\n| **Artifact Path** | `docs/out` | Static site output directory |\n\nSources: [.github/workflows/docs.yml:3-46]()\n\n## Build Job\n\nThe build job runs on `ubuntu-latest` and executes the following steps:\n\n### Step 1: Checkout Repository\n\nUses `actions/checkout@v4` to clone the repository at the commit that triggered the workflow.\n\nSources: [.github/workflows/docs.yml:22-23]()\n\n### Step 2: Setup Node.js\n\nConfigures Node.js version 20 using `actions/setup-node@v4`. The action is configured to cache npm dependencies based on `docs/package.json` to speed up subsequent builds.\n\n```mermaid\ngraph LR\n    SetupNode[\"actions/setup-node@v4\"]\n    NodeVersion[\"node-version: '20'\"]\n    CacheType[\"cache: 'npm'\"]\n    CachePath[\"cache-dependency-path:<br/>docs/package.json\"]\n    \n    SetupNode --> NodeVersion\n    SetupNode --> CacheType\n    SetupNode --> CachePath\n```\n\n**Diagram: Node.js Setup Configuration**\n\nSources: [.github/workflows/docs.yml:25-30]()\n\n### Step 3: Install Dependencies\n\nRuns `npm ci` in the `docs/` directory to install dependencies from `package-lock.json`. The `npm ci` command is preferred over `npm install` in CI environments because it:\n\n- Installs exact versions from the lock file\n- Removes `node_modules/` before installing\n- Fails if lock file is out of sync with `package.json`\n\nSources: [.github/workflows/docs.yml:32-34]()\n\n### Step 4: Build Documentation Site\n\nExecutes `npm run build` in the `docs/` directory. This command runs the Next.js build process, which:\n\n1. Compiles TypeScript/React components\n2. Generates static HTML for all pages\n3. Optimizes assets (CSS, JS, images)\n4. Outputs everything to `docs/out/` directory\n\nThe build process respects the configuration in [docs/next.config.js:1-10](), which sets:\n\n- `output: 'export'` - Enables static site generation\n- `basePath: '/rlm'` - Prepends `/rlm` to all asset paths for GitHub Pages subdirectory\n- `trailingSlash: true` - Ensures URLs end with `/` for better GitHub Pages compatibility\n- `images: { unoptimized: true }` - Disables Next.js Image Optimization API (not supported in static export)\n\nSources: [.github/workflows/docs.yml:36-38](), [docs/next.config.js:1-10]()\n\n### Step 5: Configure GitHub Pages\n\nUses `actions/configure-pages@v4` to set up the GitHub Pages environment. This action configures metadata needed for deployment, including the Pages URL and deployment token.\n\nSources: [.github/workflows/docs.yml:40-41]()\n\n### Step 6: Upload Artifact\n\nUses `actions/upload-pages-artifact@v3` to upload the `docs/out/` directory as a GitHub Pages artifact. The artifact is stored temporarily and passed to the deploy job.\n\nSources: [.github/workflows/docs.yml:43-46]()\n\n## Deploy Job\n\nThe deploy job runs after the build job completes successfully. It requires special permissions to deploy to GitHub Pages.\n\n### Job Configuration\n\n| Setting | Value | Purpose |\n|---------|-------|---------|\n| **Environment** | `github-pages` | Specifies the deployment environment |\n| **URL** | `${{ steps.deployment.outputs.page_url }}` | Captures the deployed site URL |\n| **Needs** | `build` | Ensures build completes before deploy |\n| **Permissions** | `pages: write`, `id-token: write` | Allows GitHub Pages deployment with OIDC auth |\n\n### Deployment Step\n\nUses `actions/deploy-pages@v4` to publish the artifact uploaded by the build job. This action:\n\n1. Downloads the Pages artifact\n2. Authenticates using OIDC token\n3. Deploys to GitHub Pages\n4. Returns the deployment URL\n\nThe deployed site is accessible at `https://alexzhang13.github.io/rlm/` (or the repository owner's GitHub Pages URL with `/rlm` base path).\n\n```mermaid\ngraph TB\n    BuildJob[\"Build Job<br/>(uploads artifact)\"]\n    DeployJob[\"Deploy Job<br/>(needs: build)\"]\n    \n    subgraph \"Permissions\"\n        PagesWrite[\"pages: write\"]\n        IDToken[\"id-token: write\"]\n    end\n    \n    subgraph \"Deployment\"\n        DownloadArtifact[\"Download Pages artifact\"]\n        Authenticate[\"OIDC authentication\"]\n        Deploy[\"Deploy to GitHub Pages\"]\n        OutputURL[\"output: page_url\"]\n    end\n    \n    BuildJob --> DeployJob\n    DeployJob --> PagesWrite\n    DeployJob --> IDToken\n    \n    PagesWrite --> DownloadArtifact\n    IDToken --> Authenticate\n    DownloadArtifact --> Deploy\n    Authenticate --> Deploy\n    Deploy --> OutputURL\n```\n\n**Diagram: Deploy Job Execution Flow**\n\nSources: [.github/workflows/docs.yml:48-60]()\n\n## Concurrency Control\n\nThe workflow uses concurrency control to prevent multiple simultaneous deployments:\n\n```yaml\nconcurrency:\n  group: \"pages\"\n  cancel-in-progress: false\n```\n\n- **Group**: All deployments to GitHub Pages are grouped under `\"pages\"`\n- **Cancel-in-progress**: Set to `false`, meaning new deployments wait for in-progress ones to complete rather than canceling them\n\nThis ensures that deployments happen sequentially and complete successfully, preventing race conditions or incomplete deployments.\n\nSources: [.github/workflows/docs.yml:14-16]()\n\n## Next.js Configuration for GitHub Pages\n\nThe [docs/next.config.js:1-10]() file contains essential configuration for GitHub Pages deployment:\n\n```javascript\noutput: 'export'        // Static site generation mode\nbasePath: '/rlm'        // Subdirectory for GitHub Pages\ntrailingSlash: true     // Add trailing slashes to URLs\nimages: { unoptimized: true }  // Disable Image Optimization API\n```\n\n### Key Configuration Options\n\n| Option | Value | Reason |\n|--------|-------|--------|\n| `output` | `'export'` | Generates static HTML files instead of running a Node.js server |\n| `basePath` | `'/rlm'` | GitHub Pages hosts at `username.github.io/repo-name/`, requiring base path |\n| `trailingSlash` | `true` | GitHub Pages serves `/path/` better than `/path` for directories |\n| `images.unoptimized` | `true` | Next.js Image Optimization API requires a server, not available in static export |\n\nThe `basePath` configuration ensures that all internal links, scripts, and stylesheets use the correct path when deployed to `https://alexzhang13.github.io/rlm/`.\n\nSources: [docs/next.config.js:1-10]()\n\n## Customizing Workflows\n\n### Adding New Python Versions\n\nTo test against additional Python versions, modify the `matrix.python-version` configuration in `test.yml`:\n\n```yaml\nstrategy:\n  matrix:\n    python-version: [3.11, 3.12, 3.13]  # Add 3.13\n```\n\n### Changing Trigger Conditions\n\nModify the `on:` section in workflow files to customize when workflows run. For example, to run only on specific branches:\n\n```yaml\non:\n  pull_request:\n    branches: [main, develop]\n  push:\n    branches: [main]\n```\n\n### Extending CI Checks\n\nThe workflows can be extended to include additional checks:\n\n- **Type Checking**: Add `ty` or `mypy` type checking jobs\n- **Security Scanning**: Integrate dependency vulnerability scanning\n- **Package Building**: Test package distribution builds\n- **Integration Tests**: Add workflows for end-to-end testing with real LLM APIs (using secrets)\n\nSources: [.github/workflows/README.md:83-97]()\n\n## Workflow Configuration Reference\n\n### Key GitHub Actions Used\n\nThe workflows rely on standard GitHub Actions:\n\n- `actions/checkout@v3`: Checks out the repository code\n- `actions/setup-python@v4`: Sets up the specified Python version\n- `actions/setup-node@v3`: Sets up Node.js for documentation builds\n\n### Environment Variables and Secrets\n\nWorkflows that require external service access (e.g., for deployment or API testing) should use GitHub Secrets:\n\n- Configure secrets in **Settings → Secrets and variables → Actions**\n- Reference secrets in workflows using `${{ secrets.SECRET_NAME }}`\n\nCurrently, the CI workflows do not require secrets, as API-dependent tests are excluded.\n\nSources: [.github/workflows/README.md:1-97]()\n\n## Integration with Pre-commit Hooks\n\nThe CI/CD pipeline complements local pre-commit hooks (see [Development Setup](#8.1)). While pre-commit hooks catch issues before commits, CI workflows provide:\n\n- **Validation Layer**: Ensures all contributors follow standards, even if pre-commit is bypassed\n- **Matrix Testing**: Tests across multiple Python versions (pre-commit runs on one version)\n- **Authoritative Check**: Serves as the final gate before merge\n\nThis multi-layered approach ensures code quality while maintaining developer velocity.\n\nSources: [.github/workflows/README.md:1-97]()\n\n---\n\n# Page: Documentation System\n\n# Documentation System\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [.github/workflows/docs.yml](.github/workflows/docs.yml)\n- [Makefile](Makefile)\n- [README.md](README.md)\n- [docs/next.config.js](docs/next.config.js)\n- [docs/src/components/Sidebar.tsx](docs/src/components/Sidebar.tsx)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis document describes the documentation infrastructure for the RLM project, which consists of a Next.js-based static site that is automatically built and deployed to GitHub Pages. The documentation system includes the site structure, configuration, navigation components, build process, and CI/CD deployment pipeline.\n\nFor information about the visualizer application (a separate TypeScript tool for viewing RLM trajectories), see [6.3](#6.3). For information about the logging system that generates data for the visualizer, see [6.1](#6.1).\n\n---\n\n## Architecture Overview\n\nThe RLM documentation system is a **Next.js 14+ application** configured for static site generation (SSG). The application is located in the `docs/` directory and uses React with TypeScript for component development. The build process generates static HTML, CSS, and JavaScript files that are deployed to GitHub Pages.\n\n```mermaid\ngraph TB\n    subgraph \"Source Code (docs/)\"\n        NextConfig[\"next.config.js<br/>SSG Configuration\"]\n        Components[\"src/components/<br/>React Components\"]\n        Pages[\"src/app/<br/>Page Routes\"]\n        Styles[\"src/app/globals.css<br/>Tailwind CSS\"]\n        PackageJSON[\"package.json<br/>Dependencies\"]\n    end\n    \n    subgraph \"Build Process\"\n        NextBuild[\"next build<br/>Static Generation\"]\n        OutputDir[\"out/<br/>Build Artifacts\"]\n    end\n    \n    subgraph \"Deployment\"\n        GitHubActions[\".github/workflows/docs.yml<br/>CI Pipeline\"]\n        PagesArtifact[\"upload-pages-artifact<br/>docs/out\"]\n        GitHubPages[\"GitHub Pages<br/>alexzhang13.github.io/rlm\"]\n    end\n    \n    NextConfig --> NextBuild\n    Components --> NextBuild\n    Pages --> NextBuild\n    Styles --> NextBuild\n    PackageJSON --> NextBuild\n    \n    NextBuild --> OutputDir\n    OutputDir --> GitHubActions\n    GitHubActions --> PagesArtifact\n    PagesArtifact --> GitHubPages\n```\n\n**Sources:** [docs/next.config.js:1-10](), [.github/workflows/docs.yml:1-62](), [README.md:9-12]()\n\n---\n\n## Next.js Configuration\n\nThe documentation site uses a specific Next.js configuration optimized for static export and GitHub Pages hosting. The configuration is defined in `next.config.js`:\n\n| Configuration Key | Value | Purpose |\n|------------------|-------|---------|\n| `output` | `'export'` | Enables static HTML export (SSG) |\n| `basePath` | `'/rlm'` | Sets URL prefix for GitHub Pages subpath |\n| `trailingSlash` | `true` | Adds trailing slashes to URLs |\n| `images.unoptimized` | `true` | Disables image optimization for static export |\n\nThe `output: 'export'` setting instructs Next.js to generate a fully static site with no server-side runtime requirements. The `basePath: '/rlm'` configures all routes to be prefixed with `/rlm`, allowing the site to be hosted at `https://alexzhang13.github.io/rlm/` instead of requiring the root domain.\n\n```mermaid\ngraph LR\n    NextConfig[\"next.config.js\"]\n    OutputExport[\"output: 'export'\"]\n    BasePath[\"basePath: '/rlm'\"]\n    TrailingSlash[\"trailingSlash: true\"]\n    ImagesUnopt[\"images.unoptimized: true\"]\n    \n    NextConfig --> OutputExport\n    NextConfig --> BasePath\n    NextConfig --> TrailingSlash\n    NextConfig --> ImagesUnopt\n    \n    OutputExport --> StaticHTML[\"Static HTML Generation\"]\n    BasePath --> GitHubPagesPath[\"github.io/rlm/ routing\"]\n    TrailingSlash --> SEOCompatibility[\"SEO and Link Compatibility\"]\n    ImagesUnopt --> NoServerRequired[\"No Image Optimization Server\"]\n```\n\n**Sources:** [docs/next.config.js:1-10]()\n\n---\n\n## Navigation System\n\nThe documentation site uses a sidebar navigation component implemented in `Sidebar.tsx`. This component provides hierarchical navigation with expandable sections and active page highlighting.\n\n### Sidebar Component Architecture\n\nThe `Sidebar` component implements:\n- **Client-side routing** using Next.js `usePathname` hook [docs/src/components/Sidebar.tsx:4-27]()\n- **Expandable sections** with state management via `useState` [docs/src/components/Sidebar.tsx:27-33]()\n- **Active page highlighting** based on current pathname [docs/src/components/Sidebar.tsx:89-94]()\n- **Nested navigation** for hierarchical documentation structure [docs/src/components/Sidebar.tsx:46-83]()\n\n```mermaid\ngraph TB\n    SidebarComponent[\"Sidebar Component<br/>src/components/Sidebar.tsx\"]\n    NavigationData[\"navigation array<br/>Page structure definition\"]\n    UsePathname[\"usePathname()<br/>Current route detection\"]\n    ExpandedState[\"expandedSections state<br/>Collapsible section control\"]\n    \n    ToggleSection[\"toggleSection()<br/>Expand/collapse handler\"]\n    RenderLogic[\"Conditional rendering<br/>Children vs. flat items\"]\n    \n    SidebarComponent --> NavigationData\n    SidebarComponent --> UsePathname\n    SidebarComponent --> ExpandedState\n    \n    NavigationData --> RenderLogic\n    UsePathname --> ActiveHighlight[\"Active page styling<br/>bg-accent + font-semibold\"]\n    ExpandedState --> ToggleSection\n    ToggleSection --> ChevronRotation[\"ChevronDown rotation<br/>rotate-180 transform\"]\n    \n    RenderLogic --> FlatItems[\"Flat navigation items<br/>name + href\"]\n    RenderLogic --> NestedItems[\"Nested navigation items<br/>name + children[]\"]\n```\n\n**Sources:** [docs/src/components/Sidebar.tsx:1-119]()\n\n### Navigation Structure\n\nThe navigation hierarchy is defined as a static data structure at [docs/src/components/Sidebar.tsx:9-23]():\n\n```\n- Recursive Language Models (/)\n- Using the RLM Client (/api)\n- LM Backends (/backends)\n- REPL Environments (expandable)\n  - Overview (/environments)\n  - LocalREPL (/environments/local)\n  - DockerREPL (/environments/docker)\n  - ModalREPL (/environments/modal)\n- Visualizing RLM Trajectories (/trajectories)\n```\n\nThe sidebar also includes a GitHub link in a fixed footer position [docs/src/components/Sidebar.tsx:103-115](), providing direct access to the repository.\n\n**Sources:** [docs/src/components/Sidebar.tsx:9-23](), [docs/src/components/Sidebar.tsx:103-115]()\n\n---\n\n## Build Process\n\nThe documentation build process uses npm scripts and Next.js tooling to generate static files.\n\n### Build Commands\n\n| Command | Purpose | Location |\n|---------|---------|----------|\n| `npm ci` | Install exact dependencies from lockfile | [.github/workflows/docs.yml:34]() |\n| `npm run build` | Execute Next.js static export | [.github/workflows/docs.yml:38]() |\n| `next build` | Generate static HTML/CSS/JS | Defined in `package.json` |\n\nThe build process creates output in the `docs/out/` directory, which contains:\n- Static HTML pages for each route\n- JavaScript bundles for client-side hydration\n- CSS stylesheets (Tailwind-compiled)\n- Optimized assets and metadata\n\n```mermaid\ngraph TB\n    SourceCode[\"docs/src/<br/>Source Files\"]\n    Dependencies[\"package.json<br/>node_modules/\"]\n    \n    NPMInstall[\"npm ci<br/>Install dependencies\"]\n    NextBuild[\"npm run build<br/>next build\"]\n    \n    StaticGeneration[\"Static Site Generation<br/>HTML + CSS + JS\"]\n    OutputDirectory[\"docs/out/<br/>Build artifacts\"]\n    \n    ArtifactUpload[\"upload-pages-artifact<br/>GitHub Actions\"]\n    \n    Dependencies --> NPMInstall\n    SourceCode --> NextBuild\n    NPMInstall --> NextBuild\n    \n    NextBuild --> StaticGeneration\n    StaticGeneration --> OutputDirectory\n    OutputDirectory --> ArtifactUpload\n```\n\n**Sources:** [.github/workflows/docs.yml:32-38]()\n\n---\n\n## CI/CD Deployment Pipeline\n\nThe documentation site is automatically deployed to GitHub Pages using GitHub Actions. The workflow is defined in `.github/workflows/docs.yml`.\n\n### Workflow Configuration\n\nThe deployment workflow [.github/workflows/docs.yml:1-62]() is triggered by:\n- **Push events** to the `main` branch [.github/workflows/docs.yml:4-5]()\n- **Manual workflow dispatch** [.github/workflows/docs.yml:7]()\n\nThe workflow requires the following permissions [.github/workflows/docs.yml:9-12]():\n- `contents: read` - Read repository contents\n- `pages: write` - Write to GitHub Pages\n- `id-token: write` - Generate deployment tokens\n\n### Deployment Jobs\n\nThe workflow consists of two sequential jobs:\n\n#### 1. Build Job\n\nThe build job [.github/workflows/docs.yml:19-46]() performs the following steps:\n\n```mermaid\ngraph TB\n    Checkout[\"Checkout Repository<br/>actions/checkout@v4\"]\n    SetupNode[\"Setup Node.js 20<br/>actions/setup-node@v4\"]\n    NPMCache[\"npm cache<br/>cache-dependency-path: docs/package.json\"]\n    \n    InstallDeps[\"Install dependencies<br/>npm ci in docs/\"]\n    BuildDocs[\"Build documentation<br/>npm run build in docs/\"]\n    \n    SetupPages[\"Setup Pages<br/>actions/configure-pages@v4\"]\n    UploadArtifact[\"Upload artifact<br/>path: docs/out\"]\n    \n    Checkout --> SetupNode\n    SetupNode --> NPMCache\n    NPMCache --> InstallDeps\n    InstallDeps --> BuildDocs\n    BuildDocs --> SetupPages\n    SetupPages --> UploadArtifact\n```\n\n**Sources:** [.github/workflows/docs.yml:19-46]()\n\n#### 2. Deploy Job\n\nThe deploy job [.github/workflows/docs.yml:48-61]() runs after the build job completes successfully:\n\n| Step | Action | Purpose |\n|------|--------|---------|\n| Wait for build | `needs: build` | Ensures build completes first |\n| Deploy to Pages | `actions/deploy-pages@v4` | Publishes artifact to GitHub Pages |\n\nThe deployed site becomes available at the URL specified in the workflow environment: `github-pages` [.github/workflows/docs.yml:49-51]().\n\n**Sources:** [.github/workflows/docs.yml:48-61]()\n\n### Concurrency Control\n\nThe workflow uses concurrency control [.github/workflows/docs.yml:14-16]() to prevent multiple deployments from interfering:\n- **Group:** `\"pages\"` - All deployments share the same concurrency group\n- **Cancel in progress:** `false` - Queues deployments instead of canceling\n\n**Sources:** [.github/workflows/docs.yml:14-16]()\n\n---\n\n## Local Development\n\nDevelopers can run the documentation site locally for testing and development. The README provides instructions for the visualizer [README.md:151-155](), but the same pattern applies to the documentation site:\n\n```bash\ncd docs/\nnpm install       # Install dependencies (first time only)\nnpm run dev       # Start development server\n```\n\nThe development server typically runs on `http://localhost:3000`, but with the `basePath: '/rlm'` configuration, pages are accessible at `http://localhost:3000/rlm/`.\n\n### Development Workflow\n\n```mermaid\ngraph LR\n    LocalEdit[\"Edit Source Files<br/>docs/src/\"]\n    DevServer[\"npm run dev<br/>Hot reload enabled\"]\n    BrowserPreview[\"Browser Preview<br/>localhost:3000/rlm/\"]\n    \n    BuildTest[\"npm run build<br/>Test production build\"]\n    OutDirectory[\"docs/out/<br/>Verify output\"]\n    \n    LocalEdit --> DevServer\n    DevServer --> BrowserPreview\n    BrowserPreview --> LocalEdit\n    \n    LocalEdit --> BuildTest\n    BuildTest --> OutDirectory\n```\n\n**Sources:** [README.md:151-155](), [docs/next.config.js:1-10]()\n\n---\n\n## Integration with Main Repository\n\nThe documentation system is integrated with the main RLM repository through several touchpoints:\n\n| Component | Integration Point | Description |\n|-----------|------------------|-------------|\n| README | [README.md:10]() | Links to deployed documentation site |\n| Makefile | Not directly referenced | Documentation build is separate from Python tooling |\n| CI/CD | [.github/workflows/docs.yml:1-62]() | Automatic deployment on main branch changes |\n\nThe documentation deployment is **independent** of the Python package build and deployment, allowing documentation updates without requiring package version changes.\n\n**Sources:** [README.md:10](), [.github/workflows/docs.yml:1-62]()\n\n---\n\n## Summary\n\nThe RLM documentation system is a modern, statically-generated site built with Next.js and deployed to GitHub Pages. Key characteristics include:\n\n- **Static Site Generation**: No server runtime required, optimized for fast loading\n- **Automated Deployment**: GitHub Actions workflow handles build and deployment on every main branch push\n- **React-based UI**: Component-driven architecture with TypeScript\n- **Hierarchical Navigation**: Sidebar component with expandable sections for organized content\n- **GitHub Pages Hosting**: Free, reliable hosting with custom domain support via `basePath` configuration\n\nThe system is designed for maintainability, with clear separation between source code (`docs/src/`), build configuration (`next.config.js`), and deployment pipeline (`.github/workflows/docs.yml`).\n\n**Sources:** [docs/next.config.js:1-10](), [docs/src/components/Sidebar.tsx:1-119](), [.github/workflows/docs.yml:1-62](), [README.md:9-12]()\n\n---\n\n# Page: Advanced Topics\n\n# Advanced Topics\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [rlm/core/rlm.py](rlm/core/rlm.py)\n- [rlm/environments/base_env.py](rlm/environments/base_env.py)\n\n</details>\n\n\n\nThis page covers advanced usage patterns and extension points in the RLM system. The four main advanced topics are:\n\n1. **Multi-Backend Configuration**: Route different recursion depths to different LLM providers\n2. **Persistent Environments**: Enable multi-turn conversations with state retention\n3. **Custom Environments**: Implement specialized execution environments\n4. **Usage Tracking**: Monitor token consumption and aggregate costs\n\nThese features build on the core architecture described in [Core Architecture](#3) and provide additional capabilities for production deployments and specialized use cases.\n\n## Advanced Features Overview\n\nThe RLM system provides extension points at multiple layers to support advanced usage patterns.\n\n**Advanced Feature Integration Architecture**\n\n```mermaid\ngraph TB\n    subgraph \"RLM Initialization Parameters\"\n        Backend[\"backend: ClientBackend\"]\n        OtherBackends[\"other_backends: list[ClientBackend]\"]\n        Persistent[\"persistent: bool\"]\n        EnvType[\"environment: EnvironmentType\"]\n        Logger[\"logger: RLMLogger\"]\n    end\n    \n    subgraph \"Multi-Backend System\"\n        LMHandler[\"LMHandler\"]\n        DefaultClient[\"default_client<br/>depth=0\"]\n        OtherClient[\"other_backend_client<br/>depth=1+\"]\n        RegisterClient[\"register_client(model_name, client)\"]\n        \n        Backend --> DefaultClient\n        OtherBackends --> OtherClient\n        DefaultClient --> LMHandler\n        OtherClient --> LMHandler\n        OtherBackends --> RegisterClient\n    end\n    \n    subgraph \"Persistent Environment System\"\n        PersistentEnv[\"_persistent_env: SupportsPersistence\"]\n        UpdateHandler[\"update_handler_address(address)\"]\n        AddContext[\"add_context(payload, index)\"]\n        AddHistory[\"add_history(messages, index)\"]\n        GetContextCount[\"get_context_count()\"]\n        \n        Persistent --> PersistentEnv\n        PersistentEnv --> UpdateHandler\n        PersistentEnv --> AddContext\n        PersistentEnv --> AddHistory\n        PersistentEnv --> GetContextCount\n    end\n    \n    subgraph \"Custom Environment System\"\n        GetEnvironment[\"get_environment(type, kwargs)\"]\n        BaseEnv[\"BaseEnv\"]\n        IsolatedEnv[\"IsolatedEnv\"]\n        NonIsolatedEnv[\"NonIsolatedEnv\"]\n        CustomImpl[\"Custom Implementation\"]\n        \n        EnvType --> GetEnvironment\n        GetEnvironment --> BaseEnv\n        BaseEnv --> NonIsolatedEnv\n        BaseEnv --> IsolatedEnv\n        NonIsolatedEnv --> CustomImpl\n    end\n    \n    subgraph \"Usage Tracking System\"\n        GetUsageSummary[\"lm_handler.get_usage_summary()\"]\n        UsageSummary[\"UsageSummary\"]\n        ModelUsage[\"ModelUsageSummary\"]\n        LogUsage[\"logger.log(iteration)\"]\n        \n        LMHandler --> GetUsageSummary\n        GetUsageSummary --> UsageSummary\n        UsageSummary --> ModelUsage\n        Logger --> LogUsage\n    end\n    \n    style Backend fill:#e1f5ff\n    style Persistent fill:#d4edda\n    style EnvType fill:#fff3cd\n    style Logger fill:#ffe1e1\n```\n\n**Sources:** [rlm/core/rlm.py:40-100](), [rlm/core/lm_handler.py:1-200](), [rlm/environments/base_env.py:1-183](), [rlm/core/types.py:1-100]()\n\n## Feature Categories\n\nThe advanced features address different operational concerns:\n\n| Feature | Primary Use Case | Configuration Complexity | Performance Impact |\n|---------|-----------------|------------------------|-------------------|\n| Multi-Backend | Cost optimization, model routing | Medium | Reduces cost for sub-calls |\n| Persistent Environments | Multi-turn conversations | Low | Minimal overhead |\n| Custom Environments | Specialized execution contexts | High | Varies by implementation |\n| Usage Tracking | Cost monitoring, debugging | Low | Negligible overhead |\n\n### Multi-Backend Configuration\n\nThe `other_backends` parameter enables hierarchical model selection based on recursion depth. This is detailed in [Multi-Backend Configuration](#8.1).\n\n**Key Parameters:**\n- `backend`: Primary LLM backend for depth=0 calls\n- `other_backends`: List of backends for depth=1+ calls (currently limited to one)\n- `other_backend_kwargs`: Corresponding configuration for each backend\n\n**Code Integration Points:**\n- [rlm/core/rlm.py:50-51](): Parameter declarations\n- [rlm/core/rlm.py:79-88](): Validation logic (currently enforces single additional backend)\n- [rlm/core/rlm.py:136-147](): Client creation and registration\n\n**Example Use Case:**\n```python\nrlm = RLM(\n    backend=\"openai\",\n    backend_kwargs={\"model_name\": \"gpt-4\", \"api_key\": api_key},\n    other_backends=[\"openai\"],\n    other_backend_kwargs=[{\"model_name\": \"gpt-3.5-turbo\", \"api_key\": api_key}]\n)\n```\n\n**Sources:** [rlm/core/rlm.py:40-147]()\n\n### Persistent Environments and Multi-Turn Sessions\n\nThe `persistent` parameter enables environment reuse across multiple `completion()` calls, supporting multi-turn conversations with state retention. This is detailed in [Persistent Environments and Multi-Turn Sessions](#8.2).\n\n**Key Parameters:**\n- `persistent`: Boolean flag to enable persistence mode\n- Environment must implement `SupportsPersistence` protocol\n\n**Code Integration Points:**\n- [rlm/core/rlm.py:54](): Parameter declaration\n- [rlm/core/rlm.py:98-103](): Validation at initialization\n- [rlm/core/rlm.py:152-171](): Environment reuse logic in `_spawn_completion_context()`\n- [rlm/core/rlm.py:257-258](), [rlm/core/rlm.py:284-285](): History storage after completion\n\n**Protocol Requirements:**\nThe `SupportsPersistence` protocol [rlm/environments/base_env.py:76-182]() defines five required methods:\n- `update_handler_address(address)`: Update LM handler connection\n- `add_context(payload, index)`: Add context payload with versioning\n- `get_context_count()`: Return number of loaded contexts\n- `add_history(messages, index)`: Store conversation history\n- `get_history_count()`: Return number of stored histories\n\n**Example Use Case:**\n```python\nwith RLM(backend=\"openai\", backend_kwargs=kwargs, persistent=True) as rlm:\n    answer1 = rlm.completion(\"First question\")\n    answer2 = rlm.completion(\"Follow-up question\")  # Retains context\n```\n\n**Sources:** [rlm/core/rlm.py:54-103](), [rlm/core/rlm.py:125-179](), [rlm/environments/base_env.py:76-182]()\n\n### Custom Environments\n\nUsers can implement custom execution environments by extending `BaseEnv`, `IsolatedEnv`, or `NonIsolatedEnv`. This is detailed in [Custom Environments](#8.3).\n\n**Abstract Base Classes:**\n- `BaseEnv` [rlm/environments/base_env.py:7-29](): Minimal interface (setup, load_context, execute_code)\n- `NonIsolatedEnv` [rlm/environments/base_env.py:53-73](): Same-machine execution\n- `IsolatedEnv` [rlm/environments/base_env.py:31-51](): Cloud-based sandbox execution\n\n**Required Methods:**\n- `setup()`: Initialize environment resources\n- `load_context(context_payload)`: Load context into execution namespace\n- `execute_code(code)`: Execute code and return `REPLResult`\n\n**Optional Protocol:**\n- `SupportsPersistence`: Enable multi-turn support (see above)\n\n**Registration:**\nCustom environments are registered via the `get_environment()` factory in `rlm/environments/__init__.py`.\n\n**Sources:** [rlm/environments/base_env.py:1-183](), [rlm/environments/__init__.py]()\n\n### Usage Tracking and Cost Monitoring\n\nThe RLM system automatically tracks token usage and aggregates costs across all LLM calls. This is detailed in [Usage Tracking and Cost Monitoring](#8.4).\n\n**Data Structures:**\n- `ModelUsageSummary` [rlm/core/types.py]: Per-model token counts\n- `UsageSummary` [rlm/core/types.py]: Aggregated usage across all models\n- `RLMChatCompletion` [rlm/core/types.py]: Includes usage summary for each completion\n\n**Access Points:**\n- `lm_handler.get_usage_summary()`: Get current usage statistics\n- `RLMChatCompletion.usage_summary`: Usage data in completion response\n- Logger integration: Usage automatically logged with each iteration\n\n**Code Integration Points:**\n- [rlm/core/rlm.py:252-254](): Usage retrieval after completion\n- [rlm/core/rlm.py:260-268](): Usage included in `RLMChatCompletion` response\n- [rlm/core/lm_handler.py]: Usage tracking implementation\n\n**Sources:** [rlm/core/rlm.py:192-295](), [rlm/core/types.py:1-100](), [rlm/core/lm_handler.py]()\n\n## Feature Interaction Patterns\n\n**Multi-Turn with Multi-Backend**\n\nPersistent mode works with multi-backend configuration:\n\n```mermaid\ngraph LR\n    subgraph \"First Completion\"\n        C1[\"rlm.completion(prompt1)\"]\n        H1[\"LMHandler created\"]\n        E1[\"Environment created\"]\n        Depth0_1[\"depth=0: backend\"]\n        Depth1_1[\"depth=1: other_backends[0]\"]\n        \n        C1 --> H1\n        C1 --> E1\n        H1 --> Depth0_1\n        H1 --> Depth1_1\n    end\n    \n    subgraph \"Second Completion\"\n        C2[\"rlm.completion(prompt2)\"]\n        H2[\"New LMHandler created\"]\n        E2[\"Same environment reused\"]\n        UpdateAddr[\"environment.update_handler_address()\"]\n        AddContext[\"environment.add_context(prompt2)\"]\n        Depth0_2[\"depth=0: backend\"]\n        Depth1_2[\"depth=1: other_backends[0]\"]\n        \n        C2 --> H2\n        C2 --> E2\n        E2 --> UpdateAddr\n        E2 --> AddContext\n        H2 --> Depth0_2\n        H2 --> Depth1_2\n    end\n    \n    E1 -.->|\"persistent=True\"| E2\n```\n\n**Sources:** [rlm/core/rlm.py:125-179]()\n\n**Custom Environment with Usage Tracking**\n\nCustom environments inherit usage tracking automatically:\n\n```mermaid\ngraph TB\n    CustomEnv[\"Custom Environment<br/>extends NonIsolatedEnv\"]\n    ExecuteCode[\"execute_code(code)\"]\n    LLMQuery[\"llm_query() in executed code\"]\n    LMHandler[\"LMHandler.completion()\"]\n    UsageTracking[\"Automatic usage tracking\"]\n    REPLResult[\"REPLResult with nested<br/>RLMChatCompletion objects\"]\n    \n    CustomEnv --> ExecuteCode\n    ExecuteCode --> LLMQuery\n    LLMQuery --> LMHandler\n    LMHandler --> UsageTracking\n    LMHandler --> REPLResult\n```\n\n**Sources:** [rlm/environments/base_env.py:1-73](), [rlm/core/types.py]()\n\n## When to Use Advanced Features\n\n**Decision Tree:**\n\n```mermaid\ngraph TD\n    Start[\"Need advanced features?\"]\n    \n    MultiTurn{\"Multi-turn<br/>conversations?\"}\n    CostOpt{\"Need cost<br/>optimization?\"}\n    SpecialExec{\"Special execution<br/>requirements?\"}\n    Monitor{\"Need detailed<br/>monitoring?\"}\n    \n    UsePersistent[\"Use persistent=True<br/>See section 8.2\"]\n    UseMultiBackend[\"Use other_backends<br/>See section 8.1\"]\n    UseCustomEnv[\"Implement custom environment<br/>See section 8.3\"]\n    UseTracking[\"Enable logger<br/>See section 8.4\"]\n    UseBasic[\"Use basic RLM configuration<br/>See Getting Started #2\"]\n    \n    Start --> MultiTurn\n    MultiTurn -->|\"Yes\"| UsePersistent\n    MultiTurn -->|\"No\"| CostOpt\n    \n    CostOpt -->|\"Yes\"| UseMultiBackend\n    CostOpt -->|\"No\"| SpecialExec\n    \n    SpecialExec -->|\"Yes\"| UseCustomEnv\n    SpecialExec -->|\"No\"| Monitor\n    \n    Monitor -->|\"Yes\"| UseTracking\n    Monitor -->|\"No\"| UseBasic\n```\n\n**Feature Selection Guidelines:**\n\n| Scenario | Recommended Features | Configuration Priority |\n|----------|---------------------|----------------------|\n| Production chatbot | Persistent + Usage Tracking | High reliability, cost monitoring |\n| Research experiments | Multi-Backend + Usage Tracking | Cost optimization, detailed metrics |\n| Specialized domain | Custom Environment | Domain-specific execution context |\n| Cost-sensitive deployment | Multi-Backend + Usage Tracking | Minimize token costs |\n| Simple one-off queries | None (basic RLM) | Simplicity |\n\n## Implementation Checklist\n\nBefore implementing advanced features, ensure you understand:\n\n- [ ] **Multi-Backend**: Depth-based routing model, current single-backend limitation\n- [ ] **Persistent Mode**: `SupportsPersistence` protocol requirements, supported environments\n- [ ] **Custom Environments**: Abstract base class contracts, communication protocols\n- [ ] **Usage Tracking**: Data structure hierarchy, aggregation mechanics\n\nEach advanced topic is covered in detail in its respective subsection:\n- [Multi-Backend Configuration](#8.1)\n- [Persistent Environments and Multi-Turn Sessions](#8.2)\n- [Custom Environments](#8.3)\n- [Usage Tracking and Cost Monitoring](#8.4)\n\n**Sources:** [rlm/core/rlm.py:1-400](), [rlm/environments/base_env.py:1-183](), [rlm/core/types.py:1-200]()\n</old_str>\n\n<old_str>\n## Advanced Feature Implementation Patterns\n\nUnderstanding how the advanced features are implemented in the codebase helps when extending or customizing the RLM system.\n\n### Initialization Pattern\n\nAll advanced features are configured through the `RLM.__init__()` method [rlm/core/rlm.py:40-124]():\n\n```mermaid\ngraph TD\n    Init[\"RLM.__init__()\"]\n    \n    subgraph \"Parameter Processing\"\n        StoreConfig[\"Store configuration<br/>lines 73-93\"]\n        ValidateBackends[\"Validate other_backends<br/>lines 79-88\"]\n        ValidatePersist[\"Validate persistent mode<br/>lines 101-103\"]\n    end\n    \n    subgraph \"Initialization\"\n        InitPersistEnv[\"self._persistent_env = None<br/>line 99\"]\n        CreateLogger[\"Create VerbosePrinter<br/>line 95\"]\n        LogMetadata[\"Log RLMMetadata<br/>lines 106-123\"]\n    end\n    \n    Init --> StoreConfig\n    Init --> ValidateBackends\n    Init --> ValidatePersist\n    Init --> InitPersistEnv\n    Init --> CreateLogger\n    Init --> LogMetadata\n```\n\n**Parameter Storage:**\n- Configuration stored as instance variables (lines 73-93)\n- No immediate client or environment creation\n- Validation performed at initialization time\n\n**Sources:** [rlm/core/rlm.py:40-124]()\n\n### Per-Completion Setup Pattern\n\nEach `completion()` call spawns fresh resources using the `_spawn_completion_context()` context manager [rlm/core/rlm.py:125-179]():\n\n```mermaid\ngraph TD\n    SpawnContext[\"_spawn_completion_context()\"]\n    \n    subgraph \"Client Creation\"\n        CreateDefault[\"Create default client<br/>line 134\"]\n        CreateOther[\"Create other_backend_client<br/>lines 137-139\"]\n        RegisterClients[\"Register clients by model name<br/>lines 144-147\"]\n        StartHandler[\"lm_handler.start()<br/>line 149\"]\n    end\n    \n    subgraph \"Environment Creation\"\n        CheckPersist{\"persistent and<br/>_persistent_env exists?\"}\n        ReuseEnv[\"Reuse environment<br/>lines 152-162\"]\n        CreateEnv[\"Create fresh environment<br/>lines 164-171\"]\n        \n        UpdateAddr[\"environment.update_handler_address()<br/>line 161\"]\n        AddContext[\"environment.add_context(prompt)<br/>line 162\"]\n    end\n    \n    subgraph \"Cleanup\"\n        Yield[\"yield lm_handler, environment<br/>line 174\"]\n        StopHandler[\"lm_handler.stop()<br/>line 176\"]\n        MaybeCleanup[\"Cleanup non-persistent env<br/>lines 177-178\"]\n    end\n    \n    SpawnContext --> CreateDefault\n    CreateDefault --> CreateOther\n    CreateOther --> RegisterClients\n    RegisterClients --> StartHandler\n    \n    StartHandler --> CheckPersist\n    CheckPersist -->|\"Yes\"| ReuseEnv\n    CheckPersist -->|\"No\"| CreateEnv\n    \n    ReuseEnv --> UpdateAddr\n    UpdateAddr --> AddContext\n    \n    AddContext --> Yield\n    CreateEnv --> Yield\n    \n    Yield --> StopHandler\n    StopHandler --> MaybeCleanup\n```\n\n**Key Behaviors:**\n- LMHandler created fresh for each completion (new TCP port)\n- Environment reused if `persistent=True`, otherwise created fresh\n- Context manager ensures cleanup even on exceptions\n\n**Sources:** [rlm/core/rlm.py:125-179]()\n\n### History Management Pattern\n\nPersistent environments store conversation history after successful completion [rlm/core/rlm.py:250-295]():\n\n```mermaid\ngraph LR\n    FindAnswer[\"find_final_answer()<br/>line 240\"]\n    CheckAnswer{\"final_answer<br/>is not None?\"}\n    StoreHistory[\"environment.add_history(message_history)<br/>lines 257-258, 284-285\"]\n    Return[\"Return RLMChatCompletion<br/>lines 260-268\"]\n    \n    FindAnswer --> CheckAnswer\n    CheckAnswer -->|\"Yes\"| StoreHistory\n    CheckAnswer -->|\"No\"| Continue[\"Continue iteration loop\"]\n    StoreHistory --> Return\n```\n\n**Storage Timing:**\n- History stored only after finding final answer or reaching max iterations\n- Stored after usage summary collected\n- Stored before returning `RLMChatCompletion`\n\n**Sources:** [rlm/core/rlm.py:240-295]()\n\n## Extension Guidelines\n\nWhen implementing custom advanced features, follow these architectural patterns:\n\n### Custom Backend Integration\n\nTo add a new backend type to multi-backend configuration:\n\n1. Implement `BaseLM` interface [rlm/clients/base_lm.py]()\n2. Register in `get_client()` factory [rlm/clients/__init__.py]()\n3. Pass as `other_backends` parameter\n4. Backend automatically registered by model name in LMHandler\n\n### Custom Persistent Environment\n\nTo implement a new environment with persistence support:\n\n1. Extend `NonIsolatedEnv` or `IsolatedEnv` [rlm/environments/base_env.py]()\n2. Implement `SupportsPersistence` protocol methods:\n   - `update_handler_address()`\n   - `add_context()` / `get_context_count()`\n   - `add_history()` / `get_history_count()`\n3. Register in `get_environment()` factory\n4. Add to `persistent_supported_environments` set [rlm/core/rlm.py:372]()\n\n**Example Structure:**\n```\nclass CustomPersistentEnv(NonIsolatedEnv):\n    def __init__(self, ...):\n        self._contexts = []\n        self._histories = []\n        self._handler_address = None\n    \n    def update_handler_address(self, address):\n        self._handler_address = address\n    \n    def add_context(self, payload, index=None):\n        # Implementation with versioning\n        pass\n    \n    def get_context_count(self):\n        return len(self._contexts)\n    \n    # ... other required methods\n```\n\n**Sources:** [rlm/environments/base_env.py:76-182](), [rlm/core/rlm.py:357-381]()\n\n## Next Steps\n\nFor detailed implementation guides and examples for each advanced topic, see:\n\n- **[Multi-Backend Configuration](#8.1)**: Depth-based model routing, cost optimization strategies\n- **[Persistent Environments and Multi-Turn Sessions](#8.2)**: Context versioning, history management, session lifecycle\n- **[Custom Environments](#8.3)**: Implementing specialized execution contexts, communication protocols\n- **[Usage Tracking and Cost Monitoring](#8.4)**: Token counting, cost aggregation, monitoring integration\n\n**Sources:** [rlm/core/rlm.py:1-400](), [rlm/environments/base_env.py:1-183]()\n\n\n---\n\n## Recursion Depth Architecture\n\nThe current implementation limits recursion to depth=1 through the `Sub_RLM` class. Understanding this architecture is essential for extension.\n\n**Current Recursion Model (depth=1)**\n\n```mermaid\ngraph TB\n    User[\"User Query\"]\n    \n    subgraph \"Depth 0: Root Level\"\n        RLMRoot[\"RLM_REPL instance<br/>model: gpt-5-nano\"]\n        REPLRoot[\"REPLEnv instance\"]\n        RLMRoot -->|\"manages\"| REPLRoot\n    end\n    \n    subgraph \"Depth 1: Recursive Level\"\n        SubRLM1[\"Sub_RLM instance<br/>model: gpt-5\"]\n        LLMQuery[\"llm_query() function<br/>(bound to Sub_RLM)\"]\n        REPLRoot -->|\"contains\"| SubRLM1\n        REPLRoot -->|\"exposes\"| LLMQuery\n        LLMQuery -->|\"calls\"| SubRLM1\n    end\n    \n    subgraph \"Hard Limit\"\n        BlockedCall[\"❌ Cannot make<br/>further recursive calls\"]\n        SubRLM1 -.->|\"no REPL env\"| BlockedCall\n    end\n    \n    User -->|\"query\"| RLMRoot\n    \n    Note1[\"Note: Sub_RLM.completion()<br/>makes simple API call\"]\n    Note2[\"Note: No REPLEnv in Sub_RLM<br/>= No llm_query available\"]\n    \n    SubRLM1 -.-> Note1\n    SubRLM1 -.-> Note2\n```\n\n**Key Architectural Constraint**\n\nThe `Sub_RLM` class [rlm/repl.py:15-53]() provides a minimal `completion()` method that directly calls the LLM API without creating a REPL environment. This design choice prevents infinite recursion but limits expressiveness.\n\n```\nREPLEnv.__init__():\n  - Line 86-87: Creates Sub_RLM(model=recursive_model)\n  - Line 169-171: Defines llm_query() that calls self.sub_rlm.completion()\n  - Line 174: Adds llm_query to globals\n  \nSub_RLM.completion():\n  - Line 31-43: Simple API call, no REPL, no further recursion\n```\n\n**Sources:** [rlm/repl.py:15-53](), [rlm/repl.py:86-87](), [rlm/repl.py:169-174](), [README.md:9]()\n\n---\n\n## Extending Recursion Beyond Depth=1\n\nTo enable arbitrary recursion depth, replace `Sub_RLM` with `RLM_REPL` in the REPL environment. This section explains the technical approach and trade-offs. For a step-by-step guide, see [Extending to Deeper Recursion](#5.1).\n\n**Recursive Architecture (arbitrary depth)**\n\n```mermaid\ngraph TB\n    subgraph \"Depth 0\"\n        R0[\"RLM_REPL<br/>depth=0\"]\n        E0[\"REPLEnv<br/>depth=0\"]\n        R0 -->|\"manages\"| E0\n    end\n    \n    subgraph \"Depth 1\"\n        R1[\"RLM_REPL<br/>depth=1\"]\n        E1[\"REPLEnv<br/>depth=1\"]\n        E0 -->|\"contains\"| R1\n        R1 -->|\"manages\"| E1\n    end\n    \n    subgraph \"Depth 2\"\n        R2[\"RLM_REPL<br/>depth=2\"]\n        E2[\"REPLEnv<br/>depth=2\"]\n        E1 -->|\"contains\"| R2\n        R2 -->|\"manages\"| E2\n    end\n    \n    subgraph \"Depth N\"\n        RN[\"RLM_REPL<br/>depth=N\"]\n        EN[\"REPLEnv<br/>depth=N\"]\n        R2 -.->|\"...\"| RN\n        RN -->|\"manages\"| EN\n    end\n    \n    Note1[\"Each RLM_REPL has its own<br/>REPL environment\"]\n    Note2[\"Each REPL can make<br/>further recursive calls\"]\n    Note3[\"Recursion depth limited by<br/>config or termination logic\"]\n```\n\n**Implementation Modifications**\n\nThe core change involves modifying `REPLEnv.__init__()` [rlm/repl.py:72-198]():\n\n| Current Implementation | Extended Implementation |\n|----------------------|------------------------|\n| `self.sub_rlm = Sub_RLM(model=recursive_model)` | `self.sub_rlm = RLM_REPL(model=model, recursive_model=recursive_model, ...)` |\n| Simple API call | Full RLM_REPL with REPL environment |\n| No further recursion possible | Arbitrary recursion depth |\n\n**Technical Considerations**\n\n1. **Namespace Management**: Each REPL level maintains its own `globals` and `locals` dictionaries [rlm/repl.py:90-161](). With nested REPLs, context isolation becomes critical.\n\n2. **Working Directory Conflicts**: Each `REPLEnv` creates a temporary directory [rlm/repl.py:82-83](). Nested REPLs create nested temp directories, requiring careful path management.\n\n3. **Execution Context**: The `_temp_working_directory()` context manager [rlm/repl.py:254-262]() temporarily changes `os.getcwd()`. Nested REPL calls may conflict without proper restoration.\n\n4. **Termination Logic**: Without a depth limit, infinite recursion is possible. Implement termination conditions based on:\n   - Maximum depth counter\n   - Token budget exhaustion\n   - Time limits\n   - Task completion detection\n\n**Sources:** [rlm/repl.py:15-53](), [rlm/repl.py:72-198](), [README.md:9](), [README.md:21]()\n\n---\n\n## Custom RLM Implementations\n\nThe `RLM` abstract base class [rlm/rlm.py:3-14]() defines a minimal interface for creating custom implementations. This section explains the contract and common patterns. For implementation details, see [Creating Custom RLM Implementations](#5.2).\n\n**RLM Interface Contract**\n\n```mermaid\nclassDiagram\n    class RLM {\n        <<abstract>>\n        +completion(context, query) str\n        +cost_summary() dict\n        +reset() void\n    }\n    \n    class RLM_REPL {\n        -model: str\n        -recursive_model: str\n        -max_iterations: int\n        -repl: REPLEnv\n        -client: OpenAIClient\n        -messages: list\n        +completion(context, query) str\n        +cost_summary() dict\n        +reset() void\n        -_setup_context(context, query)\n    }\n    \n    class Sub_RLM {\n        -model: str\n        -api_key: str\n        -client: OpenAIClient\n        +completion(prompt) str\n        +cost_summary() NotImplemented\n        +reset() NotImplemented\n    }\n    \n    class CustomRLM {\n        +completion(context, query) str\n        +cost_summary() dict\n        +reset() void\n        +custom_methods()\n    }\n    \n    RLM <|-- RLM_REPL\n    RLM <|-- Sub_RLM\n    RLM <|-- CustomRLM\n```\n\n**Method Requirements**\n\n| Method | Signature | Purpose | Notes |\n|--------|-----------|---------|-------|\n| `completion()` | `(context, query) -> str` | Primary query interface | Must handle string/list/dict context |\n| `cost_summary()` | `() -> dict[str, float]` | Return cost metrics | Can raise `NotImplementedError` |\n| `reset()` | `() -> void` | Reset internal state | Can raise `NotImplementedError` |\n\n**Implementation Patterns**\n\nThe system includes two reference implementations:\n\n1. **RLM_REPL Pattern** [rlm/rlm_repl.py](): Full-featured implementation with:\n   - REPL environment management\n   - Iterative processing loop (up to `max_iterations`)\n   - Message history tracking\n   - Code execution and result formatting\n\n2. **Sub_RLM Pattern** [rlm/repl.py:15-53](): Minimal implementation with:\n   - Direct API calls\n   - No REPL environment\n   - Simplified error handling\n   - Stubbed cost/reset methods\n\n**Sources:** [rlm/rlm.py:1-15](), [rlm/rlm_repl.py](), [rlm/repl.py:15-53]()\n\n---\n\n## REPL Environment Customization\n\nThe `REPLEnv` class [rlm/repl.py:71-360]() provides sandboxed Python execution. Customization strategies include modifying the built-ins whitelist, execution behavior, and context loading.\n\n**REPLEnv Architecture**\n\n```mermaid\ngraph LR\n    subgraph \"Initialization\"\n        Init[\"REPLEnv.__init__()\"]\n        TempDir[\"Create temp directory<br/>(tempfile.mkdtemp)\"]\n        SubRLM[\"Initialize sub_rlm\"]\n        Globals[\"Setup restricted globals\"]\n        Context[\"Load context files\"]\n        Init --> TempDir\n        Init --> SubRLM\n        Init --> Globals\n        Init --> Context\n    end\n    \n    subgraph \"Restricted Execution\"\n        Exec[\"code_execution()\"]\n        CaptureOut[\"_capture_output()<br/>(io.StringIO)\"]\n        TempCWD[\"_temp_working_directory()<br/>(os.chdir)\"]\n        ExecCode[\"exec() / eval()\"]\n        Exec --> CaptureOut\n        Exec --> TempCWD\n        Exec --> ExecCode\n    end\n    \n    subgraph \"Special Functions\"\n        LLMQuery[\"llm_query()<br/>(in globals)\"]\n        FinalVar[\"FINAL_VAR()<br/>(in globals)\"]\n    end\n    \n    Init --> Exec\n    Globals --> LLMQuery\n    Globals --> FinalVar\n```\n\n**Built-ins Whitelist Strategy**\n\nThe REPL restricts available Python built-ins [rlm/repl.py:90-161]() by providing a custom `__builtins__` dictionary. Key categories:\n\n| Category | Examples | Security Rationale |\n|----------|----------|-------------------|\n| **Allowed** | `print`, `len`, `str`, `int`, `list`, `dict`, `open`, `__import__` | Safe for string manipulation and file I/O within temp directory |\n| **Blocked** | `eval`, `exec`, `compile`, `input`, `globals`, `locals` | Prevent code injection and namespace inspection |\n| **Extended** | `range`, `any`, `all`, `hasattr`, `enumerate`, `zip` | Common Python patterns needed for LLM-generated code |\n\n**Customization Strategy**: Modify the `__builtins__` dictionary in `REPLEnv.__init__()` to:\n- Add domain-specific functions\n- Remove potentially dangerous operations\n- Inject custom libraries or modules\n\n**Context Loading Mechanisms**\n\nThe REPL supports two context types [rlm/repl.py:200-223]():\n\n1. **JSON Context** (`context_json` parameter):\n   - Serialized to `context.json` in temp directory\n   - Loaded via `json.load()`\n   - Accessible as `context` variable in REPL\n\n2. **String Context** (`context_str` parameter):\n   - Written to `context.txt` in temp directory\n   - Loaded via `f.read()`\n   - Accessible as `context` variable in REPL\n\n**Custom Context Loading**: Extend `load_context()` method to support:\n- Database connections\n- Remote file fetching\n- Custom serialization formats\n- Lazy loading for large contexts\n\n**Sources:** [rlm/repl.py:71-360](), [rlm/repl.py:90-161](), [rlm/repl.py:200-223]()\n\n---\n\n## Execution Model Modifications\n\nThe REPL execution model [rlm/repl.py:264-356]() implements notebook-style code evaluation with specific handling for imports, expressions, and statements.\n\n**Code Execution Pipeline**\n\n```mermaid\nflowchart TD\n    Start[\"code_execution(code)\"]\n    Split[\"Split into lines\"]\n    Classify[\"Classify:<br/>imports vs other\"]\n    \n    ExecImports[\"Execute imports<br/>in globals namespace\"]\n    ExecOther[\"Execute other code<br/>in combined namespace\"]\n    \n    CheckLast[\"Last line<br/>expression?\"]\n    EvalExpr[\"eval() last line<br/>print result\"]\n    ExecStmt[\"exec() all code\"]\n    \n    UpdateLocals[\"Update self.locals<br/>with new variables\"]\n    Return[\"Return REPLResult\"]\n    \n    Start --> Split\n    Split --> Classify\n    Classify --> ExecImports\n    ExecImports --> ExecOther\n    \n    ExecOther --> CheckLast\n    CheckLast -->|\"Yes\"| EvalExpr\n    CheckLast -->|\"No\"| ExecStmt\n    \n    EvalExpr --> UpdateLocals\n    ExecStmt --> UpdateLocals\n    UpdateLocals --> Return\n```\n\n**Key Execution Behaviors**\n\n1. **Import Handling** [rlm/repl.py:273-286]():\n   - Imports executed in `self.globals` to make modules available\n   - Other code executed in combined `globals + locals` namespace\n   - Pattern: `line.startswith(('import ', 'from '))`\n\n2. **Expression Evaluation** [rlm/repl.py:294-333]():\n   - Last non-comment line evaluated as expression if possible\n   - Result printed via `repr()` if not None\n   - Fallback to statement execution on error\n   - Detection: not starting with statements keywords, no `=`, no trailing `:`\n\n3. **Namespace Management** [rlm/repl.py:338-341]():\n   - Combined namespace: `{**self.globals, **self.locals}`\n   - Variables created during execution added to `self.locals`\n   - Imports remain in `self.globals`\n\n**Customization Opportunities**\n\n| Aspect | Current Behavior | Customization Strategy |\n|--------|------------------|----------------------|\n| Import restrictions | All imports allowed | Hook `__import__` to filter modules |\n| Expression auto-print | Last line evaluated | Disable or modify printing logic |\n| Execution timeout | No timeout | Wrap `exec()` with signal/threading timeout |\n| Variable persistence | All variables persist | Clear namespace between executions |\n| Error handling | Return error in stderr | Custom exception handling/logging |\n\n**Sources:** [rlm/repl.py:264-356](), [rlm/repl.py:273-286](), [rlm/repl.py:294-333]()\n\n---\n\n## Custom Prompt Strategies\n\nThe prompt system [rlm/utils/prompts.py]() uses a two-level architecture: system prompts define capabilities, user prompts guide iteration behavior.\n\n**Prompt Engineering Architecture**\n\n```mermaid\ngraph TB\n    subgraph \"System Level\"\n        SysPrompt[\"REPL_SYSTEM_PROMPT\"]\n        Capabilities[\"Defines:<br/>- context variable<br/>- llm_query function<br/>- REPL execution<br/>- FINAL syntax\"]\n    end\n    \n    subgraph \"Iteration Level\"\n        UserPrompt[\"next_action_prompt()\"]\n        Branch{iteration == 0?}\n        Initial[\"Initial prompt:<br/>Think step-by-step\"]\n        Followup[\"Follow-up prompt:<br/>Continue execution\"]\n        ForceFinal[\"Final prompt:<br/>Force answer\"]\n    end\n    \n    subgraph \"Response Processing\"\n        Parse[\"find_code_blocks()\"]\n        Extract[\"Extract ```repl``` blocks\"]\n        CheckFinal[\"check_for_final_answer()\"]\n        FinalMatch[\"Match FINAL() or FINAL_VAR()\"]\n    end\n    \n    SysPrompt --> UserPrompt\n    UserPrompt --> Branch\n    Branch -->|\"Yes\"| Initial\n    Branch -->|\"No\"| Followup\n    Followup --> ForceFinal\n    \n    Initial --> Parse\n    Followup --> Parse\n    ForceFinal --> Parse\n    \n    Parse --> Extract\n    Parse --> CheckFinal\n    CheckFinal --> FinalMatch\n```\n\n**Prompt Template Customization**\n\nThe system expects specific response formats from the LLM:\n\n1. **Code Blocks**: ````repl\\n{code}\\n```\n2. **Final Answers**: `FINAL({answer})` or `FINAL_VAR({variable_name})`\n\nTo customize prompts:\n\n| Customization Goal | Implementation Approach |\n|-------------------|------------------------|\n| Different code block syntax | Modify `find_code_blocks()` regex [rlm/utils/utils.py]() |\n| Alternative final answer format | Modify `check_for_final_answer()` [rlm/utils/utils.py]() |\n| Task-specific instructions | Extend `REPL_SYSTEM_PROMPT` with domain knowledge |\n| Dynamic prompting | Parameterize `next_action_prompt()` with context |\n| Multi-turn conversations | Track conversation history in message list |\n\n**Prompt Function Signatures**\n\n- `next_action_prompt(iteration: int, final_answer: bool = False) -> str`\n- System prompt is static string, user prompt is dynamically generated\n\n**Sources:** [rlm/utils/prompts.py](), [rlm/utils/utils.py](), [rlm/rlm_repl.py]()\n\n---\n\n## Alternative LLM Provider Integration\n\nThe system uses `OpenAIClient` [rlm/utils/llm.py]() for API communication. Replacing this with alternative providers requires implementing a compatible interface.\n\n**LLM Client Architecture**\n\n```mermaid\ngraph TB\n    subgraph \"Current Implementation\"\n        OpenAIClient[\"OpenAIClient<br/>(utils/llm.py)\"]\n        OpenAIAPI[\"OpenAI API<br/>(chat.completions)\"]\n        OpenAIClient -->|\"HTTP POST\"| OpenAIAPI\n    end\n    \n    subgraph \"RLM Usage\"\n        RLMImpl[\"RLM_REPL\"]\n        SubRLM[\"Sub_RLM\"]\n        RLMImpl -->|\"uses\"| OpenAIClient\n        SubRLM -->|\"uses\"| OpenAIClient\n    end\n    \n    subgraph \"Alternative Providers\"\n        CustomClient1[\"AnthropicClient\"]\n        CustomClient2[\"LocalLLMClient\"]\n        CustomClient3[\"AzureOpenAIClient\"]\n        \n        AnthropicAPI[\"Anthropic API\"]\n        LocalAPI[\"Local Model<br/>(vLLM, llama.cpp)\"]\n        AzureAPI[\"Azure OpenAI\"]\n        \n        CustomClient1 --> AnthropicAPI\n        CustomClient2 --> LocalAPI\n        CustomClient3 --> AzureAPI\n    end\n    \n    RLMImpl -.->|\"can use\"| CustomClient1\n    RLMImpl -.->|\"can use\"| CustomClient2\n    SubRLM -.->|\"can use\"| CustomClient3\n```\n\n**Client Interface Requirements**\n\nTo replace `OpenAIClient`, implement a class with:\n\n```\nRequired Methods:\n- __init__(api_key, model, **kwargs)\n- completion(messages, timeout=None, **kwargs) -> str\n  - Accept messages as: list[dict], str, or dict\n  - Return: string response content\n  - Handle: timeouts, errors, retries\n```\n\n**Integration Points**\n\n1. **RLM_REPL** [rlm/rlm_repl.py]():\n   - Replace import: `from rlm.utils.llm import OpenAIClient`\n   - Modify initialization: `self.client = CustomClient(...)`\n\n2. **Sub_RLM** [rlm/repl.py:15-53]():\n   - Replace import in `Sub_RLM.__init__()`\n   - Update `self.client = CustomClient(...)`\n\n3. **REPLEnv** [rlm/repl.py:86-87]():\n   - Modify `Sub_RLM` initialization or pass custom client\n\n**Provider-Specific Considerations**\n\n| Provider | Key Differences | Integration Notes |\n|----------|----------------|-------------------|\n| Anthropic Claude | Different message format, tool calling | Convert OpenAI format to Claude format |\n| Azure OpenAI | Requires deployment name, different endpoint | Change base URL, add deployment parameter |\n| Local models | No API key, different response format | Handle local execution, adjust timeout |\n| HuggingFace | Token-based auth, inference endpoints | Replace authentication mechanism |\n\n**Sources:** [rlm/utils/llm.py](), [rlm/rlm_repl.py](), [rlm/repl.py:15-53]()\n\n---\n\n## Cost Tracking and Optimization\n\nThe current implementation stubs out cost tracking methods [rlm/repl.py:48-49](), [rlm/repl.py:358-359](). Implementing comprehensive cost tracking requires token counting and API response parsing.\n\n**Cost Tracking Architecture**\n\n```mermaid\ngraph TB\n    subgraph \"Cost Tracking Components\"\n        TokenCounter[\"Token Counter<br/>(tiktoken or similar)\"]\n        PriceDB[\"Price Database<br/>(model -> price/token)\"]\n        CostAccum[\"Cost Accumulator<br/>(running totals)\"]\n    end\n    \n    subgraph \"Integration Points\"\n        ClientCall[\"LLM client.completion()\"]\n        REPLExec[\"REPL code_execution()\"]\n        RLMComplete[\"RLM.completion()\"]\n    end\n    \n    subgraph \"Reporting\"\n        Summary[\"cost_summary()\"]\n        Metrics[\"Metrics:<br/>- Input tokens<br/>- Output tokens<br/>- Total cost<br/>- Calls count\"]\n    end\n    \n    ClientCall -->|\"count tokens\"| TokenCounter\n    TokenCounter -->|\"lookup price\"| PriceDB\n    PriceDB -->|\"accumulate\"| CostAccum\n    \n    REPLExec -.->|\"nested calls\"| ClientCall\n    RLMComplete -->|\"primary calls\"| ClientCall\n    \n    CostAccum --> Summary\n    Summary --> Metrics\n```\n\n**Implementation Strategy**\n\n1. **Token Counting**: Use `tiktoken` library for OpenAI models:\n   ```\n   import tiktoken\n   encoding = tiktoken.encoding_for_model(model)\n   tokens = encoding.encode(text)\n   count = len(tokens)\n   ```\n\n2. **Cost Accumulation**: Track in RLM instance:\n   ```\n   self.total_input_tokens = 0\n   self.total_output_tokens = 0\n   self.api_call_count = 0\n   ```\n\n3. **Price Database**: Maintain model pricing:\n   ```\n   PRICING = {\n       \"gpt-5-nano\": {\"input\": 0.0001, \"output\": 0.0002},\n       \"gpt-5\": {\"input\": 0.001, \"output\": 0.002}\n   }\n   ```\n\n4. **Nested Call Tracking**: Recursively aggregate costs from sub-RLMs\n\n**Optimization Strategies**\n\n| Strategy | Implementation | Impact |\n|----------|----------------|--------|\n| Response caching | Hash (context + query) → cache result | Eliminate duplicate API calls |\n| Model routing | Use cheaper model for simple queries | Reduce cost per query |\n| Token budget limits | Track budget, fail early | Prevent runaway costs |\n| Prompt compression | Summarize context, remove redundancy | Reduce input tokens |\n| Batching | Group multiple queries | Amortize API overhead |\n\n**Sources:** [rlm/repl.py:48-49](), [rlm/repl.py:358-359](), [rlm/rlm.py:8-10]()\n\n---\n\n## Security Considerations\n\nThe REPL environment executes LLM-generated code with restricted built-ins but limited security guarantees. This section outlines risks and mitigation strategies.\n\n**Security Threat Model**\n\n```mermaid\ngraph TB\n    subgraph \"Attack Vectors\"\n        CodeInjection[\"Code Injection<br/>(via LLM output)\"]\n        FileAccess[\"Filesystem Access<br/>(read/write sensitive files)\"]\n        NetworkAccess[\"Network Access<br/>(exfiltrate data)\"]\n        ResourceExhaust[\"Resource Exhaustion<br/>(infinite loops, memory)\"]\n        NamespaceEscape[\"Namespace Escape<br/>(access blocked built-ins)\"]\n    end\n    \n    subgraph \"Current Mitigations\"\n        RestrictedBuiltins[\"Restricted __builtins__<br/>(no eval/exec/globals)\"]\n        TempDir[\"Isolated temp directory<br/>(separate per instance)\"]\n        NoTimeout[\"❌ No execution timeout\"]\n        OpenAllowed[\"open() allowed<br/>(within temp dir intent)\"]\n    end\n    \n    subgraph \"Additional Mitigations\"\n        Sandboxing[\"OS-level sandboxing<br/>(Docker, chroot)\"]\n        Timeout[\"Execution timeout<br/>(signal/threading)\"]\n        NetworkBlock[\"Network isolation<br/>(firewall rules)\"]\n        ResourceLimits[\"Resource limits<br/>(memory, CPU)\"]\n        Validation[\"Output validation<br/>(check before exec)\"]\n    end\n    \n    CodeInjection -.->|\"mitigated by\"| RestrictedBuiltins\n    FileAccess -.->|\"partially mitigated\"| TempDir\n    ResourceExhaust -.->|\"not mitigated\"| NoTimeout\n    \n    CodeInjection ==>|\"add\"| Validation\n    FileAccess ==>|\"add\"| Sandboxing\n    NetworkAccess ==>|\"add\"| NetworkBlock\n    ResourceExhaust ==>|\"add\"| Timeout\n    NamespaceEscape ==>|\"add\"| Sandboxing\n```\n\n**Current Security Posture**\n\nThe implementation provides **minimal security** suitable for experimentation:\n\n1. **Restricted Built-ins** [rlm/repl.py:90-161](): Blocks `eval`, `exec`, `compile`, `input`, `globals`, `locals`\n2. **Isolated Temp Directory** [rlm/repl.py:82-83](): Each REPL instance has separate filesystem\n3. **No Execution Timeout**: Code can run indefinitely\n4. **File I/O Allowed**: `open()` is available in `__builtins__`\n5. **Imports Allowed**: `__import__` is available, enabling arbitrary module loading\n\n**Risks in Production**\n\n| Risk | Severity | Current State | Mitigation Required |\n|------|----------|---------------|---------------------|\n| Arbitrary code execution | Critical | Partially mitigated | Add sandboxing |\n| Network access | High | Not mitigated | Block network in sandbox |\n| Filesystem access | High | Partially mitigated | Restrict to temp dir only |\n| Resource exhaustion | High | Not mitigated | Add timeouts and limits |\n| Import restrictions | Medium | Not mitigated | Whitelist safe modules |\n| Namespace escape | Medium | Partially mitigated | Strengthen built-ins filtering |\n\n**Production Hardening Checklist**\n\n1. **Container Isolation**: Run REPL in Docker with:\n   - No network access\n   - Read-only filesystem except temp dir\n   - Memory and CPU limits\n   - Non-root user\n\n2. **Execution Timeouts**: Implement using:\n   - `signal.alarm()` (Unix) or `threading.Timer()` (cross-platform)\n   - Kill execution after N seconds\n   - Return timeout error\n\n3. **Import Whitelist**: Replace `__import__` with filtered version:\n   ```\n   SAFE_MODULES = {'json', 're', 'math', 'datetime', 'collections'}\n   def safe_import(name, *args):\n       if name not in SAFE_MODULES:\n           raise ImportError(f\"Module {name} not allowed\")\n       return __import__(name, *args)\n   ```\n\n4. **Output Validation**: Parse and validate LLM output before execution:\n   - Check for suspicious patterns\n   - Limit code block size\n   - Validate syntax before exec\n\n5. **Resource Monitoring**: Track and limit:\n   - Memory usage per execution\n   - CPU time per execution\n   - File operations count\n   - Subprocess creation\n\n**Sources:** [rlm/repl.py:90-161](), [rlm/repl.py:82-83](), [rlm/repl.py:264-356]()\n\n---\n\n## State Management and Persistence\n\nThe RLM system maintains state across multiple iterations. Understanding state management is essential for debugging and extending functionality.\n\n**State Components**\n\n```mermaid\ngraph TB\n    subgraph \"RLM_REPL State\"\n        Messages[\"self.messages<br/>(conversation history)\"]\n        REPL[\"self.repl<br/>(REPLEnv instance)\"]\n        Client[\"self.client<br/>(OpenAIClient)\"]\n        Config[\"self.model<br/>self.recursive_model<br/>self.max_iterations\"]\n    end\n    \n    subgraph \"REPLEnv State\"\n        Globals[\"self.globals<br/>(restricted __builtins__)\"]\n        Locals[\"self.locals<br/>(execution variables)\"]\n        SubRLM[\"self.sub_rlm<br/>(recursive client)\"]\n        TempDir[\"self.temp_dir<br/>(filesystem isolation)\"]\n        Context[\"context variable<br/>(loaded from files)\"]\n    end\n    \n    subgraph \"Persistence Operations\"\n        Reset[\"reset()<br/>Clear state\"]\n        Serialize[\"Not implemented:<br/>Save/load state\"]\n    end\n    \n    REPL --> Globals\n    REPL --> Locals\n    REPL --> SubRLM\n    REPL --> TempDir\n    Locals --> Context\n    \n    Messages -.->|\"cleared by\"| Reset\n    Locals -.->|\"cleared by\"| Reset\n```\n\n**State Lifecycle**\n\n1. **Initialization**: \n   - `RLM_REPL.__init__()` creates empty `self.messages = []`\n   - `REPLEnv.__init__()` sets up globals, locals, temp directory\n\n2. **Context Setup**:\n   - `setup_context()` appends system prompt to messages\n   - Context files written to temp directory\n   - Context loaded into `self.locals['context']`\n\n3. **Iteration Loop**:\n   - Each iteration appends user prompt to messages\n   - LLM response appended to messages\n   - Code execution updates `self.locals`\n   - Execution results formatted and appended to messages\n\n4. **Reset**:\n   - `reset()` clears messages\n   - REPL environment typically recreated for new query\n\n**State Management Patterns**\n\n| Pattern | Use Case | Implementation |\n|---------|----------|----------------|\n| Message history | Multi-turn reasoning | Append to `self.messages` list |\n| Variable persistence | Cross-execution state | Store in `self.locals` dictionary |\n| Context isolation | Parallel queries | Create separate RLM_REPL instances |\n| Checkpoint/restore | Long-running tasks | Serialize messages + locals (custom) |\n\n**Limitations and Extensions**\n\n- **No Serialization**: State cannot be saved/loaded between sessions\n- **No Checkpointing**: Cannot resume from middle of iteration\n- **Memory Growth**: Message history grows unbounded\n- **Temp Directory Cleanup**: Handled by `__del__()` [rlm/repl.py:224-230]()\n\n**Extension Strategy**: Implement state serialization:\n1. Serialize `messages`, `locals`, current iteration\n2. Store to disk/database\n3. Restore on initialization\n4. Handle temp directory reconstruction\n\n**Sources:** [rlm/rlm_repl.py](), [rlm/repl.py:71-198](), [rlm/repl.py:224-230]()\n\n---\n\n## Performance Optimization\n\nSeveral strategies can improve RLM system performance for production use cases.\n\n**Optimization Strategies**\n\n```mermaid\ngraph TB\n    subgraph \"Latency Optimization\"\n        StreamResp[\"Stream responses<br/>(OpenAI streaming)\"]\n        AsyncExec[\"Async execution<br/>(parallel sub-calls)\"]\n        CacheResults[\"Cache results<br/>(deduplication)\"]\n    end\n    \n    subgraph \"Cost Optimization\"\n        ModelTier[\"Model tiering<br/>(cheap → expensive)\"]\n        PromptComp[\"Prompt compression<br/>(summarization)\"]\n        EarlyTerm[\"Early termination<br/>(confidence threshold)\"]\n    end\n    \n    subgraph \"Throughput Optimization\"\n        Batching[\"Batch queries<br/>(parallel RLM instances)\"]\n        PoolREPL[\"REPL pooling<br/>(reuse environments)\"]\n        PreloadContext[\"Preload context<br/>(avoid repeated loads)\"]\n    end\n    \n    subgraph \"Current Limitations\"\n        SyncOnly[\"Synchronous only\"]\n        NoCache[\"No caching\"]\n        NoStream[\"No streaming\"]\n        RecreateREPL[\"Recreate REPL<br/>per query\"]\n    end\n    \n    SyncOnly -.->|\"replace with\"| AsyncExec\n    NoCache -.->|\"add\"| CacheResults\n    NoStream -.->|\"add\"| StreamResp\n    RecreateREPL -.->|\"add\"| PoolREPL\n```\n\n**Performance Bottlenecks**\n\n| Bottleneck | Impact | Measurement | Solution |\n|------------|--------|-------------|----------|\n| LLM API latency | 1-10s per call | Time `client.completion()` | Use streaming, async |\n| Code execution | Variable | Time `code_execution()` | Add timeouts, optimize |\n| Message history | O(n) context size | Track `len(messages)` | Summarize or prune old messages |\n| Temp file I/O | ~100ms | Time `load_context()` | Cache context in memory |\n| Iteration overhead | O(iterations) | Count loop cycles | Reduce max_iterations, early stop |\n\n**Implementation Approaches**\n\n1. **Response Streaming**: Modify `OpenAIClient` to support streaming\n   - Use `stream=True` in OpenAI API\n   - Process tokens as they arrive\n   - Extract code blocks incrementally\n\n2. **Async Sub-Calls**: Enable parallel recursive queries\n   - Use `asyncio` for concurrent `llm_query()` calls\n   - Modify `Sub_RLM.completion()` to be async\n   - Aggregate results\n\n3. **Result Caching**: Hash-based deduplication\n   - Key: `hash((context, query, messages))`\n   - Value: `final_answer`\n   - Storage: in-memory dict or Redis\n\n4. **REPL Pooling**: Reuse REPLEnv instances\n   - Maintain pool of pre-initialized REPLs\n   - Reset locals between queries\n   - Reuse temp directory\n\n**Sources:** [rlm/utils/llm.py](), [rlm/rlm_repl.py](), [rlm/repl.py:264-356]()\n\n---\n\n## Testing and Validation\n\nThe minimal implementation lacks comprehensive testing infrastructure. This section outlines testing strategies for extension development.\n\n**Testing Architecture**\n\n```mermaid\ngraph TB\n    subgraph \"Unit Tests\"\n        TestRLM[\"Test RLM interface<br/>implementations\"]\n        TestREPL[\"Test REPL execution<br/>sandboxing\"]\n        TestUtils[\"Test utilities<br/>(parsing, formatting)\"]\n        TestPrompts[\"Test prompt<br/>generation\"]\n    end\n    \n    subgraph \"Integration Tests\"\n        TestComplete[\"Test completion()<br/>end-to-end\"]\n        TestRecursion[\"Test recursive<br/>sub-calls\"]\n        TestContext[\"Test context<br/>loading\"]\n    end\n    \n    subgraph \"System Tests\"\n        TestNIAH[\"Test needle-in-haystack\"]\n        TestReasoning[\"Test multi-step<br/>reasoning\"]\n        TestErrors[\"Test error handling\"]\n    end\n    \n    subgraph \"Validation\"\n        MockLLM[\"Mock LLM<br/>(deterministic responses)\"]\n        FixtureData[\"Fixture data<br/>(test contexts)\"]\n        Assertions[\"Assertions<br/>(expected outputs)\"]\n    end\n    \n    TestRLM --> MockLLM\n    TestREPL --> FixtureData\n    TestComplete --> MockLLM\n    TestComplete --> Assertions\n```\n\n**Test Categories**\n\n| Category | Focus | Key Tests |\n|----------|-------|-----------|\n| Unit | Individual components | RLM methods, REPL execution, parsing |\n| Integration | Component interaction | Context setup, iteration loop, sub-calls |\n| System | End-to-end behavior | Full query processing, error recovery |\n| Performance | Scalability | Large context, deep recursion, parallel queries |\n| Security | Sandbox effectiveness | Escape attempts, resource limits |\n\n**Mock LLM Strategy**\n\nReplace `OpenAIClient` with mock for deterministic testing:\n\n```python\nclass MockLLMClient:\n    def __init__(self, responses: list[str]):\n        self.responses = responses\n        self.call_count = 0\n    \n    def completion(self, messages, **kwargs):\n        response = self.responses[self.call_count]\n        self.call_count += 1\n        return response\n```\n\n**Test Fixtures**\n\nCreate reusable test contexts:\n- Small contexts (< 1KB) for fast iteration\n- Large contexts (> 1MB) for performance testing\n- Edge cases (empty, malformed, special characters)\n\n**Validation Metrics**\n\n- **Correctness**: Final answer matches expected\n- **Efficiency**: Iterations used vs. optimal\n- **Robustness**: Error recovery, edge case handling\n- **Cost**: Tokens consumed, API calls made\n\n**Sources:** [main.py](), [rlm/rlm_repl.py](), [rlm/repl.py]()\n\n---\n\n## Deployment Considerations\n\nMoving from experimentation to production requires addressing operational concerns.\n\n**Deployment Architecture**\n\n```mermaid\ngraph TB\n    subgraph \"Development\"\n        Local[\"Local execution<br/>(main.py)\"]\n        SimpleConfig[\".env file<br/>(API keys)\"]\n    end\n    \n    subgraph \"Production\"\n        API[\"REST API<br/>(FastAPI/Flask)\"]\n        Queue[\"Task queue<br/>(Celery/RQ)\"]\n        Cache[\"Cache layer<br/>(Redis)\"]\n        Monitor[\"Monitoring<br/>(metrics, logs)\"]\n    end\n    \n    subgraph \"Infrastructure\"\n        Container[\"Containerization<br/>(Docker)\"]\n        Orchestrate[\"Orchestration<br/>(Kubernetes)\"]\n        SecretMgmt[\"Secret management<br/>(Vault, AWS Secrets)\"]\n    end\n    \n    subgraph \"Scaling\"\n        LoadBalance[\"Load balancing\"]\n        HorizScale[\"Horizontal scaling\"]\n        RateLimit[\"Rate limiting\"]\n    end\n    \n    Local -.->|\"productionize\"| API\n    SimpleConfig -.->|\"replace with\"| SecretMgmt\n    API --> Queue\n    Queue --> Cache\n    API --> Monitor\n    \n    API --> Container\n    Container --> Orchestrate\n    Queue --> Container\n```\n\n**Operational Requirements**\n\n| Requirement | Development | Production | Implementation |\n|------------|-------------|------------|----------------|\n| Configuration | `.env` file | Secret manager | AWS Secrets Manager, Vault |\n| Logging | Console output | Centralized logs | ELK stack, CloudWatch |\n| Monitoring | None | Metrics + alerts | Prometheus, Grafana, PagerDuty |\n| Scaling | Single instance | Auto-scaling | Kubernetes HPA, AWS ECS |\n| Reliability | Best effort | High availability | Multi-AZ, retries, circuit breakers |\n| Security | Minimal | Production-grade | WAF, sandboxing, secrets rotation |\n\n**Containerization Example**\n\nDockerfile for RLM service:\n```dockerfile\nFROM python:3.11-slim\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\nCOPY rlm/ ./rlm/\n# Restrict network access, set resource limits\nUSER nonroot\nCMD [\"python\", \"-m\", \"rlm.api\"]\n```\n\n**API Wrapper Pattern**\n\nExpose RLM via REST API:\n- POST `/query`: Accept context + query, return answer\n- GET `/status/{query_id}`: Check query status (async)\n- POST `/reset`: Reset RLM state\n- GET `/health`: Health check endpoint\n\n**Sources:** [main.py](), [README.md:14-28](), [rlm/rlm_repl.py]()\n\n---\n\n## Debugging Strategies\n\nEffective debugging requires understanding the RLM's execution flow and available introspection tools.\n\n**Debugging Tools**\n\n```mermaid\ngraph LR\n    subgraph \"Built-in Logging\"\n        RootLogger[\"ColorfulLogger<br/>(root_logger.py)\"]\n        REPLLogger[\"REPLEnvLogger<br/>(repl_logger.py)\"]\n    end\n    \n    subgraph \"Introspection Points\"\n        Messages[\"self.messages<br/>(conversation history)\"]\n        Locals[\"self.locals<br/>(variable state)\"]\n        StdoutErr[\"stdout/stderr<br/>(execution output)\"]\n        Iterations[\"Iteration counter\"]\n    end\n    \n    subgraph \"Debugging Techniques\"\n        PrintDebug[\"Print debugging<br/>(in REPL)\"]\n        Breakpoints[\"Breakpoints<br/>(pdb in RLM)\"]\n        Trace[\"Execution tracing\"]\n        Replay[\"Replay from logs\"]\n    end\n    \n    RootLogger --> Messages\n    REPLLogger --> Locals\n    REPLLogger --> StdoutErr\n    \n    Messages --> Replay\n    Locals --> PrintDebug\n    Iterations --> Trace\n```\n\n**Common Issues and Solutions**\n\n| Issue | Symptoms | Debugging Approach | Solution |\n|-------|----------|-------------------|----------|\n| No final answer | Max iterations reached | Check `self.messages` for LLM responses | Improve prompts, increase iterations |\n| Wrong answer | Incorrect result | Trace `self.locals` evolution | Fix logic in generated code |\n| Execution errors | stderr not empty | Review code blocks in messages | Validate LLM output before exec |\n| Infinite recursion | Timeout or crash | Count `llm_query()` calls | Add depth tracking, termination logic |\n| Context not loaded | `NameError: context` | Check temp directory contents | Verify `load_context()` executed |\n\n**Message History Inspection**\n\nThe `self.messages` list [rlm/rlm_repl.py]() contains complete conversation:\n- System prompt (role: \"system\")\n- User prompts (role: \"user\")  \n- LLM responses (role: \"assistant\")\n- Execution results (role: \"user\", formatted)\n\n**Execution Tracing**\n\nAdd instrumentation to track execution:\n```python\n# In RLM_REPL.completion()\nfor iteration in range(self.max_iterations):\n    print(f\"[Iteration {iteration}]\")\n    print(f\"Messages count: {len(self.messages)}\")\n    print(f\"Locals variables: {list(self.locals.keys())}\")\n```\n\n**REPL State Inspection**\n\nAccess REPL state for debugging:\n```python\n# After RLM_REPL initialization\nrlm = RLM_REPL(...)\nresult = rlm.completion(context, query)\n\n# Inspect final state\nprint(rlm.repl.locals)  # All variables\nprint(rlm.messages)      # Full conversation\n```\n\n**Sources:** [rlm/logger/root_logger.py](), [rlm/logger/repl_logger.py](), [rlm/rlm_repl.py]()\n\n---\n\n## Conclusion\n\nThe RLM minimal implementation provides a flexible foundation for experimentation with recursive language models. Key extension points include:\n\n1. **Recursion Depth**: Replace `Sub_RLM` with `RLM_REPL` for arbitrary depth (see [5.1](#5.1))\n2. **Custom Implementations**: Extend `RLM` abstract class for new behaviors (see [5.2](#5.2))\n3. **REPL Customization**: Modify built-ins, execution logic, and context loading\n4. **Prompt Engineering**: Develop task-specific system and user prompts\n5. **Provider Integration**: Replace OpenAI with alternative LLM providers\n6. **Production Hardening**: Add security, monitoring, and operational features\n\nFor implementation details on specific extensions, refer to the subsections [5.1](#5.1) and [5.2](#5.2). For component-level documentation, see [Component Reference](#4).\n\n**Sources:** [README.md:1-32](), [rlm/rlm.py:1-15](), [rlm/repl.py:1-360](), [rlm/rlm_repl.py]()\n\n---\n\n# Page: Multi-Backend Configuration\n\n# Multi-Backend Configuration\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [rlm/clients/__init__.py](rlm/clients/__init__.py)\n- [rlm/clients/azure_openai.py](rlm/clients/azure_openai.py)\n- [rlm/core/comms_utils.py](rlm/core/comms_utils.py)\n- [rlm/core/lm_handler.py](rlm/core/lm_handler.py)\n- [rlm/core/rlm.py](rlm/core/rlm.py)\n- [rlm/core/types.py](rlm/core/types.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis document explains how to configure multiple LLM backends within a single RLM instance, enabling different models to handle different types of requests. The multi-backend system allows the main RLM iterations to use one model while code executing in REPL environments can make sub-calls to different models.\n\nFor information about the available LLM backends and their individual configurations, see [LLM Client Backends](#4). For details on how environments make LLM calls, see [Recursion and Sub-Calls](#3.4).\n\n**Sources:** [rlm/core/rlm.py:1-285](), [rlm/core/lm_handler.py:1-186]()\n\n---\n\n## Architecture Overview\n\nThe multi-backend configuration system is centered around the `LMHandler` class, which maintains a registry of multiple `BaseLM` client instances and routes requests to the appropriate client based on the requested model name.\n\n### LMHandler Client Registry\n\nThe `LMHandler` manages clients through three key components:\n\n| Component | Type | Purpose |\n|-----------|------|---------|\n| `default_client` | `BaseLM` | The primary client used for main RLM iterations |\n| `clients` | `dict[str, BaseLM]` | Registry mapping model names to client instances |\n| `register_client(model_name, client)` | Method | Adds a client to the registry |\n| `get_client(model)` | Method | Retrieves a client by model name or returns default |\n\nWhen the `LMHandler` receives a request, it examines the `model` field in the `LMRequest` and routes the call to the appropriate client. If no matching client is found, it falls back to the `default_client`.\n\n```mermaid\ngraph TB\n    subgraph RLM[\"RLM Instance\"]\n        RLMConfig[\"RLM Configuration<br/>backend + other_backends\"]\n    end\n    \n    subgraph Context[\"Completion Context (per call)\"]\n        DefaultClient[\"Default Client<br/>(primary backend)\"]\n        LMHandler[\"LMHandler\"]\n        ClientRegistry[\"Client Registry<br/>dict[str, BaseLM]\"]\n        \n        OtherClient1[\"Other Client 1<br/>(e.g., gpt-4o-mini)\"]\n        OtherClient2[\"Other Client 2<br/>(e.g., claude-3-5-sonnet)\"]\n    end\n    \n    subgraph Environment[\"REPL Environment\"]\n        CodeExec[\"Code Execution\"]\n        LLMQuery[\"llm_query(prompt, model=...)\"]\n    end\n    \n    subgraph RequestFlow[\"Request Routing\"]\n        LMRequest[\"LMRequest<br/>model: str | None\"]\n        GetClient[\"get_client(model)\"]\n        Decision{\"Model in<br/>registry?\"}\n    end\n    \n    RLMConfig -->|\"spawns\"| DefaultClient\n    RLMConfig -->|\"spawns\"| OtherClient1\n    RLMConfig -->|\"spawns\"| OtherClient2\n    \n    LMHandler -->|\"default_client\"| DefaultClient\n    LMHandler -->|\"manages\"| ClientRegistry\n    \n    ClientRegistry -->|\"registers\"| DefaultClient\n    ClientRegistry -->|\"registers\"| OtherClient1\n    ClientRegistry -->|\"registers\"| OtherClient2\n    \n    CodeExec -->|\"calls\"| LLMQuery\n    LLMQuery -->|\"creates\"| LMRequest\n    LMRequest -->|\"routes via\"| GetClient\n    GetClient --> Decision\n    \n    Decision -->|\"yes\"| ClientRegistry\n    Decision -->|\"no\"| DefaultClient\n```\n\n**Sources:** [rlm/core/lm_handler.py:102-133](), [rlm/core/rlm.py:101-134]()\n\n---\n\n## Configuration\n\nMulti-backend configuration is specified when initializing the `RLM` instance through two parameters:\n\n### Configuration Parameters\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| `backend` | `ClientBackend` | Primary backend for main RLM iterations |\n| `backend_kwargs` | `dict[str, Any]` | Configuration for the primary backend |\n| `other_backends` | `list[ClientBackend]` | Additional backends for sub-calls |\n| `other_backend_kwargs` | `list[dict[str, Any]]` | Configurations for each additional backend (ordered) |\n\nThe `other_backends` and `other_backend_kwargs` lists must be parallel arrays with matching lengths. Each backend in `other_backends` corresponds to the kwargs at the same index in `other_backend_kwargs`.\n\n### Example Configuration\n\n```python\nfrom rlm import RLM\n\nrlm = RLM(\n    # Primary backend for main iterations\n    backend=\"openai\",\n    backend_kwargs={\n        \"model_name\": \"gpt-4o\",\n        \"api_key\": \"sk-...\",\n    },\n    \n    # Additional backends for sub-calls\n    other_backends=[\"openai\", \"anthropic\"],\n    other_backend_kwargs=[\n        {\n            \"model_name\": \"gpt-4o-mini\",  # Faster, cheaper model\n            \"api_key\": \"sk-...\",\n        },\n        {\n            \"model_name\": \"claude-3-5-sonnet-20241022\",\n            \"api_key\": \"sk-ant-...\",\n        },\n    ],\n    \n    # Other RLM configuration\n    environment=\"local\",\n    max_iterations=30,\n)\n```\n\n**Sources:** [rlm/core/rlm.py:40-99]()\n\n---\n\n## Client Registration Flow\n\nWhen an RLM completion is initiated, the system spawns a dedicated `LMHandler` and registers all configured clients in the following sequence:\n\n```mermaid\nsequenceDiagram\n    participant RLM as RLM Instance\n    participant Context as _spawn_completion_context\n    participant Factory as get_client\n    participant Handler as LMHandler\n    participant Registry as clients dict\n    \n    RLM->>Context: completion() called\n    activate Context\n    \n    Context->>Factory: get_client(backend, backend_kwargs)\n    Factory-->>Context: primary_client\n    \n    Context->>Handler: LMHandler(primary_client)\n    activate Handler\n    Handler->>Registry: register_client(model_name, primary_client)\n    \n    loop For each other_backend\n        Context->>Factory: get_client(backend, kwargs)\n        Factory-->>Context: other_client\n        Context->>Handler: register_client(other_client.model_name, other_client)\n        Handler->>Registry: store client by model_name\n    end\n    \n    Context->>Handler: start() - spawn server\n    Handler-->>Context: (host, port)\n    \n    Context->>Context: Pass handler address to environment\n    \n    deactivate Handler\n    deactivate Context\n```\n\nThe key steps are:\n\n1. **Primary Client Creation** - The main backend client is created from `backend` and `backend_kwargs` [rlm/core/rlm.py:108]()\n2. **Handler Initialization** - The handler is initialized with the primary client as its `default_client` [rlm/core/lm_handler.py:110-123]()\n3. **Other Clients Registration** - Each additional backend is instantiated and registered by its `model_name` [rlm/core/rlm.py:112-115]()\n4. **Server Start** - The handler's socket server is started to accept requests [rlm/core/rlm.py:117]()\n\n**Sources:** [rlm/core/rlm.py:101-134](), [rlm/core/lm_handler.py:110-127]()\n\n---\n\n## Model Routing Mechanism\n\nThe `LMHandler` routes requests to clients using a simple lookup mechanism implemented in the `get_client` method:\n\n```python\ndef get_client(self, model: str | None = None) -> BaseLM:\n    \"\"\"Get client by model name, or return default.\"\"\"\n    if model and model in self.clients:\n        return self.clients[model]\n    return self.default_client\n```\n\n### Routing Decision Tree\n\n```mermaid\ngraph TB\n    Request[\"LMRequest received\"]\n    CheckModel{\"model field<br/>provided?\"}\n    CheckRegistry{\"model in<br/>clients dict?\"}\n    UseRegistered[\"Return registered client<br/>clients[model]\"]\n    UseDefault[\"Return default_client\"]\n    \n    Request --> CheckModel\n    CheckModel -->|\"yes\"| CheckRegistry\n    CheckModel -->|\"no\"| UseDefault\n    CheckRegistry -->|\"yes\"| UseRegistered\n    CheckRegistry -->|\"no\"| UseDefault\n    \n    UseRegistered --> Execute[\"Execute completion<br/>on matched client\"]\n    UseDefault --> Execute\n```\n\n### Request Handling\n\nThe routing happens in two places:\n\n1. **Direct Calls** - When the RLM main process calls `lm_handler.completion()`, it can optionally specify a model [rlm/core/lm_handler.py:167-169]()\n\n2. **Socket Requests** - When REPL environments make `llm_query()` calls, they send an `LMRequest` with an optional `model` field [rlm/core/lm_handler.py:46-63]()\n\nThe `LMRequestHandler` extracts the model from the request and passes it to `get_client()`:\n\n```python\ndef _handle_single(self, request: LMRequest, handler: \"LMHandler\") -> LMResponse:\n    \"\"\"Handle a single prompt request.\"\"\"\n    client = handler.get_client(request.model)  # Route by model name\n    \n    start_time = time.perf_counter()\n    content = client.completion(request.prompt)\n    end_time = time.perf_counter()\n    \n    # Return response with usage tracking\n    # ...\n```\n\n**Sources:** [rlm/core/lm_handler.py:129-133](), [rlm/core/lm_handler.py:46-63]()\n\n---\n\n## Sub-Call Model Selection\n\nCode executing in REPL environments specifies which model to use through the `model` parameter of `llm_query()` or `llm_query_batched()` functions:\n\n### Environment-Side API\n\nWhen code calls `llm_query()`, it can specify a model name that matches one of the registered clients:\n\n```python\n# Inside code executed by the REPL environment\nresult = llm_query(\"What is 2+2?\", model=\"gpt-4o-mini\")\n```\n\nIf no model is specified, the request is routed to the default client:\n\n```python\n# Uses default_client\nresult = llm_query(\"What is 2+2?\")\n```\n\n### Model Name Matching\n\nThe model name specified in the sub-call must exactly match the `model_name` configured in one of the backend kwargs. For example:\n\n```python\nrlm = RLM(\n    backend=\"openai\",\n    backend_kwargs={\n        \"model_name\": \"gpt-4o\",  # Primary model\n    },\n    other_backends=[\"openai\"],\n    other_backend_kwargs=[\n        {\n            \"model_name\": \"gpt-4o-mini\",  # Must use exact name in llm_query\n        },\n    ],\n)\n```\n\nInside the environment, code would call:\n\n```python\n# Matches \"gpt-4o-mini\" from other_backends\nfast_result = llm_query(\"Quick question\", model=\"gpt-4o-mini\")\n\n# Matches \"gpt-4o\" from primary backend\ndetailed_result = llm_query(\"Complex question\", model=\"gpt-4o\")\n```\n\n**Sources:** [rlm/core/lm_handler.py:46-63]()\n\n---\n\n## Usage Patterns\n\n### Pattern 1: Cost Optimization\n\nUse a powerful model for main reasoning and a cheaper model for auxiliary tasks:\n\n```python\nrlm = RLM(\n    backend=\"openai\",\n    backend_kwargs={\"model_name\": \"gpt-4o\"},\n    other_backends=[\"openai\"],\n    other_backend_kwargs=[\n        {\"model_name\": \"gpt-4o-mini\"},  # 15x cheaper than gpt-4o\n    ],\n)\n```\n\nWithin the environment, use the cheaper model for data processing, validation, or simple queries:\n\n```python\n# Main RLM uses gpt-4o for complex reasoning\n# But code can use gpt-4o-mini for simple tasks\nfor item in data:\n    summary = llm_query(f\"Summarize: {item}\", model=\"gpt-4o-mini\")\n```\n\n### Pattern 2: Provider Diversity\n\nMix different providers for their specific strengths:\n\n```python\nrlm = RLM(\n    backend=\"openai\",\n    backend_kwargs={\"model_name\": \"gpt-4o\"},  # Good for reasoning\n    other_backends=[\"anthropic\", \"openai\"],\n    other_backend_kwargs=[\n        {\"model_name\": \"claude-3-5-sonnet-20241022\"},  # Good for code\n        {\"model_name\": \"gpt-4o-mini\"},  # Fast for simple tasks\n    ],\n)\n```\n\nCode can route different tasks to different providers:\n\n```python\n# Use Claude for code generation\ncode = llm_query(\"Write a parser for JSON\", model=\"claude-3-5-sonnet-20241022\")\n\n# Use gpt-4o-mini for data validation\nis_valid = llm_query(f\"Is this valid? {data}\", model=\"gpt-4o-mini\")\n```\n\n### Pattern 3: Batch Processing with Multiple Models\n\nUse different models for different types of batch operations:\n\n```python\nrlm = RLM(\n    backend=\"openai\",\n    backend_kwargs={\"model_name\": \"gpt-4o\"},\n    other_backends=[\"openai\"],\n    other_backend_kwargs=[\n        {\"model_name\": \"gpt-4o-mini\"},\n    ],\n)\n```\n\nLeverage batched queries with model selection:\n\n```python\n# Batch process with cheaper model\nprompts = [f\"Categorize: {item}\" for item in items]\nresults = llm_query_batched(prompts, model=\"gpt-4o-mini\")\n```\n\n**Sources:** [rlm/core/rlm.py:40-99](), [rlm/core/lm_handler.py:65-92]()\n\n---\n\n## Usage Tracking\n\nThe `LMHandler` aggregates usage statistics across all registered clients through its `get_usage_summary()` method:\n\n```python\ndef get_usage_summary(self) -> UsageSummary:\n    \"\"\"Get the usage summary for all clients, merged into a single dict.\"\"\"\n    merged = {}\n    for client in self.clients.values():\n        client_summary = client.get_usage_summary()\n        merged.update(client_summary.model_usage_summaries)\n    return UsageSummary(model_usage_summaries=merged)\n```\n\n### Merged Usage Summary Structure\n\nThe returned `UsageSummary` contains a dictionary mapping each model name to its usage statistics:\n\n```python\n{\n    \"gpt-4o\": {\n        \"prompt_tokens\": 15000,\n        \"completion_tokens\": 3000,\n        \"total_tokens\": 18000,\n    },\n    \"gpt-4o-mini\": {\n        \"prompt_tokens\": 50000,\n        \"completion_tokens\": 10000,\n        \"total_tokens\": 60000,\n    },\n    \"claude-3-5-sonnet-20241022\": {\n        \"prompt_tokens\": 8000,\n        \"completion_tokens\": 2000,\n        \"total_tokens\": 10000,\n    },\n}\n```\n\n### Per-Call Tracking\n\nEach individual LLM call (whether from the main RLM or from sub-calls) tracks its usage in the `RLMChatCompletion` object, which includes:\n\n- `root_model`: The model that handled the request\n- `usage_summary`: Token counts for that specific call\n- `execution_time`: Duration of the call\n\nThis enables fine-grained tracking of which models were used where, and their associated costs.\n\n**Sources:** [rlm/core/lm_handler.py:179-185](), [rlm/core/lm_handler.py:54-63]()\n\n---\n\n## Configuration Best Practices\n\n### Model Name Consistency\n\nEnsure model names in `backend_kwargs` are unique and descriptive:\n\n```python\n# GOOD: Unique, clear model names\nother_backend_kwargs=[\n    {\"model_name\": \"gpt-4o-mini\"},\n    {\"model_name\": \"claude-3-5-sonnet-20241022\"},\n]\n\n# BAD: Both would register as the same model name if they're the same\nother_backend_kwargs=[\n    {\"model_name\": \"gpt-4o-mini\", \"temperature\": 0.0},\n    {\"model_name\": \"gpt-4o-mini\", \"temperature\": 0.9},  # Overwrites previous!\n]\n```\n\n### Registration Order\n\nClients are registered in the order they appear. If two clients have the same `model_name`, the later one overwrites the earlier one:\n\n```python\n# The second registration overwrites the first\nlm_handler.register_client(\"gpt-4o\", client1)  # Registered\nlm_handler.register_client(\"gpt-4o\", client2)  # Overwrites client1\n```\n\n### Default Client Fallback\n\nAlways configure your `backend` and `backend_kwargs` carefully, as this client serves as the fallback for:\n- Main RLM iterations\n- Any `llm_query()` calls without a `model` parameter\n- Any `llm_query()` calls with an unrecognized `model` parameter\n\n### Environment Considerations\n\nAll registered clients are available to code executing in any REPL environment type (local, docker, modal, prime). The model routing is handled entirely by the `LMHandler`, which is accessible via socket from all environment types.\n\n**Sources:** [rlm/core/lm_handler.py:125-133](), [rlm/core/rlm.py:112-115]()\n\n---\n\n# Page: Persistent Environments and Multi-Turn Sessions\n\n# Persistent Environments and Multi-Turn Sessions\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [rlm/core/rlm.py](rlm/core/rlm.py)\n- [rlm/environments/base_env.py](rlm/environments/base_env.py)\n- [rlm/environments/local_repl.py](rlm/environments/local_repl.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis document covers the **persistent mode** feature of RLM environments, which enables multi-turn conversations by reusing a single environment instance across multiple `completion()` calls. This allows the LLM to build upon previous execution state, access prior contexts, and reference conversation histories.\n\nFor general information about environment types and execution models, see [Execution Environments](#5). For implementing custom environments, see [Custom Environments](#8.3). For the basic RLM orchestration flow, see [RLM Class](#3.1).\n\n---\n\n## Overview of Persistent Mode\n\nBy default, each call to `RLM.completion()` creates a fresh environment that is cleaned up after the call completes. When `persistent=True` is set, the RLM instance maintains a single environment across all completion calls, enabling:\n\n- **State continuity**: Variables, imported modules, and execution state persist between turns\n- **Multi-context access**: Each completion can load a new context while retaining access to previous ones\n- **History tracking**: Prior conversation message histories are stored and accessible\n- **Handler address updates**: The LM handler address is updated between calls without recreating the environment\n\n```mermaid\ngraph TB\n    subgraph \"Non-Persistent Mode (Default)\"\n        Call1[\"completion() call 1\"]\n        Env1[\"Create Environment\"]\n        Context1[\"Load context\"]\n        Exec1[\"Execute code\"]\n        Cleanup1[\"Cleanup environment\"]\n        \n        Call2[\"completion() call 2\"]\n        Env2[\"Create NEW Environment\"]\n        Context2[\"Load context\"]\n        Exec2[\"Execute code\"]\n        Cleanup2[\"Cleanup environment\"]\n        \n        Call1 --> Env1 --> Context1 --> Exec1 --> Cleanup1\n        Call2 --> Env2 --> Context2 --> Exec2 --> Cleanup2\n    end\n    \n    subgraph \"Persistent Mode (persistent=True)\"\n        CallA[\"completion() call 1\"]\n        EnvA[\"Create Environment\"]\n        ContextA[\"add_context(payload)\"]\n        ExecA[\"Execute code\"]\n        \n        CallB[\"completion() call 2\"]\n        UpdateAddr[\"update_handler_address()\"]\n        ContextB[\"add_context(payload)\"]\n        ExecB[\"Execute code<br/>Can access context_0, context_1\"]\n        \n        CallC[\"close()\"]\n        CleanupC[\"Cleanup environment\"]\n        \n        CallA --> EnvA --> ContextA --> ExecA\n        CallB --> UpdateAddr --> ContextB --> ExecB\n        CallC --> CleanupC\n        \n        EnvA -.reused.-> UpdateAddr\n    end\n```\n\n**Sources**: [rlm/core/rlm.py:125-179](), [rlm/core/rlm.py:54]()\n\n---\n\n## The SupportsPersistence Protocol\n\nThe `SupportsPersistence` protocol defines the interface that environments must implement to support persistent mode. It is a runtime-checkable protocol located at [rlm/environments/base_env.py:76-183]().\n\n### Protocol Methods\n\n| Method | Purpose | Return Type |\n|--------|---------|-------------|\n| `update_handler_address(address)` | Update LM handler address for new completion | `None` |\n| `add_context(payload, index?)` | Add versioned context (context_N) | `int` |\n| `get_context_count()` | Return number of contexts loaded | `int` |\n| `add_history(messages, index?)` | Add versioned history (history_N) | `int` |\n| `get_history_count()` | Return number of histories stored | `int` |\n\n### Checking Support\n\nThe RLM class validates persistence support at initialization:\n\n```python\n# From RLM.__init__\nif self.persistent:\n    self._validate_persistent_environment_support()\n```\n\nThe validation checks against a whitelist of known-supporting environments:\n\n```python\n# From rlm/core/rlm.py:357-381\npersistent_supported_environments = {\"local\"}\n\nif self.environment_type not in persistent_supported_environments:\n    raise ValueError(\n        f\"persistent=True is not supported for environment type '{self.environment_type}'. \"\n        f\"Persistent mode requires environments that implement update_handler_address(), \"\n        f\"add_context(), and get_context_count(). \"\n        f\"Supported environments: {sorted(persistent_supported_environments)}\"\n    )\n```\n\nRuntime checking uses `isinstance(env, SupportsPersistence)`:\n\n```python\n# From rlm/core/rlm.py:383-385\n@staticmethod\ndef _env_supports_persistence(env: BaseEnv) -> bool:\n    return isinstance(env, SupportsPersistence)\n```\n\n**Sources**: [rlm/environments/base_env.py:76-183](), [rlm/core/rlm.py:357-385]()\n\n---\n\n## Context Versioning\n\nContexts are versioned with numeric suffixes to enable multi-turn access. Each call to `add_context()` creates a new versioned variable in the execution namespace.\n\n### Versioning Scheme\n\n```mermaid\ngraph LR\n    subgraph \"First completion() call\"\n        C0[\"add_context(data1)<br/>→ context_0\"]\n        Alias0[\"Alias: context → context_0\"]\n    end\n    \n    subgraph \"Second completion() call\"\n        C1[\"add_context(data2)<br/>→ context_1\"]\n        Access[\"Code can access:<br/>context_0, context_1, context\"]\n    end\n    \n    subgraph \"Third completion() call\"\n        C2[\"add_context(data3)<br/>→ context_2\"]\n        AccessAll[\"Code can access:<br/>context_0, context_1, context_2, context\"]\n    end\n    \n    C0 --> Alias0\n    Alias0 -.persistent env.-> C1\n    C1 --> Access\n    Access -.persistent env.-> C2\n    C2 --> AccessAll\n```\n\n### Implementation in LocalREPL\n\nThe `add_context()` method in `LocalREPL` [rlm/environments/local_repl.py:234-271]():\n\n1. Determines the context index (auto-increment if `None`)\n2. Creates a versioned variable name: `context_{index}`\n3. Writes the payload to a temp file\n4. Executes code to load it into the namespace\n5. Aliases `context_0` as `context` for convenience\n6. Updates `_context_count`\n\n```python\n# From rlm/environments/local_repl.py:234-271\ndef add_context(self, context_payload: dict | list | str, context_index: int | None = None) -> int:\n    if context_index is None:\n        context_index = self._context_count\n    \n    var_name = f\"context_{context_index}\"\n    \n    if isinstance(context_payload, str):\n        context_path = os.path.join(self.temp_dir, f\"context_{context_index}.txt\")\n        with open(context_path, \"w\") as f:\n            f.write(context_payload)\n        self.execute_code(f\"with open(r'{context_path}', 'r') as f:\\n    {var_name} = f.read()\")\n    else:\n        context_path = os.path.join(self.temp_dir, f\"context_{context_index}.json\")\n        with open(context_path, \"w\") as f:\n            json.dump(context_payload, f)\n        self.execute_code(\n            f\"import json\\nwith open(r'{context_path}', 'r') as f:\\n    {var_name} = json.load(f)\"\n        )\n    \n    # Alias context_0 as 'context'\n    if context_index == 0:\n        self.execute_code(f\"context = {var_name}\")\n    \n    self._context_count = max(self._context_count, context_index + 1)\n    return context_index\n```\n\n### Context Access in Code\n\nThe LLM-generated code can access contexts by their versioned names:\n\n```python\n# Access first context (both ways work)\nprint(context)      # Alias to context_0\nprint(context_0)    # Explicit version\n\n# Access second context\nprint(context_1)\n\n# Loop through all contexts\nfor i in range(get_context_count()):\n    print(f\"context_{i}\")\n```\n\nThe RLM informs the model about available contexts in the prompt [rlm/core/rlm.py:219-231]():\n\n```python\ncontext_count = (\n    environment.get_context_count()\n    if isinstance(environment, SupportsPersistence)\n    else 1\n)\nhistory_count = (\n    environment.get_history_count()\n    if isinstance(environment, SupportsPersistence)\n    else 0\n)\ncurrent_prompt = message_history + [\n    build_user_prompt(root_prompt, i, context_count, history_count)\n]\n```\n\n**Sources**: [rlm/environments/local_repl.py:234-271](), [rlm/core/rlm.py:219-231]()\n\n---\n\n## History Management\n\nMessage histories from completed conversations are stored as versioned variables, enabling the LLM to reflect on prior reasoning and adapt its strategy.\n\n### History Storage Flow\n\n```mermaid\nsequenceDiagram\n    participant RLM as \"RLM.completion()\"\n    participant Env as \"LocalREPL<br/>(SupportsPersistence)\"\n    participant Namespace as \"Execution Namespace\"\n    \n    Note over RLM: Build message_history<br/>during iteration loop\n    \n    RLM->>RLM: Find final_answer\n    \n    alt Final answer found\n        RLM->>Env: add_history(message_history)\n        Env->>Env: Determine history_index<br/>(auto-increment)\n        Env->>Namespace: Store as history_0\n        Env->>Namespace: Alias as 'history'\n        Env-->>RLM: Return history_index\n    end\n    \n    Note over RLM: Next completion() call<br/>(persistent=True)\n    \n    RLM->>Env: update_handler_address(new_addr)\n    RLM->>Env: add_context(new_payload)\n    Note over Namespace: Code can now access:<br/>history_0, context_0, context_1\n    \n    RLM->>RLM: Build new message_history\n    RLM->>RLM: Find final_answer\n    \n    alt Final answer found\n        RLM->>Env: add_history(message_history)\n        Env->>Namespace: Store as history_1\n        Note over Namespace: Code can now access:<br/>history_0, history_1, history\n    end\n```\n\n### Implementation in LocalREPL\n\nThe `add_history()` method [rlm/environments/local_repl.py:281-311]():\n\n1. Determines the history index\n2. Creates a versioned variable name: `history_{index}`\n3. Stores a **deep copy** of the message history in `self.locals`\n4. Aliases `history_0` as `history`\n5. Updates `_history_count`\n\n```python\n# From rlm/environments/local_repl.py:281-311\ndef add_history(\n    self, message_history: list[dict[str, Any]], history_index: int | None = None\n) -> int:\n    if history_index is None:\n        history_index = self._history_count\n    \n    var_name = f\"history_{history_index}\"\n    \n    # Store deep copy to avoid reference issues\n    self.locals[var_name] = copy.deepcopy(message_history)\n    \n    # Alias history_0 as 'history'\n    if history_index == 0:\n        self.locals[\"history\"] = self.locals[var_name]\n    \n    self._history_count = max(self._history_count, history_index + 1)\n    return history_index\n```\n\n### History Storage Trigger\n\nHistory is stored after a successful completion [rlm/core/rlm.py:256-259](), [rlm/core/rlm.py:283-286]():\n\n```python\n# When final_answer is found (in iteration loop)\nif self.persistent and isinstance(environment, SupportsPersistence):\n    environment.add_history(message_history)\n\n# When max_iterations reached (after loop)\nif self.persistent and isinstance(environment, SupportsPersistence):\n    environment.add_history(message_history)\n```\n\n**Sources**: [rlm/environments/local_repl.py:281-311](), [rlm/core/rlm.py:256-259](), [rlm/core/rlm.py:283-286]()\n\n---\n\n## Using Persistent Mode\n\n### Basic Usage\n\n```python\nfrom rlm import RLM\n\n# Create persistent RLM instance\nrlm = RLM(\n    backend=\"openai\",\n    backend_kwargs={\"model_name\": \"gpt-4\"},\n    environment=\"local\",\n    persistent=True  # Enable persistence\n)\n\n# First completion: loads context_0\nresult1 = rlm.completion(\"First document to analyze\")\n\n# Second completion: loads context_1, can access context_0\nresult2 = rlm.completion(\"Second document to compare with first\")\n\n# Third completion: loads context_2, can access context_0, context_1\nresult3 = rlm.completion(\"Third document to synthesize insights from all\")\n\n# Clean up when done\nrlm.close()\n```\n\n### Context Manager Pattern\n\nThe RLM class implements the context manager protocol for automatic cleanup [rlm/core/rlm.py:394-399]():\n\n```python\nwith RLM(\n    backend=\"openai\",\n    backend_kwargs={\"model_name\": \"gpt-4\"},\n    environment=\"local\",\n    persistent=True\n) as rlm:\n    result1 = rlm.completion(\"First context\")\n    result2 = rlm.completion(\"Second context\")\n    result3 = rlm.completion(\"Third context\")\n# Environment automatically cleaned up\n```\n\n### Environment Reuse Logic\n\nThe `_spawn_completion_context` method manages environment lifecycle [rlm/core/rlm.py:125-179]():\n\n```mermaid\ngraph TB\n    Start[\"completion() called\"]\n    CheckPersistent{\"persistent=True?\"}\n    CheckEnvExists{\"_persistent_env<br/>exists?\"}\n    CreateEnv[\"Create new environment<br/>get_environment()\"]\n    ReuseEnv[\"Reuse _persistent_env\"]\n    UpdateAddr[\"update_handler_address()\"]\n    AddContext[\"add_context(prompt)\"]\n    Execute[\"Execute iteration loop\"]\n    CheckCleanup{\"persistent=True?\"}\n    Cleanup[\"Cleanup environment\"]\n    KeepEnv[\"Keep environment\"]\n    \n    Start --> CheckPersistent\n    CheckPersistent -->|No| CreateEnv\n    CheckPersistent -->|Yes| CheckEnvExists\n    CheckEnvExists -->|No| CreateEnv\n    CheckEnvExists -->|Yes| ReuseEnv\n    \n    CreateEnv --> AddContext\n    ReuseEnv --> UpdateAddr\n    UpdateAddr --> AddContext\n    AddContext --> Execute\n    \n    Execute --> CheckCleanup\n    CheckCleanup -->|No| Cleanup\n    CheckCleanup -->|Yes| KeepEnv\n```\n\nKey code paths:\n\n```python\n# From rlm/core/rlm.py:152-171\nif self.persistent and self._persistent_env is not None:\n    environment = self._persistent_env\n    # Defensive check: ensure environment supports persistence methods\n    if not self._env_supports_persistence(environment):\n        raise RuntimeError(...)\n    environment.update_handler_address((lm_handler.host, lm_handler.port))\n    environment.add_context(prompt)\nelse:\n    env_kwargs = self.environment_kwargs.copy()\n    env_kwargs[\"lm_handler_address\"] = (lm_handler.host, lm_handler.port)\n    env_kwargs[\"context_payload\"] = prompt\n    env_kwargs[\"depth\"] = self.depth + 1\n    environment: BaseEnv = get_environment(self.environment_type, env_kwargs)\n    \n    if self.persistent:\n        self._persistent_env = environment\n```\n\n**Sources**: [rlm/core/rlm.py:125-179](), [rlm/core/rlm.py:152-171]()\n\n---\n\n## Implementation Details\n\n### Handler Address Updates\n\nEach `completion()` call spawns a new `LMHandler` with a fresh TCP port. The persistent environment must update its stored address to enable `llm_query()` calls from executed code.\n\n```mermaid\ngraph LR\n    subgraph \"First completion()\"\n        LMH1[\"LMHandler<br/>localhost:5001\"]\n        Env1[\"Environment stores<br/>address (localhost, 5001)\"]\n        Code1[\"Code calls llm_query()<br/>→ connects to :5001\"]\n    end\n    \n    subgraph \"Second completion()\"\n        LMH2[\"NEW LMHandler<br/>localhost:5002\"]\n        Update[\"update_handler_address()<br/>(localhost, 5002)\"]\n        Code2[\"Code calls llm_query()<br/>→ connects to :5002\"]\n    end\n    \n    LMH1 --> Env1 --> Code1\n    LMH2 --> Update --> Code2\n    \n    Env1 -.same environment.-> Update\n```\n\nThe `LocalREPL` implementation [rlm/environments/local_repl.py:273-275]():\n\n```python\ndef update_handler_address(self, address: tuple[str, int]) -> None:\n    \"\"\"Update the LM handler address for a new completion call.\"\"\"\n    self.lm_handler_address = address\n```\n\nThis address is used by `_llm_query()` [rlm/environments/local_repl.py:174-198]():\n\n```python\ndef _llm_query(self, prompt: str, model: str | None = None) -> str:\n    if not self.lm_handler_address:\n        return \"Error: No LM handler configured\"\n    \n    try:\n        request = LMRequest(prompt=prompt, model=model, depth=self.depth)\n        response = send_lm_request(self.lm_handler_address, request)\n        # ...\n```\n\n**Sources**: [rlm/environments/local_repl.py:273-275](), [rlm/environments/local_repl.py:174-198]()\n\n### State Persistence Across Calls\n\nThe `LocalREPL` environment maintains state in several structures:\n\n| State Component | Storage Location | Lifetime |\n|-----------------|------------------|----------|\n| `self.globals` | Python namespace dict | Entire persistent session |\n| `self.locals` | Python namespace dict | Entire persistent session |\n| `self.temp_dir` | Filesystem directory | Entire persistent session |\n| `_context_count` | Integer counter | Entire persistent session |\n| `_history_count` | Integer counter | Entire persistent session |\n| `_pending_llm_calls` | List buffer | Single code execution |\n\nThe namespace is initialized once in `setup()` [rlm/environments/local_repl.py:150-166]() and reused across all completions.\n\n**Sources**: [rlm/environments/local_repl.py:150-166](), [rlm/environments/local_repl.py:132-137]()\n\n### Supported Environments\n\nCurrently, only `LocalREPL` (environment type `\"local\"`) implements the `SupportsPersistence` protocol. The validation whitelist is defined at [rlm/core/rlm.py:372]():\n\n```python\npersistent_supported_environments = {\"local\"}\n```\n\nOther environments (`DockerREPL`, `ModalREPL`, `PrimeREPL`, `DaytonaREPL`) do not support persistent mode. Attempting to use them with `persistent=True` raises a `ValueError` at initialization.\n\n**Sources**: [rlm/core/rlm.py:372-380]()\n\n---\n\n## Cleanup and Resource Management\n\n### Manual Cleanup\n\nCall `close()` to clean up the persistent environment [rlm/core/rlm.py:387-392]():\n\n```python\nrlm = RLM(backend=\"openai\", environment=\"local\", persistent=True)\nresult = rlm.completion(\"Query\")\nrlm.close()  # Cleans up temp directory and resets state\n```\n\n### Automatic Cleanup with Context Manager\n\nThe `__enter__` and `__exit__` methods enable automatic cleanup [rlm/core/rlm.py:394-399]():\n\n```python\ndef __enter__(self) -> \"RLM\":\n    return self\n\ndef __exit__(self, exc_type, exc_val, exc_tb) -> bool:\n    self.close()\n    return False\n```\n\n### LocalREPL Cleanup\n\nThe `cleanup()` method [rlm/environments/local_repl.py:373-383]():\n\n1. Removes the temporary directory (`shutil.rmtree`)\n2. Clears the `globals` dict\n3. Clears the `locals` dict\n\nThis is called by:\n- `RLM.close()` → `environment.cleanup()` [rlm/core/rlm.py:389-392]()\n- Context manager exit → `self.close()` [rlm/core/rlm.py:398]()\n- `_spawn_completion_context` finally block (non-persistent mode) [rlm/core/rlm.py:177-178]()\n- `__del__` destructor [rlm/environments/local_repl.py:382-383]()\n\n**Sources**: [rlm/core/rlm.py:387-399](), [rlm/environments/local_repl.py:373-383](), [rlm/core/rlm.py:177-178]()\n\n---\n\n## Summary Table\n\n| Feature | Non-Persistent (Default) | Persistent Mode |\n|---------|--------------------------|-----------------|\n| **Environment lifecycle** | Created per `completion()` call | Reused across all calls |\n| **Context access** | Single `context` variable | Versioned: `context_0`, `context_1`, ... |\n| **History access** | Not available | Versioned: `history_0`, `history_1`, ... |\n| **State continuity** | Fresh state each call | Variables, imports persist |\n| **Handler address** | Set once at creation | Updated via `update_handler_address()` |\n| **Cleanup** | Automatic after each call | Manual via `close()` or context manager |\n| **Supported environments** | All environment types | Currently only `\"local\"` |\n| **Use case** | Single-turn queries | Multi-turn conversations, cross-document analysis |\n\n**Sources**: [rlm/core/rlm.py:54](), [rlm/core/rlm.py:98-103](), [rlm/core/rlm.py:152-171](), [rlm/environments/base_env.py:76-183]()\n\n---\n\n# Page: Custom Environments\n\n# Custom Environments\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [AGENTS.md](AGENTS.md)\n- [examples/lm_in_prime_repl.py](examples/lm_in_prime_repl.py)\n- [examples/prime_repl_example.py](examples/prime_repl_example.py)\n- [rlm/environments/base_env.py](rlm/environments/base_env.py)\n- [rlm/environments/constants.py](rlm/environments/constants.py)\n- [rlm/environments/local_repl.py](rlm/environments/local_repl.py)\n- [tests/test_imports.py](tests/test_imports.py)\n\n</details>\n\n\n\nThis guide covers the implementation of custom execution environments for the RLM system. Developers extending RLM with new execution contexts (e.g., new cloud providers, containerization systems, or specialized sandboxes) should follow the patterns and interfaces documented here.\n\nFor an overview of the environment abstraction layer and existing implementations, see [Environment Overview](#5.1). For details on existing environments (`LocalREPL`, `DockerREPL`, `ModalREPL`, etc.), see [LocalREPL](#5.2), [DockerREPL](#5.3), and [Cloud Sandbox Environments](#5.4). For implementing persistent multi-turn sessions, see [Persistent Environments and Multi-Turn Sessions](#8.2).\n\n---\n\n## Environment Type Hierarchy\n\nThe RLM environment system uses an abstract base class hierarchy that defines two primary execution modes: **non-isolated** (same machine as LM) and **isolated** (cloud-based sandboxes).\n\n```mermaid\ngraph TB\n    BaseEnv[\"BaseEnv<br/>(rlm.environments.base_env)\"]\n    NonIsolatedEnv[\"NonIsolatedEnv<br/>(same machine)\"]\n    IsolatedEnv[\"IsolatedEnv<br/>(cloud sandbox)\"]\n    \n    LocalREPL[\"LocalREPL<br/>(Python exec)\"]\n    DockerREPL[\"DockerREPL<br/>(Docker container)\"]\n    ModalREPL[\"ModalREPL<br/>(Modal.Sandbox)\"]\n    PrimeREPL[\"PrimeREPL<br/>(Prime API)\"]\n    DaytonaREPL[\"DaytonaREPL<br/>(Daytona SDK)\"]\n    \n    SupportsPersistence[\"SupportsPersistence<br/>(Protocol)\"]\n    \n    BaseEnv --> NonIsolatedEnv\n    BaseEnv --> IsolatedEnv\n    \n    NonIsolatedEnv --> LocalREPL\n    NonIsolatedEnv --> DockerREPL\n    IsolatedEnv --> ModalREPL\n    IsolatedEnv --> PrimeREPL\n    IsolatedEnv --> DaytonaREPL\n    \n    LocalREPL -.- SupportsPersistence\n    \n    style BaseEnv fill:#f9f9f9,stroke:#333,stroke-width:3px\n    style NonIsolatedEnv fill:#f9f9f9,stroke:#333,stroke-width:2px\n    style IsolatedEnv fill:#f9f9f9,stroke:#333,stroke-width:2px\n```\n\n**Figure 1: Environment Class Hierarchy**\n\nSources: [rlm/environments/base_env.py:1-183]()\n\n### Base Class Selection\n\n| Base Class | Communication | Typical Use Case | Example Implementations |\n|------------|---------------|------------------|------------------------|\n| `NonIsolatedEnv` | Direct TCP socket | Local execution, Docker containers | `LocalREPL`, `DockerREPL` |\n| `IsolatedEnv` | HTTP broker + polling | Cloud sandboxes with network isolation | `ModalREPL`, `PrimeREPL`, `DaytonaREPL` |\n\nThe choice between `NonIsolatedEnv` and `IsolatedEnv` is determined by network topology:\n- **Non-isolated**: Environment can directly open TCP socket to host machine (e.g., `host.docker.internal:port`)\n- **Isolated**: Environment requires HTTP tunnel/proxy for LM handler communication\n\nSources: [rlm/environments/base_env.py:31-74](), [AGENTS.md:146-158]()\n\n---\n\n## Required Abstract Methods\n\nAll custom environments must implement the `BaseEnv` interface:\n\n| Method | Signature | Purpose |\n|--------|-----------|---------|\n| `setup()` | `def setup(self) -> None` | Initialize execution namespace, globals, locals |\n| `load_context()` | `def load_context(self, context_payload: dict \\| list \\| str) -> None` | Make context available as `context` variable |\n| `execute_code()` | `def execute_code(self, code: str) -> REPLResult` | Execute code, capture output, return `REPLResult` |\n\nSources: [rlm/environments/base_env.py:7-29]()\n\n### REPLResult Structure\n\nThe `execute_code()` method must return a `REPLResult` dataclass:\n\n```python\n@dataclass\nclass REPLResult:\n    stdout: str                           # Captured stdout\n    stderr: str                           # Captured stderr (includes exceptions)\n    locals: dict[str, Any]                # Namespace variables after execution\n    execution_time: float                 # Time in seconds\n    rlm_calls: list[RLMChatCompletion]   # Nested LM calls via llm_query()\n```\n\nSources: [rlm/core/types.py]() (referenced in [rlm/environments/base_env.py:4]())\n\n---\n\n## Helper Functions in Execution Namespace\n\nAll environments must provide these functions in the execution namespace (via `globals` dict):\n\n| Function | Signature | Purpose |\n|----------|-----------|---------|\n| `llm_query` | `(prompt: str, model: str \\| None = None) -> str` | Single LM query to handler |\n| `llm_query_batched` | `(prompts: list[str], model: str \\| None = None) -> list[str]` | Batched LM queries |\n| `FINAL_VAR` | `(variable_name: str) -> str` | Extract variable value as final answer |\n\nThese enable executed code to perform recursive LM calls. The `depth` parameter from `BaseEnv.__init__()` is passed to the LM handler for depth-based routing.\n\nSources: [rlm/environments/local_repl.py:163-229](), [AGENTS.md:169-174]()\n\n---\n\n## Communication Patterns\n\nThe choice of base class determines the communication architecture between the environment and `LMHandler`.\n\n```mermaid\ngraph TB\n    subgraph \"Non-Isolated Pattern\"\n        NonIsoEnv[\"Environment<br/>(LocalREPL/DockerREPL)\"]\n        DirectSocket[\"Direct TCP Socket<br/>send_lm_request()\"]\n        LMHandlerNonIso[\"LMHandler<br/>(host, port)\"]\n        \n        NonIsoEnv -->|\"llm_query()<br/>LMRequest JSON\"| DirectSocket\n        DirectSocket -->|\"socket.connect()\"| LMHandlerNonIso\n        LMHandlerNonIso -->|\"LMResponse JSON\"| DirectSocket\n        DirectSocket -->|\"return str\"| NonIsoEnv\n    end\n    \n    subgraph \"Isolated Pattern\"\n        IsoEnv[\"Environment<br/>(ModalREPL/PrimeREPL)\"]\n        BrokerServer[\"HTTP Broker<br/>Flask @ localhost:8080\"]\n        Tunnel[\"Tunnel Layer<br/>(encrypted_ports/expose)\"]\n        PollerThread[\"Poller Thread<br/>(100ms interval)\"]\n        LMHandlerIso[\"LMHandler<br/>(host, port)\"]\n        \n        IsoEnv -->|\"llm_query()<br/>POST /enqueue\"| BrokerServer\n        BrokerServer -->|\"blocks until response\"| BrokerServer\n        PollerThread -->|\"GET /pending\"| Tunnel\n        Tunnel --> BrokerServer\n        PollerThread -->|\"send_lm_request()\"| LMHandlerIso\n        LMHandlerIso -->|\"LMResponse\"| PollerThread\n        PollerThread -->|\"POST /respond\"| Tunnel\n        Tunnel --> BrokerServer\n        BrokerServer -->|\"unblock, return str\"| IsoEnv\n    end\n    \n    style DirectSocket fill:#f9f9f9,stroke:#333,stroke-width:2px\n    style BrokerServer fill:#f9f9f9,stroke:#333,stroke-width:2px\n    style PollerThread fill:#f9f9f9,stroke:#333,stroke-width:2px\n```\n\n**Figure 2: Communication Patterns by Environment Type**\n\nSources: [AGENTS.md:209-308](), [rlm/core/comms_utils.py]()\n\n### Socket Protocol (Non-Isolated)\n\nNon-isolated environments use `send_lm_request()` from `rlm.core.comms_utils`:\n\n**Protocol**: 4-byte big-endian length prefix + UTF-8 JSON payload\n\n**Request Flow**:\n1. `llm_query(prompt)` called in executed code\n2. Creates `LMRequest(prompt=prompt, model=model, depth=self.depth)`\n3. Calls `send_lm_request(self.lm_handler_address, request)`\n4. Opens TCP socket to `(host, port)`\n5. Sends length-prefixed JSON via `socket_send()`\n6. Receives `LMResponse` via `socket_recv()`\n7. Returns `response.chat_completion.response` string\n\nSources: [rlm/environments/local_repl.py:174-198](), [rlm/core/comms_utils.py]()\n\n### HTTP Broker Pattern (Isolated)\n\nIsolated environments cannot directly connect to the host. They require:\n\n1. **Broker Server** (Flask): Runs inside sandbox at `localhost:8080`\n2. **Tunnel Exposure**: Platform-specific (Modal: `encrypted_ports`, Prime: `expose`, Daytona: preview URLs)\n3. **Poller Thread**: Background thread on host polls `/pending` endpoint every 100ms\n4. **Request Queue**: Broker queues requests, blocks `/enqueue` until response arrives\n\n**Broker Endpoints**:\n\n| Endpoint | Method | Purpose |\n|----------|--------|---------|\n| `/enqueue` | POST | Submit request from sandbox code (blocks) |\n| `/pending` | GET | Host polls for queued requests |\n| `/respond` | POST | Host submits response for request ID |\n| `/health` | GET | Health check |\n\nSources: [AGENTS.md:258-308](), example implementations in `rlm/environments/modal_repl.py`, `rlm/environments/prime_repl.py`\n\n---\n\n## Implementing SupportsPersistence Protocol\n\nFor environments supporting multi-turn sessions (context versioning, history tracking), implement the `SupportsPersistence` protocol:\n\n| Method | Purpose |\n|--------|---------|\n| `update_handler_address(address: tuple[str, int])` | Update LM handler address between completions |\n| `add_context(payload: dict \\| list \\| str, index: int \\| None) -> int` | Add versioned context (`context_0`, `context_1`, ...) |\n| `get_context_count() -> int` | Return number of contexts loaded |\n| `add_history(messages: list[dict], index: int \\| None) -> int` | Add versioned history (`history_0`, `history_1`, ...) |\n| `get_history_count() -> int` | Return number of histories stored |\n\n**Versioning Rules**:\n- First context → `context_0`, alias as `context`\n- First history → `history_0`, alias as `history`\n- Auto-increment if `index=None`\n- Store deep copies to avoid reference issues\n\nSources: [rlm/environments/base_env.py:76-183](), [rlm/environments/local_repl.py:231-311]()\n\n---\n\n## Implementation Checklist\n\n### Minimal Non-Isolated Environment\n\n```python\nfrom rlm.environments.base_env import NonIsolatedEnv\nfrom rlm.core.types import REPLResult\nfrom rlm.core.comms_utils import LMRequest, send_lm_request\nimport time\n\nclass MyCustomEnvironment(NonIsolatedEnv):\n    def __init__(\n        self,\n        lm_handler_address: tuple[str, int] | None = None,\n        context_payload: dict | list | str | None = None,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.lm_handler_address = lm_handler_address\n        self.setup()\n        if context_payload:\n            self.load_context(context_payload)\n    \n    def setup(self):\n        # Initialize execution namespace\n        self.globals = {\"__builtins__\": {...}}  # Safe builtins\n        self.locals = {}\n        self._pending_llm_calls = []\n        \n        # Provide helper functions\n        self.globals[\"llm_query\"] = self._llm_query\n        self.globals[\"FINAL_VAR\"] = self._final_var\n    \n    def _llm_query(self, prompt: str, model: str | None = None) -> str:\n        if not self.lm_handler_address:\n            return \"Error: No LM handler configured\"\n        request = LMRequest(prompt=prompt, model=model, depth=self.depth)\n        response = send_lm_request(self.lm_handler_address, request)\n        if response.success:\n            self._pending_llm_calls.append(response.chat_completion)\n            return response.chat_completion.response\n        return f\"Error: {response.error}\"\n    \n    def load_context(self, context_payload: dict | list | str):\n        self.globals[\"context\"] = context_payload\n    \n    def execute_code(self, code: str) -> REPLResult:\n        start = time.perf_counter()\n        self._pending_llm_calls = []\n        \n        # Execute code in namespace\n        try:\n            exec(code, self.globals, self.locals)\n            stdout, stderr = \"\", \"\"\n        except Exception as e:\n            stdout, stderr = \"\", str(e)\n        \n        return REPLResult(\n            stdout=stdout,\n            stderr=stderr,\n            locals=self.locals.copy(),\n            execution_time=time.perf_counter() - start,\n            rlm_calls=self._pending_llm_calls.copy()\n        )\n```\n\nReference implementation: [rlm/environments/local_repl.py:115-384]()\n\nSources: [rlm/environments/base_env.py:53-74](), [rlm/environments/local_repl.py:115-150](), [AGENTS.md:176-201]()\n\n### Minimal Isolated Environment\n\nFor isolated environments, follow the HTTP broker pattern. Key components:\n\n1. **Broker Script** (runs in sandbox):\n   - Flask server with `/enqueue`, `/pending`, `/respond` endpoints\n   - Threading synchronization for request/response matching\n   - State serialization with `dill`\n\n2. **Environment Class** (runs on host):\n   - Initialize sandbox with broker script\n   - Expose broker via tunnel (platform-specific)\n   - Start poller thread that polls `/pending` and forwards to `LMHandler`\n   - Execute code by submitting script to sandbox\n\n3. **Execution Script** (runs in sandbox):\n   - Load state from `/tmp/rlm_state.dill`\n   - Execute code block with `llm_query()` calling `localhost:8080/enqueue`\n   - Save state back to `/tmp/rlm_state.dill`\n\nReference implementations: `rlm/environments/modal_repl.py`, `rlm/environments/prime_repl.py`\n\nSources: [AGENTS.md:258-319](), [examples/lm_in_prime_repl.py:1-77]()\n\n---\n\n## Registration and Testing\n\n### 1. Register Environment\n\nAdd to `rlm/environments/__init__.py`:\n\n```python\nfrom rlm.environments.my_custom import MyCustomEnvironment\n\ndef get_environment(env_type: str, **kwargs) -> BaseEnv:\n    if env_type == \"custom\":\n        return MyCustomEnvironment(**kwargs)\n    # ... existing cases\n```\n\nSources: [rlm/environments/__init__.py](), [tests/test_imports.py:143-190]()\n\n### 2. Add Import Test\n\nAdd to `tests/test_imports.py`:\n\n```python\ndef test_custom_environment_import(self):\n    \"\"\"Test MyCustomEnvironment import.\"\"\"\n    from rlm.environments.my_custom import MyCustomEnvironment\n    assert MyCustomEnvironment is not None\n```\n\nSources: [tests/test_imports.py:159-190]()\n\n### 3. Test with RLM\n\n```python\nfrom rlm import RLM\n\nrlm = RLM(\n    backend=\"openai\",\n    backend_kwargs={\"api_key\": \"...\", \"model_name\": \"gpt-4\"},\n    environment=\"custom\",\n    environment_kwargs={\"custom_arg\": \"value\"}\n)\n\nresult = rlm.completion(\"Test prompt\")\n```\n\nSources: [examples/prime_repl_example.py:1-31]()\n\n---\n\n## Implementation Guidelines\n\n### Resource Management\n\nImplement `cleanup()` method and context manager protocol:\n\n```python\ndef cleanup(self):\n    # Release resources (temp dirs, containers, etc.)\n    pass\n\ndef __enter__(self):\n    return self\n\ndef __exit__(self, exc_type, exc_val, exc_tb):\n    self.cleanup()\n    return False\n```\n\nSources: [rlm/environments/local_repl.py:366-383]()\n\n### Sandboxed Builtins\n\nFor non-isolated environments, use restricted `__builtins__`:\n\n```python\n_SAFE_BUILTINS = {\n    \"print\": print,\n    \"len\": len,\n    # ... safe functions\n    \"eval\": None,    # Blocked\n    \"exec\": None,    # Blocked\n    \"input\": None,   # Blocked\n}\n```\n\nSources: [rlm/environments/local_repl.py:18-112]()\n\n### State Persistence\n\nFor isolated environments with persistent mode:\n\n```python\nimport dill\n\n# Save state\nwith open(\"/tmp/rlm_state.dill\", \"wb\") as f:\n    dill.dump({\"globals\": globals(), \"locals\": locals()}, f)\n\n# Load state\nwith open(\"/tmp/rlm_state.dill\", \"rb\") as f:\n    state = dill.load(f)\n    globals().update(state[\"globals\"])\n    locals().update(state[\"locals\"])\n```\n\nSources: [rlm/environments/constants.py:1-33]() (dill package requirement)\n\n### Configuration\n\nUse environment variables only for secrets (API keys). Hardcode reasonable defaults:\n\n```python\nclass MyCustomEnvironment(NonIsolatedEnv):\n    def __init__(\n        self,\n        lm_handler_address: tuple[str, int] | None = None,\n        timeout: int = 300,  # Hardcoded default\n        custom_arg: str = \"default\",  # Configurable via kwargs\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        # Initialize with defaults\n```\n\nSources: [AGENTS.md:139-143]()\n\n---\n\n## Testing Checklist\n\nBefore submitting a custom environment:\n\n- [ ] Implements all `BaseEnv` abstract methods\n- [ ] Provides `llm_query`, `llm_query_batched`, `FINAL_VAR` in globals\n- [ ] Returns valid `REPLResult` with all fields populated\n- [ ] Handles nested LM calls (`rlm_calls` tracking)\n- [ ] Implements `cleanup()` for resource management\n- [ ] Works with basic `RLM.completion()` call\n- [ ] Context loading works (`context` variable accessible)\n- [ ] Import test added to `tests/test_imports.py`\n- [ ] Registered in `rlm/environments/__init__.py`\n- [ ] Passes `ruff check --fix .` and `ruff format .`\n\nSources: [AGENTS.md:87-101](), [AGENTS.md:203-207]()\n\n---\n\n# Page: Usage Tracking and Cost Monitoring\n\n# Usage Tracking and Cost Monitoring\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [rlm/clients/__init__.py](rlm/clients/__init__.py)\n- [rlm/clients/azure_openai.py](rlm/clients/azure_openai.py)\n- [rlm/clients/openai.py](rlm/clients/openai.py)\n- [rlm/clients/portkey.py](rlm/clients/portkey.py)\n- [rlm/core/comms_utils.py](rlm/core/comms_utils.py)\n- [rlm/core/lm_handler.py](rlm/core/lm_handler.py)\n- [rlm/core/types.py](rlm/core/types.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis document explains the usage tracking and cost monitoring system in RLM. The system automatically tracks token consumption and API call counts for all LLM interactions, enabling cost estimation and resource monitoring across recursive execution traces. \n\nFor information about the LLM client architecture, see [Client Architecture](#4.1). For details on how usage data is embedded in execution results and logged, see [Log Format and Parsing](#6.2).\n\n## Overview\n\nThe RLM system tracks LLM usage at three levels:\n\n1. **Client Level**: Each `BaseLM` implementation maintains per-model counters for calls, input tokens, and output tokens\n2. **Handler Level**: The `LMHandler` aggregates usage across all registered clients, including the default backend and depth-routed backends\n3. **Execution Trace Level**: Usage data is embedded in `RLMChatCompletion` objects, which flow through the hierarchy: `REPLResult` → `CodeBlock` → `RLMIteration`\n\nThis hierarchical tracking ensures that every LLM call—whether made from the main RLM process or from recursive sub-calls within environments—is accounted for and traceable to its originating context.\n\nSources: [rlm/core/types.py:38-85](), [rlm/clients/base_lm.py](), [rlm/core/lm_handler.py:196-211]()\n\n## Core Data Types\n\n### ModelUsageSummary\n\nThe `ModelUsageSummary` dataclass tracks usage statistics for a single model:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `total_calls` | `int` | Total number of completion requests made to this model |\n| `total_input_tokens` | `int` | Cumulative input (prompt) tokens consumed |\n| `total_output_tokens` | `int` | Cumulative output (completion) tokens generated |\n\nThis type provides `to_dict()` and `from_dict()` methods for serialization, enabling usage data to be logged and transmitted across process boundaries.\n\nSources: [rlm/core/types.py:43-63]()\n\n### UsageSummary\n\nThe `UsageSummary` dataclass aggregates usage across multiple models:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `model_usage_summaries` | `dict[str, ModelUsageSummary]` | Map from model name to its usage summary |\n\nThis structure allows tracking multiple models simultaneously, which is essential for multi-backend configurations where different models handle different recursion depths or tasks.\n\nSources: [rlm/core/types.py:65-85]()\n\n### Usage Data in Execution Results\n\nThe `RLMChatCompletion` type embeds a `UsageSummary` for each LLM interaction:\n\n```\nRLMChatCompletion\n├── root_model: str\n├── prompt: str | dict[str, Any]\n├── response: str\n├── usage_summary: UsageSummary\n└── execution_time: float\n```\n\nThis ensures that usage data travels with the execution trace, enabling post-hoc analysis of token consumption at any level of recursion.\n\nSources: [rlm/core/types.py:91-118]()\n\n## Usage Tracking Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Type System\"\n        ModelUsageSummary[\"ModelUsageSummary<br/>total_calls<br/>total_input_tokens<br/>total_output_tokens\"]\n        UsageSummary[\"UsageSummary<br/>model_usage_summaries:<br/>dict[str, ModelUsageSummary]\"]\n        \n        UsageSummary -->|contains| ModelUsageSummary\n    end\n    \n    subgraph \"Client Layer\"\n        BaseLM[\"BaseLM Interface\"]\n        OpenAIClient[\"OpenAIClient\"]\n        PortkeyClient[\"PortkeyClient\"]\n        AzureOpenAIClient[\"AzureOpenAIClient\"]\n        \n        OpenAIClient -->|implements| BaseLM\n        PortkeyClient -->|implements| BaseLM\n        AzureOpenAIClient -->|implements| BaseLM\n        \n        ClientTracking[\"Per-Model Tracking:<br/>model_call_counts<br/>model_input_tokens<br/>model_output_tokens<br/>model_total_tokens\"]\n        \n        OpenAIClient -->|maintains| ClientTracking\n        PortkeyClient -->|maintains| ClientTracking\n        AzureOpenAIClient -->|maintains| ClientTracking\n    end\n    \n    subgraph \"Handler Layer\"\n        LMHandler[\"LMHandler\"]\n        DefaultClient[\"default_client: BaseLM\"]\n        OtherBackendClient[\"other_backend_client: BaseLM\"]\n        RegisteredClients[\"clients: dict[str, BaseLM]\"]\n        \n        LMHandler -->|owns| DefaultClient\n        LMHandler -->|owns| OtherBackendClient\n        LMHandler -->|owns| RegisteredClients\n        \n        MergedSummary[\"get_usage_summary()<br/>Merges all client summaries\"]\n        \n        LMHandler -->|provides| MergedSummary\n    end\n    \n    subgraph \"Execution Trace\"\n        RLMChatCompletion[\"RLMChatCompletion<br/>usage_summary\"]\n        REPLResult[\"REPLResult<br/>rlm_calls: list[RLMChatCompletion]\"]\n        CodeBlock[\"CodeBlock<br/>result: REPLResult\"]\n        RLMIteration[\"RLMIteration<br/>code_blocks: list[CodeBlock]\"]\n        \n        RLMChatCompletion -->|embedded in| REPLResult\n        REPLResult -->|embedded in| CodeBlock\n        CodeBlock -->|embedded in| RLMIteration\n    end\n    \n    BaseLM -->|returns| UsageSummary\n    BaseLM -->|returns| ModelUsageSummary\n    LMHandler -->|returns| UsageSummary\n    UsageSummary -->|embedded in| RLMChatCompletion\n```\n\n**Diagram: Usage Tracking Data Flow Through System Layers**\n\nThis diagram shows how usage data flows from individual clients through the handler to execution traces. Each client maintains per-model counters, the handler aggregates across clients, and usage summaries are embedded in execution results for hierarchical tracking.\n\nSources: [rlm/core/types.py:38-118](), [rlm/clients/base_lm.py](), [rlm/core/lm_handler.py:106-211]()\n\n## Client-Level Tracking\n\n### Tracking Mechanism\n\nEach `BaseLM` implementation maintains four dictionaries that map model names to counters:\n\n```\nmodel_call_counts: dict[str, int]\nmodel_input_tokens: dict[str, int]\nmodel_output_tokens: dict[str, int]\nmodel_total_tokens: dict[str, int]\n```\n\nThese are typically initialized as `defaultdict(int)` to avoid KeyError on first access.\n\nSources: [rlm/clients/openai.py:48-51](), [rlm/clients/portkey.py:28-32](), [rlm/clients/azure_openai.py:66-70]()\n\n### Token Extraction\n\nAfter each API call, the `_track_cost()` method extracts usage information from the provider's response object:\n\n**OpenAI/Azure OpenAI Pattern:**\n```python\ndef _track_cost(self, response: openai.ChatCompletion, model: str):\n    self.model_call_counts[model] += 1\n    \n    usage = getattr(response, \"usage\", None)\n    if usage is None:\n        raise ValueError(\"No usage data received. Tracking tokens not possible.\")\n    \n    self.model_input_tokens[model] += usage.prompt_tokens\n    self.model_output_tokens[model] += usage.completion_tokens\n    self.model_total_tokens[model] += usage.total_tokens\n    \n    # Track last call for handler to read\n    self.last_prompt_tokens = usage.prompt_tokens\n    self.last_completion_tokens = usage.completion_tokens\n```\n\nThe `last_prompt_tokens` and `last_completion_tokens` attributes store the most recent call's usage, which the `LMHandler` reads to create per-call `ModelUsageSummary` objects.\n\nSources: [rlm/clients/openai.py:99-113](), [rlm/clients/azure_openai.py:112-126]()\n\n**Portkey Pattern:**\n```python\ndef _track_cost(self, response: ChatCompletions, model: str):\n    self.model_call_counts[model] += 1\n    self.model_input_tokens[model] += response.usage.prompt_tokens\n    self.model_output_tokens[model] += response.usage.completion_tokens\n    self.model_total_tokens[model] += response.usage.total_tokens\n    \n    # Track last call for handler to read\n    self.last_prompt_tokens = response.usage.prompt_tokens\n    self.last_completion_tokens = response.usage.completion_tokens\n```\n\nSources: [rlm/clients/portkey.py:69-77]()\n\n### Client Interface Methods\n\nAll clients implement two methods defined in `BaseLM`:\n\n**get_usage_summary()**\n\nReturns a `UsageSummary` containing all models tracked by this client:\n\n```python\ndef get_usage_summary(self) -> UsageSummary:\n    model_summaries = {}\n    for model in self.model_call_counts:\n        model_summaries[model] = ModelUsageSummary(\n            total_calls=self.model_call_counts[model],\n            total_input_tokens=self.model_input_tokens[model],\n            total_output_tokens=self.model_output_tokens[model],\n        )\n    return UsageSummary(model_usage_summaries=model_summaries)\n```\n\nSources: [rlm/clients/openai.py:114-122](), [rlm/clients/portkey.py:79-87](), [rlm/clients/azure_openai.py:127-135]()\n\n**get_last_usage()**\n\nReturns a `ModelUsageSummary` for the most recent completion:\n\n```python\ndef get_last_usage(self) -> ModelUsageSummary:\n    return ModelUsageSummary(\n        total_calls=1,\n        total_input_tokens=self.last_prompt_tokens,\n        total_output_tokens=self.last_completion_tokens,\n    )\n```\n\nThis method is called by `LMHandler` after each request to create the per-call usage data embedded in `RLMChatCompletion` objects.\n\nSources: [rlm/clients/openai.py:124-129](), [rlm/clients/portkey.py:89-94](), [rlm/clients/azure_openai.py:137-142]()\n\n## Handler-Level Aggregation\n\n### Request Handling and Usage Capture\n\nThe `LMHandler` creates `RLMChatCompletion` objects for each request, embedding usage data:\n\n**Single Request Flow:**\n\n```mermaid\nsequenceDiagram\n    participant Env as \"Environment/RLM\"\n    participant Handler as \"LMHandler\"\n    participant Client as \"BaseLM Client\"\n    \n    Env->>Handler: \"LMRequest(prompt, model, depth)\"\n    Handler->>Handler: \"get_client(model, depth)\"\n    Handler->>Client: \"completion(prompt)\"\n    Client->>Client: \"_track_cost(response, model)\"\n    Client-->>Handler: \"response string\"\n    Handler->>Client: \"get_last_usage()\"\n    Client-->>Handler: \"ModelUsageSummary(1 call, tokens)\"\n    Handler->>Handler: \"Create UsageSummary\"\n    Handler->>Handler: \"Create RLMChatCompletion with usage\"\n    Handler-->>Env: \"LMResponse(chat_completion)\"\n```\n\n**Diagram: Single Request Usage Tracking Flow**\n\nThe handler obtains the last usage from the client immediately after each completion, creating a `UsageSummary` with a single model entry. This is embedded in the `RLMChatCompletion` object returned to the caller.\n\nSources: [rlm/core/lm_handler.py:46-65]()\n\n**Batched Request Flow:**\n\nFor batched requests, the handler creates one `RLMChatCompletion` per prompt, all sharing the same usage summary (with per-prompt execution time approximation):\n\n```python\ndef _handle_batched(self, request: LMRequest, handler: \"LMHandler\") -> LMResponse:\n    client = handler.get_client(request.model, request.depth)\n    \n    start_time = time.perf_counter()\n    \n    async def run_all():\n        tasks = [client.acompletion(prompt) for prompt in request.prompts]\n        return await asyncio.gather(*tasks)\n    \n    results = asyncio.run(run_all())\n    end_time = time.perf_counter()\n    \n    total_time = end_time - start_time\n    model_usage = client.get_last_usage()\n    root_model = request.model or client.model_name\n    usage_summary = UsageSummary(model_usage_summaries={root_model: model_usage})\n    \n    chat_completions = [\n        RLMChatCompletion(\n            root_model=root_model,\n            prompt=prompt,\n            response=content,\n            usage_summary=usage_summary,\n            execution_time=total_time / len(request.prompts),\n        )\n        for prompt, content in zip(request.prompts, results, strict=True)\n    ]\n    \n    return LMResponse.batched_success_response(chat_completions=chat_completions)\n```\n\nNote: For batched requests, the usage summary represents the aggregate usage across all prompts in the batch, as `get_last_usage()` returns cumulative stats from the async operations.\n\nSources: [rlm/core/lm_handler.py:67-96]()\n\n### Multi-Client Aggregation\n\nThe `LMHandler.get_usage_summary()` method merges usage from all active clients:\n\n```python\ndef get_usage_summary(self) -> UsageSummary:\n    \"\"\"Get the usage summary for all clients, merged into a single dict.\"\"\"\n    merged = {}\n    # Include default client\n    default_summary = self.default_client.get_usage_summary()\n    merged.update(default_summary.model_usage_summaries)\n    # Include other backend client if it exists\n    if self.other_backend_client is not None:\n        other_summary = self.other_backend_client.get_usage_summary()\n        merged.update(other_summary.model_usage_summaries)\n    # Include all registered clients\n    for client in self.clients.values():\n        client_summary = client.get_usage_summary()\n        merged.update(client_summary.model_usage_summaries)\n    return UsageSummary(model_usage_summaries=merged)\n```\n\nThis enables tracking across:\n- **default_client**: The primary backend specified in `RLM.__init__(backend=...)`\n- **other_backend_client**: The secondary backend for depth-based routing (see [Multi-Backend Configuration](#8.1))\n- **clients**: Any explicitly registered clients via `register_client()`\n\nIf multiple clients use the same model name, later updates override earlier ones in the merged dictionary.\n\nSources: [rlm/core/lm_handler.py:196-211]()\n\n## Hierarchical Usage Tracking in Execution Flow\n\n### Embedding in Execution Results\n\nUsage data flows through the execution hierarchy via the `rlm_calls` field in `REPLResult`:\n\n```\nREPLResult\n├── stdout: str\n├── stderr: str\n├── locals: dict\n├── execution_time: float\n└── rlm_calls: list[RLMChatCompletion]  ← Sub-LM calls with usage data\n```\n\nWhen code executes in an environment and makes sub-LM calls via `llm_query()` or `llm_query_batched()`, each call creates an `RLMChatCompletion` object that is appended to the `rlm_calls` list. This enables tracking of:\n\n1. **Direct usage**: Tokens consumed by the main RLM iteration\n2. **Recursive usage**: Tokens consumed by code executing in environments\n3. **Multi-depth usage**: Tokens consumed at different recursion depths when using depth-based routing\n\nSources: [rlm/core/types.py:120-153]()\n\n### Aggregation Example\n\nConsider a single RLM iteration where:\n- The main LLM call consumes 1000 input tokens and 200 output tokens\n- Generated code makes 3 sub-LM calls, each consuming 100 input tokens and 50 output tokens\n\nThe resulting structure would be:\n\n```\nRLMIteration\n├── response: \"...\" (main LLM response)\n├── usage_summary: UsageSummary({\"gpt-4\": {1000 input, 200 output}})\n└── code_blocks: [\n    CodeBlock\n    └── result: REPLResult\n        └── rlm_calls: [\n            RLMChatCompletion(usage_summary: {\"gpt-4\": {100 input, 50 output}}),\n            RLMChatCompletion(usage_summary: {\"gpt-4\": {100 input, 50 output}}),\n            RLMChatCompletion(usage_summary: {\"gpt-4\": {100 input, 50 output}})\n        ]\n]\n```\n\nTotal usage for this iteration: 1300 input tokens, 350 output tokens.\n\nSources: [rlm/core/types.py:156-180]()\n\n## Accessing Usage Data\n\n### Runtime Access via LMHandler\n\nDuring RLM execution, the active `LMHandler` instance provides current usage statistics:\n\n```python\n# Within RLM code\nrlm = RLM(backend=\"openai\", backend_kwargs={\"model_name\": \"gpt-4\"})\n# ... after some completions ...\nusage = rlm.lm_handler.get_usage_summary()\n\nfor model_name, model_usage in usage.model_usage_summaries.items():\n    print(f\"{model_name}:\")\n    print(f\"  Calls: {model_usage.total_calls}\")\n    print(f\"  Input tokens: {model_usage.total_input_tokens}\")\n    print(f\"  Output tokens: {model_usage.total_output_tokens}\")\n```\n\nThis provides real-time visibility into token consumption across all models and backends.\n\nSources: [rlm/core/lm_handler.py:196-211]()\n\n### Post-Execution Access via Logs\n\nThe `RLMLogger` serializes complete execution traces including all usage data (see [RLM Logger and Verbose Output](#6.1)). Each `RLMIteration` in the log contains:\n\n- Main iteration usage in the top-level response\n- Nested usage in `code_blocks[].result.rlm_calls[]`\n\nLog parsing utilities can aggregate usage across the entire execution:\n\n```python\n# Pseudocode for log-based usage aggregation\ntotal_usage = defaultdict(lambda: {\"calls\": 0, \"input\": 0, \"output\": 0})\n\nfor iteration in log_file.iterations:\n    # Main iteration usage\n    for model, usage in iteration.usage_summary.items():\n        total_usage[model][\"calls\"] += usage.total_calls\n        total_usage[model][\"input\"] += usage.total_input_tokens\n        total_usage[model][\"output\"] += usage.total_output_tokens\n    \n    # Sub-call usage\n    for code_block in iteration.code_blocks:\n        for rlm_call in code_block.result.rlm_calls:\n            for model, usage in rlm_call.usage_summary.items():\n                total_usage[model][\"calls\"] += usage.total_calls\n                total_usage[model][\"input\"] += usage.total_input_tokens\n                total_usage[model][\"output\"] += usage.total_output_tokens\n```\n\nSources: [rlm/core/types.py:165-180]()\n\n## Cost Calculation Considerations\n\n### Token-to-Cost Mapping\n\nThe RLM system tracks token counts but does not perform cost calculations directly. To estimate costs:\n\n1. **Obtain pricing data**: Each LLM provider publishes per-token pricing (typically separate rates for input and output tokens)\n2. **Apply model-specific rates**: Different models have different pricing\n3. **Account for batch discounts**: Some providers offer reduced rates for batch API usage\n\nExample calculation:\n\n```python\n# Example pricing (as of documentation writing, verify current rates)\nPRICING = {\n    \"gpt-4\": {\"input\": 0.03 / 1000, \"output\": 0.06 / 1000},  # $ per token\n    \"gpt-3.5-turbo\": {\"input\": 0.0015 / 1000, \"output\": 0.002 / 1000},\n}\n\nusage = lm_handler.get_usage_summary()\ntotal_cost = 0.0\n\nfor model_name, model_usage in usage.model_usage_summaries.items():\n    if model_name in PRICING:\n        input_cost = model_usage.total_input_tokens * PRICING[model_name][\"input\"]\n        output_cost = model_usage.total_output_tokens * PRICING[model_name][\"output\"]\n        total_cost += input_cost + output_cost\n        print(f\"{model_name}: ${input_cost + output_cost:.4f}\")\n\nprint(f\"Total estimated cost: ${total_cost:.4f}\")\n```\n\n### Multi-Backend Cost Attribution\n\nWhen using multi-backend configurations (see [Multi-Backend Configuration](#8.1)), usage is tracked separately per model:\n\n| Backend | Model | Usage Tracking |\n|---------|-------|----------------|\n| Default (depth=0) | `gpt-4` | Tracked under key `\"gpt-4\"` |\n| Other (depth≥1) | `gpt-3.5-turbo` | Tracked under key `\"gpt-3.5-turbo\"` |\n\nThis separation enables:\n- **Cost attribution**: Identify which recursion level consumes most tokens\n- **Optimization opportunities**: Replace expensive models at certain depths\n- **Budget enforcement**: Monitor costs per backend independently\n\n### Caching and Cost Reduction\n\nSome providers offer prompt caching (e.g., Anthropic's prompt caching, OpenAI's prompt caching). These features can significantly reduce input token costs for repeated context. The RLM usage tracking system captures the billed tokens as reported by the provider, which may be lower than the actual prompt length when caching is active.\n\nTo leverage caching effectively:\n- Use persistent environments (see [Persistent Environments and Multi-Turn Sessions](#8.2)) to maintain context\n- Structure prompts to maximize cache hits on shared prefixes\n- Monitor usage data to verify cache effectiveness\n\n## Usage Tracking Flow Diagram\n\n```mermaid\ngraph TB\n    subgraph \"1. Completion Request\"\n        UserCode[\"User Code<br/>rlm.completion(prompt)\"]\n        RLMMain[\"RLM.completion()\"]\n        IterLoop[\"Iteration Loop<br/>max_iterations\"]\n        \n        UserCode -->|\"1\"| RLMMain\n        RLMMain -->|\"2\"| IterLoop\n    end\n    \n    subgraph \"2. LM Handler Processing\"\n        LMRequest[\"LMRequest<br/>prompt, model, depth\"]\n        GetClient[\"LMHandler.get_client()<br/>Route by depth/model\"]\n        ClientCompletion[\"Client.completion()\"]\n        TrackCost[\"Client._track_cost()<br/>Increment counters\"]\n        GetLastUsage[\"Client.get_last_usage()\"]\n        \n        IterLoop -->|\"3\"| LMRequest\n        LMRequest -->|\"4\"| GetClient\n        GetClient -->|\"5\"| ClientCompletion\n        ClientCompletion -->|\"6\"| TrackCost\n        TrackCost -->|\"7\"| GetLastUsage\n    end\n    \n    subgraph \"3. Usage Data Creation\"\n        ModelUsageSummary1[\"ModelUsageSummary<br/>1 call, N input, M output\"]\n        UsageSummary1[\"UsageSummary<br/>model_usage_summaries\"]\n        RLMChatCompletion1[\"RLMChatCompletion<br/>prompt, response,<br/>usage_summary\"]\n        \n        GetLastUsage -->|\"8\"| ModelUsageSummary1\n        ModelUsageSummary1 -->|\"9\"| UsageSummary1\n        UsageSummary1 -->|\"10\"| RLMChatCompletion1\n    end\n    \n    subgraph \"4. Code Execution\"\n        ExtractCode[\"Extract code blocks\"]\n        EnvExecute[\"Environment.execute_code()\"]\n        SubLMCalls[\"llm_query() calls<br/>in user code\"]\n        \n        RLMChatCompletion1 -->|\"11\"| ExtractCode\n        ExtractCode -->|\"12\"| EnvExecute\n        EnvExecute -->|\"13\"| SubLMCalls\n    end\n    \n    subgraph \"5. Recursive Usage Tracking\"\n        SubRequest[\"LMRequest (depth+1)\"]\n        SubHandler[\"LMHandler processes<br/>recursive request\"]\n        SubRLMChatCompletion[\"RLMChatCompletion<br/>(sub-call)\"]\n        \n        SubLMCalls -->|\"14\"| SubRequest\n        SubRequest -->|\"15\"| SubHandler\n        SubHandler -->|\"16\"| SubRLMChatCompletion\n    end\n    \n    subgraph \"6. Result Aggregation\"\n        REPLResult[\"REPLResult<br/>rlm_calls: [RLMChatCompletion, ...]\"]\n        CodeBlock[\"CodeBlock<br/>result: REPLResult\"]\n        RLMIteration[\"RLMIteration<br/>code_blocks: [CodeBlock, ...]\"]\n        \n        SubRLMChatCompletion -->|\"17\"| REPLResult\n        REPLResult -->|\"18\"| CodeBlock\n        CodeBlock -->|\"19\"| RLMIteration\n    end\n    \n    subgraph \"7. Final Usage Summary\"\n        HandlerSummary[\"LMHandler.get_usage_summary()<br/>Aggregates all clients\"]\n        MergedUsage[\"UsageSummary<br/>All models, all depths\"]\n        \n        RLMIteration -->|\"20 (if done)\"| HandlerSummary\n        HandlerSummary -->|\"21\"| MergedUsage\n    end\n    \n    style GetClient fill:#f9f9f9,stroke:#333,stroke-width:2px\n    style TrackCost fill:#f9f9f9,stroke:#333,stroke-width:2px\n    style HandlerSummary fill:#f9f9f9,stroke:#333,stroke-width:2px\n```\n\n**Diagram: Complete Usage Tracking Flow from Request to Aggregation**\n\nThis diagram illustrates the complete lifecycle of usage tracking: (1) a completion request enters the system, (2) the handler routes it to a client which tracks usage, (3) usage data is embedded in the response, (4) code execution may trigger recursive LM calls, (5) recursive calls are tracked with incremented depth, (6) results aggregate usage hierarchically, and (7) the handler provides a merged summary across all operations.\n\nSources: [rlm/core/lm_handler.py:46-96](), [rlm/core/lm_handler.py:196-211](), [rlm/core/types.py:43-118]()\n\n## Summary Table: Key Classes and Methods\n\n| Component | Location | Key Methods/Fields | Purpose |\n|-----------|----------|-------------------|---------|\n| `ModelUsageSummary` | [rlm/core/types.py:43-63]() | `total_calls`, `total_input_tokens`, `total_output_tokens` | Tracks usage for a single model |\n| `UsageSummary` | [rlm/core/types.py:65-85]() | `model_usage_summaries: dict[str, ModelUsageSummary]` | Aggregates usage across models |\n| `RLMChatCompletion` | [rlm/core/types.py:91-118]() | `usage_summary: UsageSummary` | Embeds usage in execution trace |\n| `BaseLM` | [rlm/clients/base_lm.py]() | `get_usage_summary()`, `get_last_usage()` | Interface for client usage tracking |\n| `OpenAIClient` | [rlm/clients/openai.py:48-129]() | `model_call_counts`, `model_input_tokens`, `_track_cost()` | Tracks OpenAI API usage |\n| `PortkeyClient` | [rlm/clients/portkey.py:28-94]() | `model_call_counts`, `model_input_tokens`, `_track_cost()` | Tracks Portkey API usage |\n| `AzureOpenAIClient` | [rlm/clients/azure_openai.py:66-142]() | `model_call_counts`, `model_input_tokens`, `_track_cost()` | Tracks Azure OpenAI API usage |\n| `LMHandler` | [rlm/core/lm_handler.py:196-211]() | `get_usage_summary()` | Aggregates usage across all clients |\n| `REPLResult` | [rlm/core/types.py:120-153]() | `rlm_calls: list[RLMChatCompletion]` | Contains usage from sub-LM calls |\n\nSources: [rlm/core/types.py:38-153](), [rlm/clients/openai.py:48-129](), [rlm/clients/portkey.py:28-94](), [rlm/clients/azure_openai.py:66-142](), [rlm/core/lm_handler.py:196-211]()\n\n---\n\n# Page: API Reference\n\n# API Reference\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [rlm/clients/__init__.py](rlm/clients/__init__.py)\n- [rlm/clients/azure_openai.py](rlm/clients/azure_openai.py)\n- [rlm/core/comms_utils.py](rlm/core/comms_utils.py)\n- [rlm/core/lm_handler.py](rlm/core/lm_handler.py)\n- [rlm/core/rlm.py](rlm/core/rlm.py)\n- [rlm/core/types.py](rlm/core/types.py)\n- [rlm/environments/base_env.py](rlm/environments/base_env.py)\n\n</details>\n\n\n\nThis page provides comprehensive API documentation for all public classes, methods, and data types in the RLM system. It covers type definitions, class interfaces, method signatures, and protocol specifications.\n\nFor architectural context about how these components interact, see [Core Architecture](#3). For usage examples, see [Getting Started](#2). For extending the system with custom implementations, see [Advanced Topics](#8).\n\n---\n\n## Core Type System\n\nThe RLM system uses a strongly-typed dataclass-based architecture for representing execution traces, usage tracking, and configuration metadata.\n\n### Backend and Environment Literals\n\n```python\n# Available LLM client backends\nClientBackend = Literal[\n    \"openai\",      # OpenAI API\n    \"portkey\",     # Portkey AI Gateway\n    \"openrouter\",  # OpenRouter API\n    \"vercel\",      # Vercel AI Gateway\n    \"vllm\",        # vLLM local server\n    \"litellm\",     # LiteLLM proxy\n    \"anthropic\",   # Anthropic API\n    \"azure_openai\", # Azure OpenAI\n    \"gemini\",      # Google Gemini\n]\n\n# Available execution environments\nEnvironmentType = Literal[\n    \"local\",   # LocalREPL (same-machine, sandboxed)\n    \"docker\",  # DockerREPL (containerized)\n    \"modal\",   # ModalREPL (Modal cloud)\n    \"prime\",   # PrimeREPL (Prime Intellect)\n    \"daytona\", # DaytonaREPL (Daytona cloud)\n]\n```\n\n**Sources:** [rlm/core/types.py:5-16]()\n\n### Usage Tracking Types\n\n```mermaid\nclassDiagram\n    class ModelUsageSummary {\n        +int total_calls\n        +int total_input_tokens\n        +int total_output_tokens\n        +to_dict() dict\n        +from_dict(dict) ModelUsageSummary\n    }\n    \n    class UsageSummary {\n        +dict~str,ModelUsageSummary~ model_usage_summaries\n        +to_dict() dict\n        +from_dict(dict) UsageSummary\n    }\n    \n    UsageSummary *-- ModelUsageSummary : \"contains\"\n```\n\n#### ModelUsageSummary\n\nTracks token usage for a single model.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `total_calls` | `int` | Number of completion calls made |\n| `total_input_tokens` | `int` | Total prompt tokens consumed |\n| `total_output_tokens` | `int` | Total completion tokens generated |\n\n**Methods:**\n- `to_dict() -> dict`: Serialize to dictionary\n- `from_dict(data: dict) -> ModelUsageSummary`: Deserialize from dictionary\n\n**Sources:** [rlm/core/types.py:43-62]()\n\n#### UsageSummary\n\nAggregates usage across multiple models.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `model_usage_summaries` | `dict[str, ModelUsageSummary]` | Per-model usage summaries keyed by model name |\n\n**Methods:**\n- `to_dict() -> dict`: Serialize to dictionary\n- `from_dict(data: dict) -> UsageSummary`: Deserialize from dictionary\n\n**Sources:** [rlm/core/types.py:65-84]()\n\n### Execution Result Types\n\n```mermaid\ngraph TB\n    RLMChatCompletion[\"RLMChatCompletion\"]\n    REPLResult[\"REPLResult\"]\n    CodeBlock[\"CodeBlock\"]\n    RLMIteration[\"RLMIteration\"]\n    UsageSummary[\"UsageSummary\"]\n    \n    REPLResult -->|\"rlm_calls: list\"| RLMChatCompletion\n    RLMChatCompletion -->|\"usage_summary\"| UsageSummary\n    CodeBlock -->|\"result\"| REPLResult\n    RLMIteration -->|\"code_blocks: list\"| CodeBlock\n```\n\n#### RLMChatCompletion\n\nRecords a single LLM call made from within the environment.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `root_model` | `str` | Model identifier that generated the response |\n| `prompt` | `str \\| dict[str, Any]` | Input prompt (string or message dict) |\n| `response` | `str` | Generated text response |\n| `usage_summary` | `UsageSummary` | Token usage for this call |\n| `execution_time` | `float` | Wall-clock time in seconds |\n\n**Methods:**\n- `to_dict() -> dict`: Serialize to dictionary\n- `from_dict(data: dict) -> RLMChatCompletion`: Deserialize from dictionary\n\n**Sources:** [rlm/core/types.py:91-117]()\n\n#### REPLResult\n\nCaptures the outcome of executing a code block in an environment.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `stdout` | `str` | Standard output captured during execution |\n| `stderr` | `str` | Standard error captured during execution |\n| `locals` | `dict` | Local variables after execution (serialized) |\n| `execution_time` | `float` | Wall-clock time in seconds |\n| `rlm_calls` | `list[RLMChatCompletion]` | Sub-LLM calls made during code execution |\n\n**Constructor Parameters:**\n```python\nREPLResult(\n    stdout: str,\n    stderr: str,\n    locals: dict,\n    execution_time: float = None,\n    rlm_calls: list[RLMChatCompletion] = None,\n)\n```\n\n**Methods:**\n- `to_dict() -> dict`: Serialize to dictionary (locals are serialized using `_serialize_value`)\n- `__str__() -> str`: Human-readable string representation\n\n**Sources:** [rlm/core/types.py:121-153]()\n\n#### CodeBlock\n\nPairs code with its execution result.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `code` | `str` | Source code that was executed |\n| `result` | `REPLResult` | Execution outcome |\n\n**Methods:**\n- `to_dict() -> dict`: Serialize to dictionary\n\n**Sources:** [rlm/core/types.py:156-161]()\n\n#### RLMIteration\n\nRepresents a complete reasoning step in the RLM loop.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `prompt` | `str \\| dict[str, Any]` | Messages sent to LLM |\n| `response` | `str` | LLM's text response |\n| `code_blocks` | `list[CodeBlock]` | Extracted code blocks and their results |\n| `final_answer` | `str \\| None` | Final answer if found (default: None) |\n| `iteration_time` | `float \\| None` | Wall-clock time in seconds (default: None) |\n\n**Methods:**\n- `to_dict() -> dict`: Serialize to dictionary\n\n**Sources:** [rlm/core/types.py:165-179]()\n\n### Metadata Types\n\n#### RLMMetadata\n\nConfiguration metadata for an RLM instance.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `root_model` | `str` | Primary model identifier |\n| `max_depth` | `int` | Maximum recursion depth allowed |\n| `max_iterations` | `int` | Maximum iteration loop count |\n| `backend` | `str` | Client backend name |\n| `backend_kwargs` | `dict[str, Any]` | Client configuration (serialized) |\n| `environment_type` | `str` | Environment name |\n| `environment_kwargs` | `dict[str, Any]` | Environment configuration (serialized) |\n| `other_backends` | `list[str] \\| None` | Additional backends for sub-calls |\n\n**Methods:**\n- `to_dict() -> dict`: Serialize to dictionary (sensitive keys filtered)\n\n**Sources:** [rlm/core/types.py:188-212]()\n\n#### QueryMetadata\n\nAnalyzes prompt structure for logging and debugging.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `context_lengths` | `list[int]` | Length of each context chunk |\n| `context_total_length` | `int` | Sum of all context lengths |\n| `context_type` | `str` | Type of context: \"str\", \"dict\", or \"list\" |\n\n**Constructor:**\n```python\nQueryMetadata(prompt: str | list[str] | dict[Any, Any] | list[dict[Any, Any]])\n```\n\nAutomatically computes lengths based on prompt type.\n\n**Sources:** [rlm/core/types.py:221-265]()\n\n---\n\n## RLM Orchestrator Class\n\nThe `RLM` class is the main entrypoint for recursive language model completions.\n\n### Class: RLM\n\n```mermaid\ngraph LR\n    User[\"User Code\"] -->|\"rlm.completion(prompt)\"| RLM\n    RLM -->|\"spawns per-call\"| LMHandler[\"LMHandler\"]\n    RLM -->|\"spawns per-call\"| Env[\"BaseEnv\"]\n    RLM -->|\"logs to\"| RLMLogger[\"RLMLogger\"]\n    RLM -->|\"tracks\"| Persistent[\"Persistent Environment<br/>SupportsPersistence\"]\n```\n\n#### Constructor\n\n```python\nRLM(\n    backend: ClientBackend = \"openai\",\n    backend_kwargs: dict[str, Any] | None = None,\n    environment: EnvironmentType = \"local\",\n    environment_kwargs: dict[str, Any] | None = None,\n    depth: int = 0,\n    max_depth: int = 1,\n    max_iterations: int = 30,\n    custom_system_prompt: str | None = None,\n    other_backends: list[ClientBackend] | None = None,\n    other_backend_kwargs: list[dict[str, Any]] | None = None,\n    logger: RLMLogger | None = None,\n    verbose: bool = False,\n    persistent: bool = False,\n)\n```\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `backend` | `ClientBackend` | `\"openai\"` | Primary LLM client backend |\n| `backend_kwargs` | `dict[str, Any] \\| None` | `None` | Configuration for primary backend |\n| `environment` | `EnvironmentType` | `\"local\"` | Code execution environment type |\n| `environment_kwargs` | `dict[str, Any] \\| None` | `None` | Configuration for environment |\n| `depth` | `int` | `0` | Current recursion depth (0-indexed) |\n| `max_depth` | `int` | `1` | Maximum recursion depth |\n| `max_iterations` | `int` | `30` | Maximum iteration loop count |\n| `custom_system_prompt` | `str \\| None` | `None` | Override default system prompt |\n| `other_backends` | `list[ClientBackend] \\| None` | `None` | Backends for depth=1 routing |\n| `other_backend_kwargs` | `list[dict[str, Any]] \\| None` | `None` | Configuration for other backends |\n| `logger` | `RLMLogger \\| None` | `None` | Logger for trajectory capture |\n| `verbose` | `bool` | `False` | Enable rich console output |\n| `persistent` | `bool` | `False` | Reuse environment across calls |\n\n**Raises:**\n- `ValueError`: If `other_backends` has length != 1 (currently only one additional backend supported)\n- `ValueError`: If `persistent=True` but environment doesn't support persistence\n\n**Sources:** [rlm/core/rlm.py:40-123]()\n\n#### Methods\n\n##### completion()\n\n```python\ndef completion(\n    self,\n    prompt: str | dict[str, Any],\n    root_prompt: str | None = None\n) -> RLMChatCompletion\n```\n\nMain entry point for recursive language model completions.\n\n**Parameters:**\n- `prompt`: Context to pass to the model (string or message dictionary)\n- `root_prompt`: Optional small prompt visible to root LM (e.g., user question)\n\n**Returns:** `RLMChatCompletion` containing final answer, usage statistics, and execution time\n\n**Behavior:**\n1. If `depth >= max_depth`, falls back to direct LLM call\n2. Spawns LMHandler and environment for this call\n3. Runs iteration loop up to `max_iterations` times\n4. Each iteration: prompts LLM → extracts code blocks → executes code → checks for final answer\n5. Returns when final answer found or max iterations reached\n\n**Sources:** [rlm/core/rlm.py:192-295]()\n\n##### close()\n\n```python\ndef close(self) -> None\n```\n\nClean up persistent environment. Must be called when done with multi-turn conversations if `persistent=True`.\n\n**Sources:** [rlm/core/rlm.py:387-392]()\n\n##### Context Manager Support\n\n```python\nwith RLM(...) as rlm:\n    result = rlm.completion(prompt)\n# Automatically calls close() on exit\n```\n\n**Sources:** [rlm/core/rlm.py:394-399]()\n\n#### Private Methods (Internal)\n\n| Method | Purpose |\n|--------|---------|\n| `_spawn_completion_context()` | Context manager that creates LMHandler and environment |\n| `_setup_prompt()` | Builds system prompt with metadata |\n| `_completion_turn()` | Executes single iteration: LLM call → code execution |\n| `_default_answer()` | Generates answer if max iterations reached |\n| `_fallback_answer()` | Direct LLM call when at max depth |\n| `_validate_persistent_environment_support()` | Validates environment supports persistence |\n| `_env_supports_persistence()` | Static check for SupportsPersistence protocol |\n\n**Sources:** [rlm/core/rlm.py:125-385]()\n\n---\n\n## LM Handler API\n\nThe `LMHandler` manages concurrent LLM requests via a multi-threaded TCP server.\n\n### Class: LMHandler\n\n```mermaid\ngraph TB\n    LMHandler[\"LMHandler\"] -->|\"routes to\"| DefaultClient[\"default_client<br/>BaseLM\"]\n    LMHandler -->|\"routes to\"| OtherClient[\"other_backend_client<br/>BaseLM\"]\n    LMHandler -->|\"registry\"| Clients[\"clients dict<br/>model_name → BaseLM\"]\n    \n    Environment[\"Environment Code\"] -->|\"socket request\"| Server[\"ThreadingLMServer<br/>host:port\"]\n    Server -->|\"handles via\"| LMHandler\n```\n\n#### Constructor\n\n```python\nLMHandler(\n    client: BaseLM,\n    host: str = \"127.0.0.1\",\n    port: int = 0,  # auto-assign\n    other_backend_client: BaseLM | None = None,\n)\n```\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `client` | `BaseLM` | Required | Primary LLM client (depth=0) |\n| `host` | `str` | `\"127.0.0.1\"` | Server bind address |\n| `port` | `int` | `0` | Server port (0 = auto-assign) |\n| `other_backend_client` | `BaseLM \\| None` | `None` | Client for depth=1 routing |\n\n**Sources:** [rlm/core/lm_handler.py:114-129]()\n\n#### Methods\n\n##### start()\n\n```python\ndef start(self) -> tuple[str, int]\n```\n\nStart the socket server in a background daemon thread.\n\n**Returns:** `(host, port)` tuple for connecting\n\n**Sources:** [rlm/core/lm_handler.py:164-175]()\n\n##### stop()\n\n```python\ndef stop(self) -> None\n```\n\nShutdown the socket server.\n\n**Sources:** [rlm/core/lm_handler.py:177-182]()\n\n##### register_client()\n\n```python\ndef register_client(self, model_name: str, client: BaseLM) -> None\n```\n\nRegister a client that can be requested by model name.\n\n**Sources:** [rlm/core/lm_handler.py:131-133]()\n\n##### get_client()\n\n```python\ndef get_client(self, model: str | None = None, depth: int = 0) -> BaseLM\n```\n\nGet client by model name or depth-based routing.\n\n**Routing Logic:**\n1. If `model` specified and registered → use that client\n2. Else if `depth == 1` and `other_backend_client` exists → use other backend\n3. Else → use `default_client`\n\n**Sources:** [rlm/core/lm_handler.py:135-150]()\n\n##### completion()\n\n```python\ndef completion(self, prompt: str, model: str | None = None) -> str\n```\n\nDirect completion call for main process use (bypasses socket server).\n\n**Sources:** [rlm/core/lm_handler.py:184-186]()\n\n##### get_usage_summary()\n\n```python\ndef get_usage_summary(self) -> UsageSummary\n```\n\nAggregate usage statistics from all registered clients.\n\n**Sources:** [rlm/core/lm_handler.py:196-210]()\n\n#### Properties\n\n| Property | Type | Description |\n|----------|------|-------------|\n| `port` | `int` | Actual port (useful when auto-assigned) |\n| `address` | `tuple[str, int]` | `(host, port)` tuple |\n\n**Sources:** [rlm/core/lm_handler.py:152-162]()\n\n---\n\n## Communication Protocol API\n\n### Socket Protocol\n\nThe RLM system uses a 4-byte big-endian length prefix + JSON payload protocol for socket communication.\n\n```mermaid\nsequenceDiagram\n    participant Env as \"Environment\"\n    participant Sock as \"Socket Protocol\"\n    participant Handler as \"LMHandler\"\n    \n    Env->>Sock: send_lm_request(address, LMRequest)\n    Sock->>Sock: socket_send(socket, request.to_dict())\n    Sock->>Handler: [4-byte length][JSON payload]\n    Handler->>Handler: socket_recv(socket)\n    Handler->>Handler: Process request\n    Handler->>Sock: socket_send(socket, response.to_dict())\n    Sock->>Sock: socket_recv(socket)\n    Sock->>Env: LMResponse\n```\n\n#### Functions\n\n##### socket_send()\n\n```python\ndef socket_send(sock: socket.socket, data: dict) -> None\n```\n\nSend a length-prefixed JSON message.\n\n**Protocol:** 4-byte big-endian length prefix + UTF-8 JSON payload\n\n**Sources:** [rlm/core/comms_utils.py:146-152]()\n\n##### socket_recv()\n\n```python\ndef socket_recv(sock: socket.socket) -> dict\n```\n\nReceive a length-prefixed JSON message.\n\n**Returns:** Empty dict if connection closed before length received\n\n**Raises:** `ConnectionError` if connection closes mid-message\n\n**Sources:** [rlm/core/comms_utils.py:155-176]()\n\n##### socket_request()\n\n```python\ndef socket_request(\n    address: tuple[str, int],\n    data: dict,\n    timeout: int = 300\n) -> dict\n```\n\nSend request and receive response over a new socket connection.\n\n**Sources:** [rlm/core/comms_utils.py:179-196]()\n\n##### send_lm_request()\n\n```python\ndef send_lm_request(\n    address: tuple[str, int],\n    request: LMRequest,\n    timeout: int = 300,\n    depth: int | None = None\n) -> LMResponse\n```\n\nSend typed LM request and return typed response.\n\n**Sources:** [rlm/core/comms_utils.py:204-224]()\n\n##### send_lm_request_batched()\n\n```python\ndef send_lm_request_batched(\n    address: tuple[str, int],\n    prompts: list[str | dict[str, Any]],\n    model: str | None = None,\n    timeout: int = 300,\n    depth: int = 0\n) -> list[LMResponse]\n```\n\nSend batched LM request, returns list of responses (one per prompt, same order).\n\n**Sources:** [rlm/core/comms_utils.py:227-264]()\n\n### Message Types\n\n#### LMRequest\n\n```python\n@dataclass\nclass LMRequest:\n    prompt: str | dict[str, Any] | None = None\n    prompts: list[str | dict[str, Any]] | None = None\n    model: str | None = None\n    depth: int = 0\n```\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `prompt` | `str \\| dict \\| None` | Single prompt (mutually exclusive with `prompts`) |\n| `prompts` | `list[...] \\| None` | Batched prompts (mutually exclusive with `prompt`) |\n| `model` | `str \\| None` | Optional model name for routing |\n| `depth` | `int` | Recursion depth for routing (default: 0) |\n\n**Properties:**\n- `is_batched -> bool`: True if `prompts` field is non-empty\n\n**Methods:**\n- `to_dict() -> dict`: Serialize (excludes None values)\n- `from_dict(data: dict) -> LMRequest`: Deserialize\n\n**Sources:** [rlm/core/comms_utils.py:22-58]()\n\n#### LMResponse\n\n```python\n@dataclass\nclass LMResponse:\n    error: str | None = None\n    chat_completion: RLMChatCompletion | None = None\n    chat_completions: list[RLMChatCompletion] | None = None\n```\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `error` | `str \\| None` | Error message if request failed |\n| `chat_completion` | `RLMChatCompletion \\| None` | Single response |\n| `chat_completions` | `list[...] \\| None` | Batched responses |\n\n**Properties:**\n- `success -> bool`: True if `error` is None\n- `is_batched -> bool`: True if `chat_completions` is not None\n\n**Methods:**\n- `to_dict() -> dict`: Serialize\n- `from_dict(data: dict) -> LMResponse`: Deserialize\n- `success_response(chat_completion) -> LMResponse`: Class method for success\n- `batched_success_response(chat_completions) -> LMResponse`: Class method for batched\n- `error_response(error) -> LMResponse`: Class method for errors\n\n**Sources:** [rlm/core/comms_utils.py:61-138]()\n\n---\n\n## Client Backend API\n\n### Abstract Base Class: BaseLM\n\nAll LLM client implementations inherit from `BaseLM`.\n\n```mermaid\nclassDiagram\n    class BaseLM {\n        <<abstract>>\n        +str model_name\n        +completion(prompt) str*\n        +acompletion(prompt) str*\n        +get_usage_summary() UsageSummary*\n        +get_last_usage() ModelUsageSummary*\n    }\n    \n    class OpenAIClient {\n        +OpenAI client\n        +AsyncOpenAI async_client\n        +completion(prompt, model) str\n        +acompletion(prompt, model) str\n    }\n    \n    class AnthropicClient {\n        +Anthropic client\n        +AsyncAnthropic async_client\n        +completion(prompt, model) str\n        +acompletion(prompt, model) str\n    }\n    \n    class AzureOpenAIClient {\n        +AzureOpenAI client\n        +AsyncAzureOpenAI async_client\n        +completion(prompt, model) str\n        +acompletion(prompt, model) str\n    }\n    \n    class PortkeyClient {\n        +Portkey client\n        +completion(prompt, model) str\n        +acompletion(prompt, model) str\n    }\n    \n    BaseLM <|-- OpenAIClient\n    BaseLM <|-- AnthropicClient\n    BaseLM <|-- AzureOpenAIClient\n    BaseLM <|-- PortkeyClient\n```\n\n#### Abstract Methods\n\n```python\n@abstractmethod\ndef completion(\n    self,\n    prompt: str | list[dict[str, Any]],\n    model: str | None = None\n) -> str\n```\n\nSynchronous completion call.\n\n**Parameters:**\n- `prompt`: String prompt or list of message dictionaries\n- `model`: Optional model override\n\n**Returns:** Generated text response\n\n---\n\n```python\n@abstractmethod\nasync def acompletion(\n    self,\n    prompt: str | list[dict[str, Any]],\n    model: str | None = None\n) -> str\n```\n\nAsynchronous completion call (for batched requests).\n\n---\n\n```python\n@abstractmethod\ndef get_usage_summary(self) -> UsageSummary\n```\n\nGet aggregated usage statistics across all calls.\n\n---\n\n```python\n@abstractmethod\ndef get_last_usage(self) -> ModelUsageSummary\n```\n\nGet usage statistics from the most recent call.\n\n**Sources:** Referenced from inheritance hierarchy\n\n### Factory Function: get_client()\n\n```python\ndef get_client(\n    backend: ClientBackend,\n    backend_kwargs: dict[str, Any],\n) -> BaseLM\n```\n\nFactory function to instantiate the appropriate client implementation.\n\n**Supported Backends:**\n\n| Backend | Implementation | Notes |\n|---------|---------------|-------|\n| `\"openai\"` | `OpenAIClient` | Direct OpenAI API |\n| `\"vllm\"` | `OpenAIClient` | Requires `base_url` in kwargs |\n| `\"openrouter\"` | `OpenAIClient` | Sets `base_url=\"https://openrouter.ai/api/v1\"` |\n| `\"vercel\"` | `OpenAIClient` | Sets `base_url=\"https://ai-gateway.vercel.sh/v1\"` |\n| `\"portkey\"` | `PortkeyClient` | Portkey AI gateway |\n| `\"anthropic\"` | `AnthropicClient` | Anthropic API |\n| `\"azure_openai\"` | `AzureOpenAIClient` | Azure OpenAI service |\n| `\"gemini\"` | `GeminiClient` | Google Gemini |\n| `\"litellm\"` | `LiteLLMClient` | LiteLLM proxy |\n\n**Raises:** `ValueError` if backend is not recognized\n\n**Sources:** [rlm/clients/__init__.py:11-63]()\n\n### Example Client: AzureOpenAIClient\n\n```python\nAzureOpenAIClient(\n    api_key: str | None = None,\n    model_name: str | None = None,\n    azure_endpoint: str | None = None,\n    api_version: str | None = None,\n    azure_deployment: str | None = None,\n    **kwargs,\n)\n```\n\n**Environment Variables:**\n- `AZURE_OPENAI_API_KEY`: API key (if not provided via `api_key`)\n- `AZURE_OPENAI_ENDPOINT`: Azure endpoint URL (required)\n- `AZURE_OPENAI_API_VERSION`: API version (default: \"2024-02-01\")\n- `AZURE_OPENAI_DEPLOYMENT`: Deployment name\n\n**Tracking:**\n- Maintains per-model call counts and token usage in internal dicts\n- Updates `last_prompt_tokens` and `last_completion_tokens` after each call\n\n**Sources:** [rlm/clients/azure_openai.py:17-142]()\n\n---\n\n## Environment API\n\n### Abstract Base Classes\n\n```mermaid\ngraph TB\n    BaseEnv[\"BaseEnv<br/>(abstract)\"]\n    NonIsolatedEnv[\"NonIsolatedEnv<br/>(abstract)\"]\n    IsolatedEnv[\"IsolatedEnv<br/>(abstract)\"]\n    \n    LocalREPL[\"LocalREPL\"]\n    DockerREPL[\"DockerREPL\"]\n    ModalREPL[\"ModalREPL\"]\n    PrimeREPL[\"PrimeREPL\"]\n    DaytonaREPL[\"DaytonaREPL\"]\n    \n    BaseEnv --> NonIsolatedEnv\n    BaseEnv --> IsolatedEnv\n    \n    NonIsolatedEnv --> LocalREPL\n    NonIsolatedEnv --> DockerREPL\n    \n    IsolatedEnv --> ModalREPL\n    IsolatedEnv --> PrimeREPL\n    IsolatedEnv --> DaytonaREPL\n```\n\n#### Class: BaseEnv\n\n```python\nclass BaseEnv(ABC):\n    def __init__(self, persistent: bool = False, depth: int = 1, **kwargs):\n        self.persistent = persistent\n        self.depth = depth\n        self.kwargs = kwargs\n```\n\n**Abstract Methods:**\n\n```python\n@abstractmethod\ndef setup(self) -> None\n```\n\nInitialize the environment (e.g., start processes, connect to services).\n\n---\n\n```python\n@abstractmethod\ndef load_context(self, context_payload: dict | list | str) -> None\n```\n\nLoad context data into the environment's execution namespace.\n\n---\n\n```python\n@abstractmethod\ndef execute_code(self, code: str) -> REPLResult\n```\n\nExecute code and return results including stdout, stderr, locals, and nested LLM calls.\n\n**Sources:** [rlm/environments/base_env.py:7-29]()\n\n#### Class: NonIsolatedEnv\n\nEnvironments that run on the same machine as the LM process.\n\n**Inherits:** All abstract methods from `BaseEnv`\n\n**Examples:** `LocalREPL`, `DockerREPL`\n\n**Sources:** [rlm/environments/base_env.py:53-73]()\n\n#### Class: IsolatedEnv\n\nEnvironments that run on a completely separate machine (cloud sandboxes).\n\n**Inherits:** All abstract methods from `BaseEnv`\n\n**Examples:** `ModalREPL`, `PrimeREPL`, `DaytonaREPL`\n\n**Sources:** [rlm/environments/base_env.py:31-50]()\n\n### Protocol: SupportsPersistence\n\nEnvironments that support multi-turn conversations and state persistence implement this protocol.\n\n```python\n@runtime_checkable\nclass SupportsPersistence(Protocol):\n    def update_handler_address(self, address: tuple[str, int]) -> None: ...\n    def add_context(self, context_payload, context_index: int | None = None) -> int: ...\n    def get_context_count(self) -> int: ...\n    def add_history(self, message_history, history_index: int | None = None) -> int: ...\n    def get_history_count(self) -> int: ...\n```\n\n#### Protocol Methods\n\n##### update_handler_address()\n\n```python\ndef update_handler_address(self, address: tuple[str, int]) -> None\n```\n\nUpdate the LM handler address for nested LLM calls. Called by RLM when the handler address changes between completions in persistent mode.\n\n**Parameters:**\n- `address`: `(host, port)` tuple for the LM handler server\n\n---\n\n##### add_context()\n\n```python\ndef add_context(\n    self,\n    context_payload: dict | list | str,\n    context_index: int | None = None\n) -> int\n```\n\nAdd a context payload, making it available as `context_N` in code.\n\n**Versioning:**\n- `context_index=None`: Auto-increment (0, 1, 2, ...)\n- `context_index=N`: Use specific index N\n\n**Storage Requirements:**\n- Must be accessible as `context_0`, `context_1`, etc.\n- `context` must alias to `context_0`\n\n**Returns:** The index used (for auto-increment, returns the assigned index)\n\n---\n\n##### get_context_count()\n\n```python\ndef get_context_count(self) -> int\n```\n\nReturn the number of contexts added so far.\n\n---\n\n##### add_history()\n\n```python\ndef add_history(\n    self,\n    message_history: list[dict[str, Any]],\n    history_index: int | None = None\n) -> int\n```\n\nAdd a message history, making it available as `history_N` in code.\n\n**Versioning:**\n- `history_index=None`: Auto-increment (0, 1, 2, ...)\n- `history_index=N`: Use specific index N\n\n**Storage Requirements:**\n- Must be accessible as `history_0`, `history_1`, etc.\n- `history` must alias to `history_0`\n- **Must store a deep copy**, not a reference\n\n**Returns:** The index used\n\n---\n\n##### get_history_count()\n\n```python\ndef get_history_count(self) -> int\n```\n\nReturn the number of histories added so far.\n\n**Usage Check:**\n```python\nfrom rlm.environments import SupportsPersistence\n\nif isinstance(env, SupportsPersistence):\n    env.add_context(prompt)\n    count = env.get_context_count()\n```\n\n**Sources:** [rlm/environments/base_env.py:76-182]()\n\n---\n\n## Utility Functions and Helpers\n\n### Value Serialization\n\n```python\ndef _serialize_value(value: Any) -> Any\n```\n\nConvert a value to a JSON-serializable representation for logging and transmission.\n\n**Behavior:**\n- Primitives (`None`, `bool`, `int`, `float`, `str`) → unchanged\n- `ModuleType` → `\"<module 'name'>\"`\n- `list`, `tuple` → recursively serialize elements\n- `dict` → recursively serialize keys and values\n- Callables → `\"<type 'name'>\"`\n- Other types → `repr(value)` or `\"<type>\"`\n\n**Sources:** [rlm/core/types.py:19-35]()\n\n### Type Hierarchy Summary Table\n\n| Category | Base Class | Concrete Implementations | Purpose |\n|----------|-----------|-------------------------|---------|\n| **Data Types** | N/A | `ModelUsageSummary`, `UsageSummary`, `RLMChatCompletion`, `REPLResult`, `CodeBlock`, `RLMIteration`, `RLMMetadata`, `QueryMetadata` | Structured data for execution traces |\n| **Orchestration** | N/A | `RLM` | Main entrypoint for completions |\n| **Request Handling** | N/A | `LMHandler` | Multi-threaded TCP server for LLM requests |\n| **Communication** | N/A | `LMRequest`, `LMResponse` | Socket protocol messages |\n| **LLM Clients** | `BaseLM` | `OpenAIClient`, `AnthropicClient`, `AzureOpenAIClient`, `PortkeyClient`, `GeminiClient`, `LiteLLMClient` | LLM provider integrations |\n| **Environments** | `BaseEnv` | `LocalREPL`, `DockerREPL`, `ModalREPL`, `PrimeREPL`, `DaytonaREPL` | Code execution contexts |\n| **Environment Types** | N/A | `NonIsolatedEnv`, `IsolatedEnv` | Environment isolation categories |\n| **Protocols** | N/A | `SupportsPersistence` | Multi-turn conversation support |\n\n**Sources:** All files cited above"
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "text": "使用 **RLM (Recursive Language Model)** 的方式非常「程式化」。它不是那種你開一個網頁就能用的工具，而是一個**開發庫**。\n\n你問的「每次問都要 CALL」其實指出了它的運作本質：它透過**遞迴 (Recursion)** 將「大問題」拆解成「小問題」，並自動發送多次 API 呼叫。\n\n以下是它的具體用法和邏輯分析：\n\n### 1. 如何使用 RLM？\n你需要在 Python 程式碼中初始化它，然後呼叫 `.completion()`。\n\n```python\nfrom rlm import RLM\n\n# 1. 初始化 (設定後端模型和執行環境)\nrlm = RLM(\n    backend=\"openai\",\n    backend_kwargs={\"model_name\": \"gpt-4\"},\n    environment=\"local\"  # 程式碼執行的地方\n)\n\n# 2. 呼叫 (這一步會觸發 RLM 的內部遞迴)\nresult = rlm.completion(\"分析這 100 萬行的 Log 檔案並找錯\")\n```\n\n### 2. 它是「自動 CALL」還是「手動 CALL」？\n答案是：**你 CALL 它一次，它內部會 CALL 自己很多次。**\n\n*   **傳統 LLM**：你問 -> LLM 答。如果 Context 太長，LLM 就會忘記或報錯。\n*   **RLM 的過程**：\n    1.  你發送請求。\n    2.  RLM 發現任務太重，它會**寫一段 Python 程式碼**來處理部分數據。\n    3.  這段程式碼在沙盒執行時，會調用一個 `llm_query()` 函數（這就是遞迴點）。\n    4.  這個 `llm_query()` 會**再次 CALL 一個子 AI** 去處理更小的碎片。\n    5.  最後一層層回傳結果，總結給你。\n\n**所以對你（使用者）來說，你只 CALL 了一次；但對 API 供應商來說，產生了多次連鎖呼叫。**\n\n### 3. RLM 與 Gemini CLI 的共享記憶有什麼關係？\n這就是兩者最大的差別：\n\n*   **Gemini CLI (你現在用的)**：靠**「查表」**（Search）。我需要什麼，就去你的硬碟裡 `grep` 什麼。我的記憶是存在 `task.md` 這種檔案裡的。\n*   **RLM (alexzhang13/rlm)**：靠**「分治」**（Divide and Conquer）。它把你的 Context 變成程式變數，讓 AI 透過「寫程式」來管理自己的記憶。\n\n### 總結：什麼時候該用 RLM？\n*   **不適合**：一般的日常對話、寫幾行小程式。用 RLM 反而太慢且浪費 Token。\n*   **適合**：**超大規模數據處理**。例如你要 AI 讀完一整本原始碼庫，或者分析幾 GB 的數據檔案。這時候「遞迴」才是唯一能突破 Context 限制的方法。\n\n如果你是因為覺得我（Gemini CLI）有時候會忘記之前的對話，我建議我們先嘗試**「更新 `task.md` 並重啟 Session」**，這通常能解決 95% 的記憶問題，而且比 RLM 便宜且快速得多！\n\n你需要我幫你把目前的對話進度總結到 `task.md` 中嗎？這樣你重啟 CLI 後我就能「瞬間恢復記憶」。"
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "text": "那這樣的話，假設分析師想要研究台積電的報告.. 他叫AI用RLM研究完一次後.. 問幾個問題，CONTEXT也不會受影響？"
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "text": "沒錯，你抓到重點了！這正是 RLM（或這類架構）最強大的地方。\n\n傳統 AI 的問題是：你把 500 頁的報告丟給它，它的**腦袋（Context Window）就塞滿了**，你再問它問題，它就會開始「頭暈」（表現變差）。\n\n但使用 RLM 研究台積電報告時，邏輯會變成這樣：\n\n### 1. 記憶的「離片化」處理 (Offloading)\nRLM 不會試圖把整份報告一次吞下去。它會把報告當成一個**外部變數**（例如儲存在 Python 環境裡的 `context_0`）。\n*   **傳統 AI**：腦袋裡塞著 500 頁紙。\n*   **RLM AI**：腦袋是空的，但手邊有一台電腦，電腦硬碟裡存著那 500 頁紙。\n\n### 2. 為什麼 Context 不受影響？\n當分析師問：「台積電去年的研發毛利是多少？」\nRLM 的反應不是去「回憶」，而是**自動寫一段 Python 程式碼**去 `context_0` 裡搜尋關鍵字。\n*   **結果**：AI 的對話框裡只會出現「搜尋結果的幾行字」，而不是整份報告。這確保了 AI 的 **Context Window 永遠保持乾淨、專注**。\n\n### 3. 持久化 Session (Persistent Sessions)\n在 RLM 的設計中（特別是開啟了 `persistent=True` 模式時）：\n*   **第一次研究**：它可能已經把報告做好了索引（Index）或摘要變數存起來了。\n*   **問後續問題**：它直接調用之前在 Python 環境裡存好的變數（State）。\n*   **影響**：即便你問了 100 個問題，AI 的表現依然會像剛開始對話一樣精準，因為它不需要重複讀取整份報告，每次呼叫都是「精準打擊」。\n\n---\n\n### 總結分析師的情境：\n\n| 比較維度 | 傳統 LLM (如 ChatGPT 網頁版) | 使用 RLM 架構 |\n| :--- | :--- | :--- |\n| **處理 10 份報告** | Context 瞬間爆炸，開始胡言亂語 | 報告存在環境變數，腦袋依然清醒 |\n| **問細節問題** | 可能會遺忘報告中間的數據 | 透過程式碼搜尋，數據 100% 準確 |\n| **對話長度** | 對話越長，AI 越笨（注意力分散） | 對話長度對 AI 核心邏輯影響極小 |\n\n### 我（Gemini CLI）能做到嗎？\n雖然我不是 RLM 庫，但我現在就在用類似的邏輯為你服務：\n*   你剛剛問我 RLM 的事，我不是「背下來」的，我是用 `web_fetch` 和 `read_wiki` 去抓取資訊。\n*   我的腦袋裡只存了「你的問題」和「搜尋到的結論」，而不是整個 GitHub 倉庫。\n\n**結論：** 如果你要處理的是「大量數據的持續問答」，RLM 這種**「AI + 程式碼執行環境」**的架構，確實是目前解決 AI 「健忘症」和「性能下滑」的最佳解法。\n\n你想試著在你的專案中引入這種「持久化環境」的概念嗎？我可以幫你修改 `.gemini/GEMINI.md`，讓我學會如何用這種方式管理你的專案記憶。"
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "text": "怎麼把這段對話先存起來？"
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "text": "我會將這段對話的關鍵洞察（包括 RLM 的運作原理、與 Gemini CLI 的記憶差異，以及如何處理大規模報告的策略）整理並存入你專案中的 `.gemini/task.md` 檔案中。\n\n這樣做的好處是：即便你現在關閉 CLI，下次進來時我只要讀取這個檔案，就能立刻找回這些知識，而不必占用寶貴的對話長度。\n\n我先讀取目前的 `task.md` 內容，再進行更新。"
      },
      {
        "functionCall": {
          "name": "read_file",
          "args": {
            "file_path": ".gemini/task.md"
          },
          "id": "read_file-1770837242131-01fc984eeb8818"
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "read_file-1770837242131-01fc984eeb8818",
          "name": "read_file",
          "response": {
            "output": "# Task: Implement Webview State Persistence\n\n## Status\n- [x] Investigate current webview implementation <!-- id: 0 -->\n- [x] Enable `retainContextWhenHidden` in `SidebarProvider.ts` <!-- id: 1 -->\n- [x] Implement VS Code webview state persistence in Vue 3 frontend <!-- id: 2 -->\n- [x] Verify state persistence when switching tabs <!-- id: 3 -->\n- [x] Generate walkthrough.md <!-- id: 4 -->\n\n## Progress\n- Initialized task.md\n- Added `retainContextWhenHidden: true` to `registerWebviewViewProvider` in `extension.ts`.\n- Implemented `vscode.getState()` and `vscode.setState()` in `App.vue`.\n- Added deep watch to persist state on changes.\n- Added logic to restore selected template from state when templates are loaded.\n- Verified build with `npm run package`.\n"
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "text": "我已經將關於 **RLM 與 AI 記憶機制** 的深度對話內容整理好，並新增到了 `.gemini/task.md` 的「研究筆記」章節中。\n\n這包含了：\n1. **Gemini CLI vs RLM**：搜尋模式與遞迴模式的差異。\n2. **分析師情境模擬**：如何利用持久化環境處理大規模報告（如台積電研究）。\n3. **解決長對話效能下滑的策略**：將 Context 轉化為程式變數。\n\n你可以隨時輸入 `cat .gemini/task.md` 查看，或在重啟 Session 後要求我「讀取研究筆記」來接續討論。"
      },
      {
        "functionCall": {
          "name": "replace",
          "args": {
            "file_path": "/Users/poseidomhung/Documents/github/Infinity/Harmony/.gemini/task.md",
            "old_string": "## Progress\n- Initialized task.md\n- Added `retainContextWhenHidden: true` to `registerWebviewViewProvider` in `extension.ts`.\n- Implemented `vscode.getState()` and `vscode.setState()` in `App.vue`.\n- Added deep watch to persist state on changes.\n- Added logic to restore selected template from state when templates are loaded.\n- Verified build with `npm run package`.\n",
            "instruction": "Add a \"Research Notes\" section to store the discussion about LLM memory sharing and RLM architecture.",
            "new_string": "## Progress\n- Initialized task.md\n- Added `retainContextWhenHidden: true` to `registerWebviewViewProvider` in `extension.ts`.\n- Implemented `vscode.getState()` and `vscode.setState()` in `App.vue`.\n- Added deep watch to persist state on changes.\n- Added logic to restore selected template from state when templates are loaded.\n- Verified build with `npm run package`.\n\n## Research Notes: LLM Memory & RLM\n- **Memory Sharing**: Gemini CLI instances share long-term memory via `save_memory` and project-level state via `.gemini/task.md`.\n- **Context Management**: To prevent AI performance degradation in long chats, use \"External Brain\" strategies: offloading details to files and using targeted searches (`grep`).\n- **RLM (Recursive Language Model)**: \n    - **Logic**: Programmatic recursion. It turns context into variables in a REPL environment.\n    - **Efficiency**: Instead of filling the context window with raw data, it writes code to process data fragments and sub-calls itself.\n    - **Use Case**: Massive data analysis (e.g., TSMC analyst reports). It keeps the AI's \"main mind\" clean while using a \"sandbox PC\" to handle the heavy lifting.\n"
          },
          "id": "replace-1770837253713-1360451491a678"
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "replace-1770837253713-1360451491a678",
          "name": "replace",
          "response": {
            "output": "Successfully modified file: /Users/poseidomhung/Documents/github/Infinity/Harmony/.gemini/task.md (1 replacements)."
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "text": "對話內容已成功存入 `.gemini/task.md`。\n\n這樣下次你開新對話時，只要跟我說 **「繼續研究 task.md 裡的 RLM 筆記」**，我就能立刻接上目前的思路。還有其他關於 RLM 或記憶機制想探討的嗎？"
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "text": "好神奇，為什麼我在另外一個資料夾，我把剛剛在專案存的TASK.MD複製到另一個專案的同樣地方，但GEMINI從LIST卻看不到？"
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "text": "這通常是因為 **Gemini CLI 的「視野範圍」** 是由你啟動它時所在的**工作目錄（Working Directory）**決定的。\n\n如果你在 A 專案開啟了 Gemini CLI，它會掃描 A 專案的檔案。即使你用滑鼠手動把 `task.md` 複製到了 B 專案，**目前對話中的這個 Gemini CLI 實例** 依然只看得到 A 專案。\n\n以下是幾個可能的原因與解決方法：\n\n### 1. 工作目錄限制 (Scope)\n當我啟動時，我會讀取當前專案根目錄下的 `.gemini/`。如果你開的是「Harmony」專案的 CLI，我無法「通靈」看到另一個「Harmony-Backup」專案裡的檔案，除非你明確告訴我路徑。\n\n*   **驗證方法**：你可以問我「請列出當前目錄下的檔案」，看看顯示的是不是你預期的那個資料夾。\n\n### 2. `.geminiignore` 檔案\n如果你在另一個專案裡有一個 `.geminiignore` 或 `.gitignore` 檔案，且裡面寫了 `task.md`，那麼我在執行 `list_directory` 時會自動過濾掉它，假裝它不存在（為了保護隱私或減少雜訊）。\n\n### 3. CLI 需要「刷新」視野\n如果你是在**對話進行中**手動複製檔案的，我並不會每秒鐘掃描一次你的硬碟。我只有在呼叫 `glob` 或 `grep_search` 時才會去「看」硬碟。\n\n---\n\n**我們可以現在做個實驗：**\n請告訴我你複製到的那個專案的**絕對路徑**，我直接嘗試讀取它。例如：\n`「請讀取 /Users/poseidomhung/Documents/github/AnotherProject/.gemini/task.md」`\n\n如果我能讀到，就代表只是「視野範圍」的問題。\n\n**另外一個小提醒：**\n如果你是在 VS Code 的不同視窗開啟 Gemini，每個視窗都是一個獨立的 Session。它們之間唯一的「共同記憶」是 `save_memory` 存下的事實，檔案系統則是各自獨立的。\n\n你想讓我試著找找看那個「消失」的檔案嗎？你可以給我路徑或關鍵字。"
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "text": "@/Users/poseidomhung/Desktop/TREND/BEST/rlm-mcp/.gemini/task.md"
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "text": "我會嘗試讀取你指定的絕對路徑檔案 `/Users/poseidomhung/Desktop/TREND/BEST/rlm-mcp/.gemini/task.md`，以確認我是否能跨越目前的工作目錄存取它。"
      },
      {
        "thoughtSignature": "CiEBjz1rX949iaBhJFdvSFS8YDI01xKo8ZlWBn1zIouruw4=",
        "functionCall": {
          "name": "read_file",
          "args": {
            "file_path": "/Users/poseidomhung/Desktop/TREND/BEST/rlm-mcp/.gemini/task.md"
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "read_file-1770838738134-7441cd269077d",
          "name": "read_file",
          "response": {
            "error": "Path not in workspace: Attempted path \"/Users/poseidomhung/Desktop/TREND/BEST/rlm-mcp/.gemini/task.md\" resolves outside the allowed workspace directories: /Users/poseidomhung/Documents/github/Infinity/Harmony, /Users/poseidomhung/.gemini/extensions/ralph, /Users/poseidomhung/.gemini/tmp/08526d3f4291f9d55d22dac70d4aac16a9b53eb9b4ef382e5fbb1d9ee234fd74/plans or the project temp directory: /Users/poseidomhung/.gemini/tmp/08526d3f4291f9d55d22dac70d4aac16a9b53eb9b4ef382e5fbb1d9ee234fd74"
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "text": "這證實了我的推測：**安全邊界（Workspace Boundaries）**。\n\nGemini CLI 有一套嚴格的安全機制：我只能存取你啟動 CLI 時指定的 **「工作目錄」** 及其子資料夾。\n\n### 為什麼我讀不到？\n1.  **隔離性**：你目前的 CLI 是在 `/Users/poseidomhung/Documents/github/Infinity/Harmony` 啟動的。\n2.  **安全限制**：為了防止 AI 意外修改或讀取電腦上的敏感資料（如 SSH Key 或照片），系統禁止我跳出這個「工作空間」去讀取 `/Users/poseidomhung/Desktop/...` 的檔案。\n\n### 解決方法：\n如果你想讓我也能處理那個 `rlm-mcp` 專案，你有兩個選擇：\n\n1.  **在該目錄重開一個 CLI**：\n    開啟終端機，`cd /Users/poseidomhung/Desktop/TREND/BEST/rlm-mcp`，然後在那裡啟動 `gemini`。這樣它就會自動讀取該目錄下的 `task.md`。\n    \n2.  **將該目錄加入工作空間**：\n    如果你是用 VS Code 擴充功能，可以將該資料夾也加入目前的 Workspace。\n\n**總結來說：** 我看不到是因為我被「鎖」在了 Harmony 專案裡。這保證了不同專案之間的資料不會互相污染。\n\n如果你現在需要我分析 `rlm-mcp` 裡的內容，請在那個資料夾重新啟動我！"
      }
    ]
  }
]